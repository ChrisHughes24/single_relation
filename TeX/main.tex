\def\paperversiondraft{draft}
\def\paperversionblind{blind}

\ifx\paperversion\paperversionblind
\else
  \def\paperversion{blind}
\fi

% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage{xargs}
\usepackage{todonotes}
\usepackage{xparse}
\usepackage{xifthen, xstring}
\usepackage{ulem}
\usepackage{xspace}
\bibliographystyle{amsalpha}
\makeatletter
\font\uwavefont=lasyb10 scaled 652
\DeclareSymbolFontAlphabet{\mathrm}    {operators}
\DeclareSymbolFontAlphabet{\mathnormal}{letters}
\DeclareSymbolFontAlphabet{\mathcal}   {symbols}
\DeclareMathAlphabet      {\mathbf}{OT1}{cmr}{bx}{n}
\DeclareMathAlphabet      {\mathsf}{OT1}{cmss}{m}{n}
\DeclareMathAlphabet      {\mathit}{OT1}{cmr}{m}{it}
\DeclareMathAlphabet      {\mathtt}{OT1}{cmtt}{m}{n}
\newcommand\colorwave[1][blue]{\bgroup\markoverwith{\lower3\p@\hbox{\uwavefont\textcolor{#1}{\char58}}}\ULon}
% \makeatother

% \ifx\paperversion\paperversiondraft
% \newcommand\createtodoauthor[2]{%
% \def\tmpdefault{emptystring}
% \expandafter\newcommand\csname #1\endcsname[2][\tmpdefault]{\def\tmp{##1}\ifthenelse{\equal{\tmp}{\tmpdefault}}
%    {\todo[linecolor=#2!20,backgroundcolor=#2!25,bordercolor=#2,size=\tiny]{\textbf{#1:} ##2}}
%    {\ifthenelse{\equal{##2}{}}{\colorwave[#2]{##1}\xspace}{\todo[linecolor=#2!10,backgroundcolor=#2!25,bordercolor=#2]{\tiny \textbf{#1:} ##2}\colorwave[#2]{##1}}}}}
% \else
% \newcommand\createtodoauthor[2]{%
% \expandafter\newcommand\csname #1\endcsname[2][\@nil]{}}
% \fi


% \createtodoauthor{chris}{red}
% \createtodoauthor{tobias}{blue}
% \createtodoauthor{authorThree}{green}
% \createtodoauthor{authorFour}{orange}
% \createtodoauthor{authorFive}{purple}


%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

% Broaden margins to make room for todo notes
\ifx\paperversion\paperversiondraft
  \paperwidth=\dimexpr \paperwidth + 6cm\relax
  \oddsidemargin=\dimexpr\oddsidemargin + 3cm\relax
  \evensidemargin=\dimexpr\evensidemargin + 3cm\relax
  \marginparwidth=\dimexpr \marginparwidth + 3cm\relax
  \setlength{\marginparwidth}{4cm}
\fi

\usepackage[utf8x]{inputenc}
\usepackage{amssymb}
\usepackage{listings}
\def\lstlanguagefiles{lstlean.tex}
\lstset{language=lean}

\usepackage{color}
\definecolor{keywordcolor}{rgb}{0.7, 0.1, 0.1}   % red
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4}   % grey
\definecolor{symbolcolor}{rgb}{0.0, 0.1, 0.6}    % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}      % green

\usepackage{listings}


%%% PACKAGES
\usepackage{inputenc}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float

\usepackage{textcomp}


% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{upgreek}
\usepackage{tikz-cd}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{corol}{Corollary}[theorem]
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}[theorem]
\theoremstyle{definition}
\newtheorem{sublemma}{Lemma}[theorem]
\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}
\theoremstyle{definition}
\newtheorem{subdef}{Definition}[theorem]
\newtheorem{exmpl}{Example}[theorem]

\usepackage{hyperref}

%%% END Article customizations

%%% The "real" document content comes below...

\title{A Lean tactic to solve the word problem in One Relator Groups}
\author{Christopher Hughes}
\begin{document}

\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}
% This paper describes an implementation of Magnus' method for solving the
% word problem in one-relator groups \cite{mccool_schupp_1973}\cite{PutmanOneRelator}.
% This implementation is used as part of a Lean tactic to prove equalities in groups
% given a single equality as a hypothesis; for example, it might prove that $abab^2 = 1$ implies $ab = ba$.

This paper describes two methods for automating proofs of equalities in groups, given
a number of equalities as hypotheses. The first section describes Magnus' method
\cite{mccool_schupp_1973}, which decides the word problem in one-relator groups.

The second method is capable of using more than one equality as a hypothesis,
with the caveat that it may not terminate if the equality it is asked to prove is false.
It is a known result that the word problem for finitely presented groups is undecidable
\cite{collins1986}, however it is semidecidable meaning that  there is an algorithm
that will terminate with a proof whenever an equality is true, but will not necessarily
terminate when an equality does not follow from the hypotheses.

In order to be used as part of a Lean tactic, the implementation of the
algorithm must not only decide the problem, but also produce a proof certificate
that can be converted into a Lean proof to be checked by Lean's kernel. The Lean
tactic can prove that two words in a one relator group are equal, but not that they are unequal.



\section{Introduction to Lean}

Lean \cite{de_moura_kong_avigad_van_doorn_von_raumer_2018} is a proof assistant; it is a language capable of expressing mathematical propositions
and definitions, and also a language for writing formal proofs of these propositions which
can then be checked by Lean's kernel. A formal proof checked by Lean's kernel provides
an extremely high level of confidence in a checked proof, at the expense of requiring a lot
of time for a user to write a proof in a completely formal language.

As well as being a proof assistant, Lean is also a programming language. The definitions
that are written in Lean are also executable programs, and Lean can be used to prove correctness
of programs written in Lean as well as pure mathematical proofs. Lean can
be used as a programming language to write Lean tactics, described in the next section.

There is a large library of formal mathematics in Lean, called mathlib
\cite{ThemathlibCommunity2020}. As of January 2021 this contains 470000 lines of code.

\subsection{Terms and Types}

Lean is based on type theory, every expression has a type,
including types themselves.
The notation \lstinline{x : y} indicates that
\lstinline{x} has type \lstinline{y}.

Lean makes use of the Curry Howard correspondence, every proposition is a type,
and the inhabitants of the proposition can be thought of as proofs of the proposition.
The main difference between propositions and other types, is that every element
of a proposition is equal to itself.

If $P$ and $Q$ are propositions and it is known that $P$ implies $Q$, and
$P$ is true, then $Q$ is also true. T

By the Curry Howard correspondence, theorems can be regarded as functions, and the
implication arrow \lstinline{→} is the same as the arrow defining function types.

For example if \lstinline{X} and \lstinline{Y} are types, then
\lstinline{f : X → Y} could be read as \lstinline{f} is a function from
\lstinline{X} to \lstinline{Y}. But if \lstinline{X} and \lstinline{Y} are propositions,
then \lstinline{f} is a proof that \lstinline{X} implies \lstinline{Y}.

Suppose \lstinline{X} and \lstinline{Y} are propositions, and we have \lstinline{x : X},
and \lstinline{f : X → Y}, i.e. \lstinline{X} is true,
and \lstinline{X → Y} is also true. Then by the modus ponens rule, we know \lstinline{Y}
is also true. The proof of this, the term of type \lstinline{Y}, is simply \lstinline{f x}
(Function application can be written without brackets in Lean).
The same syntax would be used if \lstinline{X} and \lstinline{Y} were Types,
then if \lstinline{x : X} and \lstinline{f : X → Y} then applying \lstinline{f}
to \lstinline{x}, gives \lstinline{f x : Y}.

Similarly the forall quantifier corresponds to Pi-types, or the type of dependent functions,
where the output type depends on the input to the function.
For example if \lstinline{s : set ℕ} is a set of natural numbers,
and \lstinline{h : ∀ x : ℕ, x ∈ s}, is a proof that every natural number is contained
in this set, then this function applied at \lstinline{2} gives \lstinline{h 2 : 2 ∈ s},
the proof in the special case when \lstinline{x} is \lstinline{2}. In fact the
symbols \lstinline{∀} and \lstinline{Π} are interchangeable in Lean, it is only a
convention that \lstinline{∀} should be used for propositions and \lstinline{Π} for types.

We have seen that the function arrow corresponds to the implication arrow. There
are other correspondences between logical connectives, and operators for making types.
For example logical "and", \lstinline{∧} corresponds to the product of types,
logical "or" \lstinline{∨} correspond to the disjoint union or coproduct of types.

\subsection{Tactics}

Because of the difficulty of writing a formal proof in Lean directly, it is possible
to write tactics in Lean. The tactic framework is a domain specific language for writing
Lean expressions, which include Lean proofs. Tactics will return a Lean expression,
which will then be checked by the Lean kernel. The tactic itself is not proven correct,
it is possible that a tactic returns a proof that is not accepted by the Lean kernel.

When using tactics to write a Lean proof, a tactic state is displayed to a user.
This tactic state can be manipulated using tactics. An example of a tactic state is displayed
below.

\begin{exmpl}

The code below is a statement of the theorem that if $G$ is a group, and
$a, b ∈ G$ are such that $ab = b^2a$, then for any natural number $n$,
$a^nba^{-n} = b^{2^n}$. Between the \lstinline{begin ... end}, the user can
write tactics to produce a proof of the proposition.

\begin{lstlisting}
  example {G : Type*} [group G] (a b : G) (h : a * b = b ^ 2 * a) (n : ℕ) :
    a ^ n * b * a ^ -↑n = b ^ 2 ^ n :=
  begin

  end
\end{lstlisting}
If the user types the above code then the following tactic state is displayed to the user.

\begin{lstlisting}
  1 goal
  G : Type u_1,
  _inst_1 : group G,
  a b : G,
  h : a * b = b ^ 2 * a,
  n : ℕ
  ⊢ a ^ n * b * a ^ -↑n = b ^ 2 ^ n
\end{lstlisting}

The turnstile \lstinline{⊢} indicates the current goal, the proposition the user
is currently trying to prove. The preceding lines indicate the hypotheses and variables.
\lstinline{G : Type u_1} indicates that \lstinline{G} is a type in an arbitrary universe
\lstinline{u_1}. \lstinline{_inst_1} is a group structure on the Type \lstinline{G}.
A term of type \lstinline{group G}, is a tuple containing a binary operation on
\lstinline{G}, an identity, and an inverse function, as well as proofs that these
satisfy the group axioms. \lstinline{a} and \lstinline{b} are elements of
\lstinline{G}, \lstinline{h} is the hypothesis that \lstinline{a * b = b ^ 2 * a},
and \lstinline{n} is a natural number.

If the user writes a tactic, then the tactic state will change. If the user
wanted to prove this by induction on \lstinline{n} they could write

\begin{lstlisting}
  induction n with k ih
\end{lstlisting}

The tactic state after this line will now be

\begin{lstlisting}
  2 goals
  case nat.zero
  G : Type u_1,
  _inst_1 : group G,
  a b : G,
  h : a * b = b ^ 2 * a
  ⊢ a ^ 0 * b * a ^ -↑0 = b ^ 2 ^ 0

  case nat.succ
  G : Type u_1,
  _inst_1 : group G,
  a b : G,
  h : a * b = b ^ 2 * a,
  k : ℕ,
  ih : a ^ k * b * a ^ -↑k = b ^ 2 ^ k
  ⊢ a ^ k.succ * b * a ^ -↑(k.succ) = b ^ 2 ^ k.succ
\end{lstlisting}

The tactic split the goal into two cases, in the first case, labelled \lstinline{case nat.zero},
\lstinline{n} is zero, and in the second case, labelled \lstinline{case nat.succ}
\lstinline{n} is the successor of \lstinline{k}. In the second case, an induction
hypothesis \lstinline{ih} has been added.

The user could then use the \lstinline{rw} tactic to solve the first goal,
for example typing \lstinline{rw pow_zero} changes the goal to
\begin{lstlisting}
  ⊢ 1 * b * a ^ -↑0 = b ^ 2 ^ 0
\end{lstlisting}

The \lstinline{a^0} has been substituted
for \lstinline{1}; \lstinline{pow_zero} is the theorem that says if $a$ is an
element of any monoid, then $a^0= 1$.

To solve the first goal the \lstinline{simp} tactic can be used.
The \lstinline{simp} tactic applies as many simplification rules as possible.
These rules are lemmas in the library marked by the library authors as simplification
lemmas that are proofs of an equality where the right hand side is judged to be simpler
than the left hand side. For example there is a lemma simplifying \lstinline{1 * a} to
\lstinline{a}, or \lstinline{b ^ 1} to \lstinline{b}.

To solve the second goal, the \lstinline{group_rel} tactic described later in this document can be used.

\end{exmpl}

\section{Magnus' Method}

This section starts by defining and stating a few important results about both
HNN extensions and free groups. Next, the proof certificate is described, and then
the algorithm is described along with how to generate the proof certificates.

\subsection{Free Group}

The free group is implemented in Lean as the set of reduced words. An element of
the free group over a type $S$ of letters is a list of pairs $S \times \mathbb{Z}$,
the letter and the exponent.
A list of the exponent part of every element of the list is non zero, and
no two adjacent elements of the list have the same letter. The free group is the set of
reduced lists.

Multiplication of elements of the free group is implemented by appending the lists
whilst replacing any adjacent occurrences of $(s, m)$ and $(s, n)$ with $(s, m + n)$, and removing
any occurrence of $(s, 0)$. Inversion is given by reversing the list and negating
the exponent part of every pair. The identity is given by the empty list.

\begin{defn}[Length]\label{length}
  The length of a word $w$ in the free group is the sum of the absolute values of the exponent
  parts of each element of the corresponding reduced list.
\end{defn}

\begin{defn}[Cyclically Reduced]\label{cycred}
  A word $w$ in the free group is cyclically reduced if it cannot be made shorter
  by conjugating.
\end{defn}

We state the following theorem \textit{Freiheitsatz}. This theorem is an important
part of the correctness of Magnus' method. The proof is omitted.

\begin{theorem}[Freiheitsatz]\label{freiheitsatz}
  Suppose $F(S)$ is the free group over a set $S$ and $r$ is a a cyclically reduced
  word, and $T \subset S$ is a set of letters such that $r$ cannot be written
  using any letters in $T$. Then $T$ is a basis for a free subgroup of $F(S)$.
  \cite{mccool_schupp_1973}
\end{theorem}

\subsection{HNN Extensions}\label{HNN}

Magnus' method makes use of isomorphisms between one-relator groups and HNN extensions.
In this section we define an HNN extension of a group relative to an
an isomorphism between two subgroups.

\begin{defn}[HNN Extension]
  Given a group $G,$ subgroups $A$ and $B$ of $G$, and an isomorphism $\phi: A \to B$, we can define
  the \textit{HNN extension} relative to $\phi$ of $G$. Let $\langle t \rangle$ be an infinite cyclic
  group generated by $t$. The HNN extension is the coproduct
  of $G$ and $\langle t \rangle$ quotiented
  by the normal closure of the set $\{ta t^{-1} \phi(a^{-1}) | a \in A\}$
\end{defn}

\begin{defn}[HNN normal form]\label{HNNnormalform}
  Let $w = g_0t^{k_1}g_1t^{k_2}g_2 \cdots t^{k_n}g_n \in G \ast \langle t \rangle$.
  Then $w$ is in \textit{HNN normal form} if for every $i$, $k_i \ne 0$, $k_i > 0$
  implies $g_i \notin A$ and $k_i < 0$ implies $g_i \notin B$.
\end{defn}

Note that the HNN normal form is not unique; two words $w, v \in G \ast \langle t \rangle$
that are equal after mapping into the HNN extension and both in normal form might not be
equal as elements of $G \ast \langle t \rangle$. However, if $w \in G \ast \langle t \rangle $
maps to $1$ in the HNN extension
then the following lemma tells us that the unique HNN normal form for $w$ is $1$.

\begin{theorem}[Britton's Lemma]\label{britton}
  Let $w \in G \ast \langle t \rangle$.
  If $w$ is in HNN normal form and $w$ contains a $t$, then $w \ne 1$
  \cite{CharlesF.Miller1968OBTA}
\end{theorem}

\begin{corol}\label{genbritton}
  If a word $w$ meets the conditions of Britton's Lemma,
  then $w$ cannot be written as a $t$ free word.
\end{corol}
\begin{proof}
Suppose $w = g$ with $g \in G$;
then $g^{-1}w$ also meets the conditions
in Theorem \ref{britton}, and therefore $gw^{-1}\ne 1$, contradicting $w = g$.
\end{proof}
Given a word $w \in G \ast \langle t \rangle$, the HNN normalization process
replaces any occurrences of $ta$ with $\phi(a)t$ when $a \in A$, and
any occurrence of $t^{-1}b$ with $\phi^{-1}(b)t^{-1}$ when $b \in B$.
Applying this rewriting procedure will always produce a word $w'$ in HNN normal form and
such that $w$ and $w'$ are equal after quotienting by the defining relations
of the HNN extension, $\{ta t^{-1} \phi(a^{-1}) | a \in A\}$.

The HNN normalization process describes an algorithm for deciding
whether two words $w, v \in G \ast \langle t \rangle$ are equal
after mapping into the HNN extension, by applying the normalization procedure to
 $wv^{-1}$.
In order to compute this algorithm, it is also necessary to have an algorithm for checking equality
of elements in $G$,
for checking whether an element of $G$ is in either of the subgroups $A$ or $B$, and for computing $\phi$.

\subsection{The Proof Certificate}\label{proofcert}

An element of a group $G$ is equal to $1$ in the quotient by the normal closure
of a relator $r$ if and only if it can be written as a product of conjugates of $r$ and $r^{-1}$.
More precisely, there is a group homomorphism $\textit{Eval}: F(G) \to G$ from the free group
over $G$ into $G$ that sends a basis element of $F(G)$,
$g \in G,$ to $grg^{-1} \in G$. The image of this map is exactly the kernel of the quotient map.
Therefore an element $p$ of $F(G)$ such that $\text{Eval}(p) = w$
can be seen as a witness that $w$ is in the kernel of the quotient map.

\begin{defn}(Eval)\label{Eval}
  $\text{Eval}(r)$ is a map $F(G) \to G$, sending a basis element $g \in G$ to $grg^{-1}$.
\end{defn}

The certificate is a pair of a normalised word $w'$ and
$p \in F(G)$ such that $w = \text{Eval}(p) w'$.

\begin{defn}[P functor]
  For any $g \in G$ define an automorphism $\text{MulFree}(g)$ of $F(G)$ by sending a basis
  element $h \in G$ to $gh$. This defines a left action of $G$ on $F(G)$.
  Define the group $P(G)$ to be
  \begin{equation}
  P(G) := F(G) \rtimes_{MulFree} G
  \end{equation}
  This group has multiplication given by $(a, b) (a', b') = (a \text{MulFree}(b)(a'), bb')$
\end{defn}

\begin{subdef}[lhs and rhs]
Define two group homomorphisms from $P$ into $G$: let
$\text{rhs}$ be the obvious map sending $(a, b)$ to $b$ and let
$\text{lhs}$ be the map sending $(a,b)$ to $\text{Eval}(a)b$.
Since $\text{Eval}(a)$ is in the kernel of the quotient map,
for any $p\in P(G)$, $\text{lhs}(p)$ and $\text{rhs}(p)$ are equal in the quotient by $r$.
Therefore an element $p$ of $P(G)$ can be regarded as a certificate of the congruence
$\text{lhs}(p) \equiv \text{rhs}(p) \text{ mod } r$.
\end{subdef}

One way of expressing this is that the group $G / r$ is the coequalizer of
the two surjective maps lhs and rhs

\begin{tikzcd}
  P(G) \arrow[rr, "lhs"', shift right] \arrow[rr, "rhs", shift left] &  & G \arrow[r] & G/r
\end{tikzcd}

Because both $\text{lhs}$ and $\text{rhs}$ are group homomorphisms, if $p \in P(G)$ is a certificate
of the congruence $a \equiv b \text{ mod } r$ and $q$ is a certificate of the congruence
$c \equiv d \text{ mod } r$, then $pq$ is a certificate of the congruence $ac \equiv bd \text{ mod } r$.
Similarly, $p^{-1}$ is a certificate of the congruence $a^{-1} \equiv b^{-1} \text{ mod } r$.

\begin{subdef}($P$ is functorial).
  Given a homomorphism $f: G \to H$,
  functoriality of the free group gives a natural map $F(f): F(G) \to F(H)$.
  Define the map $P(f): P(G) \to P(H)$ to send $(p, b) \in P(G)$ to $(F(f)(p), f(b)) \in P(H)$.
  Given a certificate of the congruence $a \equiv b \text{ mod } r$, this map returns
  a certificate of the congruence $f(a) \equiv f(b) \text{ mod } f(r)$.
\end{subdef}

\begin{subdef}(Trans)
  Given $p,q \in P(G)$ such that $p$ is a certificate of the congruence $a = b \text{ mod } r$
  and $q$ is a certificate of the congruence $b = c \text{ mod } r$, it is possible to define
  $\text{Trans}(p,q)$ such that $\text{Trans}(p,q)$ is a certificate of the congruence $a = c \text{ mod } r$.
  If $p = (p_1, p_2)$ and $q = (q_1, q_2)$, then $\text{Trans}(p,q) = (p_1q_1, q_2)$.
\end{subdef}

\begin{subdef}(Refl)
  Given $a \in G$, $(1, a)$ is a certificate of the congruence $a = a \text{ mod } r$. Call
  this $\text{Refl}(a)$.
\end{subdef}

It is also possible to define $\textit{Symm}$ such that $\text{lhs}(\text{Symm}(p)) = \text{rhs}(p)$
and vice versa, but this is not used in the algorithm.

\begin{subdef}(ChangeRel)
  Given a certificate $p$ of the congruence $a \equiv b \text{ mod } r$, it is possible
  to make a certificate of the congruence $a \equiv b \text{ mod } g r g^{-1}$ for any $g \in G$.
  For any $g \in G,$ let $\phi(g): F(G) \to F(G)$ be the map sending $h \in G$ to
  $hg$. Then $\text{ChangeRel}(g,(p_1,p_2))$ is defined to be $(\phi(g)(p_1), p_2)$ for
  $g \in G$ and $(p_1, p_2) \in P(G)$.
\end{subdef}

\subsubsection{Performance}


The representation of $F(F(S))$ can be improved. The automorphism $MulFree$ multiplies
every letter in a word by another word. The consequence is that many of the words in
$F(S)$ making up an element of $F(F(S))$ are very similar. Take the word $[w][v] \in F(F(S))$,
where $w, v, u \in F(S)$, and consider the element $\text{MulFree}(u)[w][v] = [uw][uv]$.
It is more efficient to only store the difference between adjacent letters, so the
element $[w][v]$ would be represented as the sequence $w, w^{-1}v$, and
the element $[uw][uv]$ would be represented as the sequence $uw, v^{-1}$.
If $u$ is a long word, which it often is, then this representation will usually be shorter.
The longer the word $x$ such that the algorithm is attempting
to prove $\overline{w} = 1$, the longer a typical length of $u$ is, so in fact
the standard representation tends to give certificates more of length or less quadratic in
the length of $x$.
This also has the advantage that to compute $MulFree$ only the first term in the sequence needs
to be changed.

An improved representation of the group $F(F(S))$ for a set $F(S)$, is to again
represent an element as a finite sequence of elements of $F(S) \times \mathbb{N}_{\ge 1}
\times\{-1,1\}$.

Define the set $X$ to be the set of finite sequences of elements of
$F(S) \times \mathbb{N}_{\ge 1}
\times\{-1,1\}$, with the property that there are no adjacent pairs of the form
$(g_i, n_i, s_i)(1, n_{i+1}, -s_i)$. A pair of this form will evaluate
to $(g_i r^{s_i})^{n_i}r^{-s_in_{i+1}} \dots g_i^{-n_i}$,
so there is a natural cancellation that can be made, whilst preserving the same
evaluation. This reduction corresponds to cancellation of inverses in
the usual representation of the free group. Later,
we will define a Reduce function on sequences.

Given a pair $(g, n, b) \in F(S) \times \mathbb{N}_{\ge 1} \times\{-1,1\}$,
define an element $f(g, n, b) \in P(F(S))$.

\begin{equation}
  \begin{cases}
    f(g, n, 1) := ([g], g) ^ n \\
    f(g, n, -1) := ((1, g^{-1})([g]^{-1}, 1))^{-n}
  \end{cases}
\end{equation}

Therefore given a sequence $(g_1, n_1, s_1), (g_2, n_2,s_2) \dots (g_k, n_k,s_k)$,
we can define the corresponding element of $F(F(S))$ to be

\begin{equation}
    \left(\prod_{i=1}^k f(g_i, n_i, s_i)\right)\left(\prod_{i=1}^k (1, g_i)^{-n_i}\right)
\end{equation}

The evaluation map into $F(S)$ can then be define as

\begin{equation}
  \text{Eval}((g_1, n_1, s_1), (g_2, n_2,s_2) \dots (g_k, n_k,s_k)) :=
  \left(\prod_{i=1}^k (g_ir^s_i)^{n_i}\right)\left(\prod_{i=1}^k g_i^{n_i}\right)
\end{equation}

One way of viewing this representation is that it stores a way of representing
as a sequence of applications of $h(g_i,s_i)$, to a word $w$, where each $n_i$
represents how many times the map $h(g_i, s_i)$ should be applied.

\begin{equation}
  h(g_i, s_i)(w) = g_ir^{s_i}wg_i^{-1}
\end{equation}

The map MulFree can be defined on this representation much more efficiently,
since only the first
element in the list need be changed.

\begin{equation}
  \begin{cases}
    \text{MulFree}(w)(1) := 1 \\
    \text{MulFree}(w)((g_1, 1, s_1), \dots ,(g_k, n_k, s_k)) := (wg_1, 1, s_1), \dots, (g_k, n_k, s_k)\\
    \text{MulFree}((g_1, 1, s_1),\dots ,(g_k, n_k, s_k)) :=
    (wg_1, 1, s_1),(g_1, n_1 - 1, s_1),\dots,(g_k, n_k, s_k) & \text{if }n_1 > 1
  \end{cases}
\end{equation}

Multiplication can also now be defined on this representation.

We can now define a reduction map to eliminate pairs of the form
 $(g_i, n_i, s_i)(1, n_{i+1}, -s_i)$.
\begin{defn}[Reduce]
  Reduce eliminates pairs of this form in a sequence with some rewriting rules.
  In this definition $S$ is some finite sequence.
  \begin{equation}
    \begin{cases}
    (g_i, 1, s_i), (1, 1, -s_i), S \to \text{MulFree}(g_i)(S) \\
    (g_i, n_i, s_i), (1, 1, -s_i), S \to (g_i, n_i - 1, s_i),
      \text{MulFree}(g_i)(S) & \text{if } n_i > 1\\
    (g_i, 1, s_i), (1, n_{i+1}, -s_i) \to
      (g_i, 1, -s_i), (1, n_{i+1} - 1, -s_i) &\text{if } n_{i+1} > 1 \\
    (g_i, n_i, s_i), (1, n_{i+1}, -s_i) \to
    (g_i, n_i - 1, s_i), (g_i, 1, -s_i), (1, n_{i + 1}, -s_i) &
      \text{if } n_i > 1\text{ and } n_{i+1} > 1
    \end{cases}
  \end{equation}
  % \begin{cases}
  %   %     (g_ig_{i+2}, 1, s_{i+2}), (g_{i+2}, n_{i+2} - 1, s_{i+2}) & n_i = n_{i+1} = 1 \\
  %   %     (g_i, n_i - 1, s_i), (g_i g_{i+2}, 1, s_{i+2}), (g_{i+2}, n_{i+2}, s_{i+2}) &
  %   %       n_i > 1 \text{ and } n_{i+1} = 1 \\
  %   %     (g_i, 1, -s_i), (1, n_{i+1}-1, -s_i), (g_i g_{i+2}, n_{i+2}, s_{i+2}) &
  %   %       n_i = 1 \text{ and } n_{i+1} > 1

  %   %   \end{cases}
  %   % \end{equation}
\end{defn}
% \begin{defn}[Reduce]
%   Reduce elimates pairs of this form

%   \begin{equation}
%     \begin{cases}
%       \text{Reduce}(S) = s & \text{The length of }s\text{ is less than or equal to }1 \\
%       \text{Reduce}((g_i, 1, s_i)(1, 1, -s_i), S) = \text{Reduce}{\text{MulFree}(g_i)(S) \\
%       \text{Reduce}((g_i, n_i, s_i)(1,1,-s_i), S) = \text{MulFree}
%     \end{cases}
%   \end{equation}
  % \begin{equation}
  %    (g_i, n_i, s_i), (1, n_{i+1}, -s_i), s
  % \end{equation}
  % and replaces it with the following
    % \begin{equation}
    %   \begin{cases}
    %     (g_ig_{i+2}, 1, s_{i+2}), (g_{i+2}, n_{i+2} - 1, s_{i+2}) & n_i = n_{i+1} = 1 \\
    %     (g_i, n_i - 1, s_i), (g_i g_{i+2}, 1, s_{i+2}), (g_{i+2}, n_{i+2}, s_{i+2}) &
    %       n_i > 1 \text{ and } n_{i+1} = 1 \\
    %     (g_i, 1, -s_i), (1, n_{i+1}-1, -s_i), (g_i g_{i+2}, n_{i+2}, s_{i+2}) &
    %       n_i = 1 \text{ and } n_{i+1} > 1

    %   \end{cases}
    % \end{equation}

Multiplication can be defined on this representation.
For a finite sequence $S := (g_1, n_1, s_1), \dots, (g_k, n_k, s_k)$
define
\begin{equation}
  p(S) := \prod_{i=1}^k g_i
\end{equation}

For a pair of sequence $S$ and $T$ use the notation $S,T$ to append the sequences.
Then the product of two sequences $S$ and $T$, $S\cdot T$ is defined to be
\begin{equation}
  S \cdot T = \text{Reduce}(S, \text{MulFree}(p(S), T))
\end{equation}

It may be sensible to store $p(S)$ as part of the data of a sequence $S$ so it does not
need to be recomputed every time sequences are multiplied.

As an illustration of the efficiency of this
representation of the free group, consider certificates of equalities
of the form $\overline(wr)^n \overline{w^{-n}} = 1$ for
large positive values of $n$,
have a much shorter representation.
In the efficient representation this certificate will
be
\begin{equation}
  (w, |n|, \text{sgn}(n))
\end{equation}
so the sequence
will be of length either $2$ or $1$. In the less efficient representation,
this certificate will be
\begin{equation}
  \prod_{i=1}^n [w^i]
\end{equation}

This certificate will be a sequence of length $n$ even after the word is reduced,
the overall data used will be quadratic in $n$, since the length of the letters
increases with the size of $n$ as well.

\subsection{Adding and Removing Subscripts}

Given a letter $t$ in the free group over a set $S$, we can define a map into a
semidirect product.

\begin{defn}[ChangeSubscript]\label{csub}
  Define a homomorphism ChangeSubscript from $\mathbb{Z}$ to the automorphism
  group of $F(S \times \mathbb{Z})$. If $(x, n) \in S \times \mathbb{Z}$ is a basis
  element of the free group, then $\text{ChangeSubscript}(m)(x, n) = (x, m + n)$.
\end{defn}

\begin{defn}[AddSubscripts]\label{AddSubscripts}
  There is a homomorphism AddSubscripts($t$) from $F(S)$ into $F(S \times \mathbb{Z})
  \rtimes_{\text{ChangeSubscript}} \mathbb{Z}$ sending a basis element $s \in S$ to
  $(s, 0) \in F(S \times \mathbb{Z})$ when $s \ne t$ and sending $t$ to
  $(1, 1_\mathbb{Z}) \in F(S \times \mathbb{Z}) \rtimes \mathbb{Z}$. Loosely, this map
  replaces occurrences of $t^n a t^{-n}$ with $a_t$.
\end{defn}

The map AddSubscripts is only used during the algorithm on words $w$ when the sum of the
exponents of $t$ in $w$ is zero, meaning the result will always be of the form
$(w', 0_{\mathbb{Z}})$.

\begin{defn}[RemoveSubscripts]
  RemoveSubscripts sends a basis element of $F(S\times \mathbb{Z})$,  $(s, n) \in S\times \mathbb{Z},$
  to $t^n s t^{-n}$.
\end{defn}
RemoveSubscripts is a group homomorphism and if $r$ is a word such that $\text{AddSubscripts}(r)$ is of the form $(r', 0_\mathbb{Z})$,
then $\text{RemoveSubscripts}(r')=r$.

\subsection{Overview of The Method}

Given an element $w \in F(S)$ of a free group, a relation $r$ in the free group, and a subgroup of the free group
generated by a set of letters $T$,
we write $\overline {w}$ for the corresponding element in $F(S) / r$
and $\overline{T}$ for the image of the subgroup generated by $T$ in $F(S)/r$.

The algorithm decides whether $\overline{w} \in F(s) / r$ is in $\overline{T}$,
and if it is, returns an element $w'$ such that $\overline{w'} = \overline{w}$ and $w' \in T$.

We describe the implementation of a function $\textit{Solve}$ whose arguments are a word $w$
in the free group $F(S)$, a relator $r \in F(S)$,
and a subset $T$ of $S$. If there is a word $w' \in F(S)$
such that $w' \in T$ and $\overline{w} \in F(S) / r$ is
equal to $\overline{W'}$, then it returns an element $p$ of $P(F(S))$
such that $\text{lhs}(p) = w$ and $\text{rhs}(p) = w'$. It terminates
without returning anything if there is no such word.

Without loss of generality we can assume $r$ is cyclically reduced and
conjugate $r$ if this is not the case. We can use $\text{ChangeRel}$,
to make the correct proof certificate after conjugating $r$.



\subsubsection{Case 1: All letters in r are in T}\label{allinT}
For this case it is helpful to consider the group $F(S)$ as the
coproduct of the subgroup generated by the letters in $T$ and
the subgroup generated by the rest of the letters:
$F(S) \cong F(T) \ast F(S \backslash T)$.

Since every letter in $r$ is also in $T$ then $F(S) / r \cong F(T) / r \ast F(S \backslash T)$.
An element of $F(S)$ can therefore be written in the form
$w_0v_0w_1v_1 \dots w_nv_n$, where $w_i \in F(T)$ and $v_i \in F(S \backslash T)$.
The problem can be reduced to deciding whether an element of $w_i \in F(T)$ is equal to an element of the quotient.
To decide the word problem whenever $\overline{w_i} = 1$, perform this substitution and
then check whether the resulting word in $F(T) \ast F(S \backslash T)$ is in $T$.

\subsubsection{Case 2: There is a letter in r that is not in T}\label{xandt}
\textbf{Base Case}
The base case is the case where the relation $r$ is of the form $a^n$ with
$n \in \mathbb{Z}$, and $a$ a letter in $S$. It is straightforward to decide
the word problem in this group, since $F(S) / a^n$ is isomorphic
to the binary coproduct of $F(S \backslash \{a\})$ and $\mathbb{Z}/n\mathbb{Z}$.

\textbf{Case 2a: Letter with exponent sum zero}\label{expsumzero}
Apply the map $\text{AddSubscripts}(t)$ (Definition \ref{AddSubscripts}) to $r$.
Since the exponent sum of $t$ is equal to zero, $\text{AddSubscripts}(t)(r)$ is
of the form $(r', 0_\mathbb{Z})$.
The length (Definition \ref{length}) of the relation $r' \in F(S \times \mathbb{Z})$
is less than the length of $r$.
If $t \notin T$ and the exponent sum of $t$ in $w$ is not zero,
then $\overline{w} \notin \overline{T}$.
If $t \in T$, then $w$ can be written in the
form $w' t^n$ where $t$ has exponent sum zero in $w'$, and $w'$ is a word in $T$ if
and only if $\overline{w} \in \overline{T}$.

A naive approach would be to apply $\text{AddSubscripts}(t)$ to $w$, and solve the word
problem in $F(S \times \mathbb{Z})$ with respect to $r'$. However, the image of the normal closure of $r'$ under $\text{AddSubscripts}(t)$ restricted
to $F(S \times \mathbb{Z})$ is not the normal closure of $r'$; it is the normal closure of
the set of all relations of the form $\text{ChangeSubscript}(n)(r')$ for every $n$.

Pick $x \in S$ such that $x \ne t$,
  $x$ is a letter in $r$ and such that $t \in T$ implies $x \notin T$.
  If this is not possible, then apply the procedure in Section \ref{allinT}.
  We can assume that the first letter of $r$ is
$x$, since otherwise $r$ can be conjugated until the first letter is $x$.
Let $a$ and $b$ be respectively the
smallest and greatest subscript of $x$ in $r'$. Let $S'$ be the set
\begin{equation}
S' := \{(i_1, i_2) \in S \backslash \{t\} \times \mathbb{Z} \
| \ i_1 \ne x \vee a \le i_2 \le b \}
\end{equation}

Define two subsets of $S'$ by
\begin{equation}
  A := S' \backslash \{x_b\}
\end{equation}
\begin{equation}
  B := S' \backslash \{x_a\}
\end{equation}
Then there is an isomorphism $\phi$ between $A$ and $B$ given by
$\phi := \text{ChangeSubscript}(1)$.
We claim the group $F(S) / r$ is isomorphic to the HNN extension of $F(S') / r'$ relative to $\phi$.

The homomorphism $\alpha$ from $F(S)/r$ to the HNN extension sends a letter
$s \in S \backslash \{t\}$ to $s_0$ and
the letter $t$ to the stable letter $t$ of the HNN extension. Since $t s_i t^{-1} = s_{i+1}$ in
the HNN extension for $s_i \in S'$, $r$ is sent to $r'$ by this map so that $\alpha$ is well defined
on the quotient.

Now let $\beta$ send $s_i \in S'$ to $t^{i} s t^{-i}$ and the stable letter $t$ to $t$.
Again, $r'$ is sent to $r$ by $\beta$, and $\beta (t s_i t^{-1}) = t^{i+1} s t^{-(i+1)} =
\beta (\phi (s_i))$ so $\beta$ preserves the defining relations of the HNN extension and it
is well defined. It can be checked $\beta$ is a two sided inverse to $\alpha$, and thus $\alpha$
is an isomorphism.

We then apply the HNN normalization procedure, which will be described in detail in Section
\ref{HNNnorm}. We chose $x$ and $t$ such that either $x \notin T$ or $t \notin T$.

In either case, if $\overline{w}$ can be written as a word in $T$,
then an HNN normal form of $w$ will be of the form $g t^n$
with $g \in F(S') / r'$. In the case $x \notin T$, then because any word in
$F(S')$ not containing $x_i$ must be in $A \cap B$, there can be no occurrence of
$tg$ with $g \notin A$ or $t^{-1}g.$
If $t \notin T$, then it must be possible to write $w$ without $t$, so in fact it can be normalized
to $g \in F(S') / r'$. We can check whether any words in $F(S') / r'$ are in the subgroups
generated by $A$ or $B$ using
Magnus' method again for the shorter relation $r'$, and rewrite these words using the letters
in $A$ or $B$ when possible.

Once in the form $g t^n$ with $g \in F(S')$, it is enough to check that
$\overline{\text{RemoveSubscripts}(g)}$ can be written as a word in $T$.
If $t \in T$ then this amounts to solving the word problem for $r'$
and the set $T' := \{ s_i \in S' | s \in T, i \in \mathbb{Z} \}$. If $t \notin T$,
this amounts to checking that $n = 0$ and solving the word problem for
$r'$ and the set $T' := \{s_0 \in S' | s \in T\}$.

\textbf{Case 2.b: No letter with exponent sum zero}\label{noexpsumzero}

If there is no letter $t$ in $r$ with exponent sum zero, then choose $x$
and $t$ such that $x \ne t$ and such that if $t \notin T$ then $x \notin T$.
Let $\alpha$ be the exponent sum
of $t$ in $r$ and let $\beta$ be the exponent sum of $x$.

Then define the map $\psi$ on $F(S)$ by

\begin{equation}
  \psi(s) =
  \begin{cases}
     t^\beta & \text{if }s = t \\
     xt^{-\alpha} &\text{if } s = x \\
     s & \text{otherwise}
  \end{cases}
\end{equation}

The map $\psi$ descends to a map $\overline{\psi}: F(S) / r \to F(s) / \psi(r)$. The map
$\psi$ is equal to $\psi_1 \circ \psi_2$,
where $\psi_2$ and $\psi_1$ are defined as follows:

\begin{equation}
  \psi_1(s) =
  \begin{cases}
     xt^{-\alpha} &\text{if } s = x \\
     s & \text{otherwise.}
  \end{cases}
\end{equation}

\begin{equation}
  \psi_2(s) =
  \begin{cases}
     t^\beta & \text{if } s = t \\
     s & \text{otherwise}
  \end{cases}
\end{equation}

We have that $\overline{\psi_1}: F(S) / r \to F(r) / \psi_1(r)$ is an isomorphism, with inverse given by sending $x$ to $xt^\alpha$. Meanwhile,
$\overline{\psi_2}: F(S) / r$ to $F(r) / \psi_2(r)$
is also injective.
This is proven constructively in Theorem \ref{powproof}.
Hence $\overline{\psi}: F(S) / r$ to $F(r) / \psi(r)$ is injective.

The image of the subgroup generated
by $T$ under $\psi$ might not be the subgroup generated by a set of letters, but it is
always contained in $T$.
By the Freiheitsatz, if $\overline{\psi(w)}$ can be written
as a word $w'$ using letters in $T$ then this solution is unique.
Therefore, to check if $\overline{\psi(w)}$ is in the subgroup generated by
$\overline{\psi(T)}$, one can first write it as a word in $w' \in T $ if possible,
and then check if $w'$ is in $\psi(T)$.
The exponent sum of $t$ in $\psi(r)$ is $0$, so the problem of checking
if $\psi(w)$ can be written as a word in $T$ can be solved using
the method described in Section \ref{expsumzero} (Sort out this label).

If $t \in T$ then $\psi(T)$ is generated by
$T' := T \backslash \{t\} \cup t^\beta$. By the Freiheitsatz, if $\psi(w)$ can be written
as a word $w'$ using letters in $T$ then this solution is unique. Therefore,
to check if $\psi(w)$ is in the subgroup generated by $T'$, one can first
write it as a word in $w'$ in $T$ if possible, and then check that for every
occurrence of $t^k$ in $w'$, $k$ is a multiple of $\alpha$.

If $t \notin T$, then  $\psi(T) = T$.


% \subsection{Base Case}\label{BaseCase}

% The base case is the case where the relation $r$ is of the form $a^n$ with
% $n \in \mathbb{Z}$, and $a$ a letter in $S$. It is straightforward to decide
% the word problem in this group, since $F(S) / a^n$ is isomorphic
% to the binary coproduct of $F(S \backslash \{a\})$ and $\mathbb{Z}/n\mathbb{Z}$.
% However computing the appropriate proof term requires some explanation.

% To compute the proof term, we write a function that takes an unnormalized word
% $w \in F(S)$, and a normalized proof word with proof $p \in P(F(S))$, and returns
% a word $q \in P(F(S))$, such that $\text{lhs}(q) = w\text{lhs}(p)$ and
% $\text{rhs}(q)$ is a normalization of $w \text{rhs}(p)$. By normalized,
% we mean that there is no occurrence of $a^k$ where $k \ne 0$ is a multiple of $n$.

% We can normalize the word $a^k$ where $k$ is a multiple of $n$ to
% $([1]^(n / k), 1) \in P(F(S))$, where $[1] \in F(F(S))$, is the basis element
% corresponding to $1 \in F(S)$.

% \begin{defn}(BaseCaseCore)
%   $\text{BaseCaseCore}(w, p)$ is defined by recursion on the list representation of $w$.
%   If $w = 1$, then return $p$. If $\text{rhs}(p) = 1$ then,
% \end{defn}

\subsection{HNN normalization}\label{HNNnorm}

We first present a simplified version of the HNN normalization that does not compute
the proof certificates, and then explain how to compute the certficates at the
same time as normalization.

To compute the HNN normalized term,
first compute the following isomorphism from $F(S)$ into the binary
coproduct $F(S') \ast \langle t \rangle$, where $\langle t \rangle$ is an infinite
cyclic group generated by $t$.

\begin{defn}\label{tocoprod}
  Define a map on a basis element $i$ as follows
  \begin{equation}
    \begin{cases}
      i_0 \in S' & i \ne t \\
      t & i = t
    \end{cases}
  \end{equation}
\end{defn}

It is important that $a \le 0 \le b$, to ensure that the image
of this map is contained in $F(S' \times \langle t \rangle)$.

Then apply the HNN normalization procedure. For this particular HNN extension
$\phi$ is \textit{ChangeSubscript} (Definition \ref{csub}).
We work in the $F(S') \ast \langle t \rangle$, and apply the following rewriting rules.

For each occurrence of $tw$ where there is an $a \in A$ such that
$\overline{a} = \overline{w}$ replace $tw$ with $\phi(a)t$.

For each occurrence of $t^{-1}w$ where there is an $b \in B$ such that
$\overline{b} = \overline{w}$ replace $tw$ with $\phi^{-1}(b)t^{-1}$.

We can use $\textit{Solve}$ to check whether there is such $a$ and $b$ with
these properties.



% \begin{defn}[HNNNormalizeCore]
% The function \textit{HNNNormalizeCore} takes an unnormalized word $w$ and a normalized word $v$
% both in the binary coproduct. It will return an HNN normalized word $vw$. It is defined by cases.
% When $w = 1$, it returns $v$. When $w = gw'$ for some $g \in F(S')$ and $v = 1$, then it returns

%   \begin{equation}
%       \text{HNNNormalizeCore}(v, w) = \\
%       \begin{cases}
%         v & \text{if } w=1 \\
%         \text{HNNNormalizeCore}(n, w') & \text{if } w = gw' \text{ for some } g \text{ and } v = 1 \\
%         \text{HNNNormalizeCore}(vg, w') & \text{if } w = gw' \text{ for some } g \in F(S') \\
%         \text{HNNNormalizeCore}(v' \text{ChangeSubscript(k, {Solve}(a, A), w')) \text{if } w = aw'
%           \text{ for some } a \in A
%       \end{cases}
%   \end{equation}
% \end{defn}

\subsubsection{Computing Proof Certificates}\label{HNNPC}

To compute proof certificates a slight modification of the procedure described in Section
\ref{HNNnorm} is used.

First define a modification of Definition \ref{tocoprod}, from $F(S)$ into the binary
coproduct $P(F(S \times \mathbb{Z})) \ast \langle t \rangle$.

\begin{defn}\label{tocoprodP}
  Define a map on the basis as follows
  \begin{equation}
    \begin{cases}
      \text{Refl}(i, t^0) \in F(S' \times \langle t \rangle) & i \in S \text{ and } i \ne t \\
      t & i = t
    \end{cases}
  \end{equation}
\end{defn}

There is also a map $Z$ from $P(F(S \times \mathbb{Z})) \ast \langle t \rangle$ into
$P(F(S))$. This map is not computed as part of the algorithm, but is useful to define anyway.

\begin{defn}
  The map $Z$ sends $t' \in \langle t \rangle$ to $\text{Refl}(t) \in P(F(S))$.
  It sends $p \in P(F(S \times \mathbb{Z}))$ to $P(\text{RemoveSubscripts})(p) \in P(F(S))$
\end{defn}

The aim is to define a normalization process into that turns a word $w \in F(S)$ into
word $n \in P(F(S \times \mathbb{Z})) \ast \langle t \rangle$
such that after applying \textit{rhs}, the same word is returned as in the
normalization process described in Section \ref{HNNnorm}. We also want
$\text{lhs}(Z(n))$ to be equal to $w$, so we end up with a certificate that $w$
is equal to some normalized word.

\begin{defn}(conjP)\label{conjP}
  Let $(p, a) \in P(F(S \times \mathbb{Z}))$ and $k \in \mathbb{Z}$.
  Define \textit{ConjP} to map into $P(F(S \times \mathbb{Z}))$
  \begin{equation}
    \text{ConjP}(k, (p, a)) = (\text{MulFree}((t,0)^k, p), \text{ChangeSubscript}(k, a))
  \end{equation}
\end{defn}

\textit{conjP} has the property that
$\text{lhs}(Z(\text{conjP}(k, p))) = t^k \text{lhs}(Z(p))t^{-k}$,
and similarly for \textit{rhs}. Note that $\textit{conjP}$ maps into $P(F(S \times \mathbb{Z}))$
and not $P(F(S'))$, although $\textit{rhs}$ of every word computed
will be in $F(S')$.

The procedure described in Section \ref{HNNnorm}
replaced each occurrence of $wt^{-1}$ with \newline $t^{-1}\text{ChangeSubscript}(-1)(a)$,
where $a \in A$ was a word equal to $w \in F(S')$ in the quotient $F(S') / r'$.

To compute the certificates apply the following rewriting procedure:
for each occurrence of $tp$ where $\overline{\text{rhs}(p)} = \overline{a}$ for
some $a \in A$, and $q$ is a certificate of this equality,
replace $tp$ with $\text{ConjP}(1, \text{Trans}(p, q))t$

Similarly, for each occurrence of $t^{-1}p$ where $\overline{\text{rhs}(p)} = \overline{b}$ for
some $b \in B$ and $q$ is a certificate of this equality,
replace $t^{-1}p$ with $\text{ConjP}(-1, \text{Trans}(p, q))t^{-1}$

\subsubsection{Performance}

The order in which the rewriting rules are applied can have a big effect on the performance
of the algorithm.

\begin{exmpl}\label{ltrbad}
Suppose $r' = {x_1}{x_0}^{-2}$ and
$w = t^n x_1 t x_0^{-1}$, where $n > 0$.
Then $S'$ is the set $\{x_0, x_1\}$,
$A$ is the subgroup generated by $S' \backslash x_1$ and
$B$ the subgroup generated by $S' \backslash x_0$.
Suppose we first make the substitution $tx_0^{-1}$ to $x_1^{-1} t$;
then $w$ becomes $t^{n}x_1x_1^{-1}t = t^{n+1}$.
This is in HNN normal form.

Now consider trying the HNN normalization process from the left.
For any $m \in \mathbb{Z}$, $x_1^m = x_0^{2m}$,
so the HNN normalization proces will rewrite $tx_1^m to x_1^{2m}t$.
Therefore $t^nx_1$ will be rewritten to $x_1^{2^n}t^n$.
Hence $w$ gets rewritten to $x_1^{2^n} t^{n+1} x_0^{-1}$,
which then will eventually be rewritten to $t^{n+1}$. The maximum length
of $w$ during the normalization process was greater than $2^n$.
\end{exmpl}

Applying one rewrite rule first
might mean that another rewrite is unnecessary, or a call to \textit{Solve} is
given an easier problem.

\begin{exmpl}\label{rtlbad}
Consider the word $tw_0t^{-1}w_1$ with $w_0, w_1 \in F(S')$. In this situation
it is best to start by attempting to prove $\overline{w_0} \in \overline{A}$ in the quotient.
Applying the left hand rewrite first will put the word into HNN normal form
straight away; it will not be necessary to check $\overline{w_1} \in \overline{B}$.

Rewriting starting on the right first might give a word such as
$tw_0 \phi^{-1}(b)t^{-1}$, where $\overline{b} = \overline{w_1}$. But since
$\phi^{-1}(b) \in A$, checking whether $\overline{w_0\phi^{-1}(b)} \in \overline{A}$ is
no easier than checking $\overline{w_0} \in \overline{A}$ has not become
any easier. So in this example it is better to start rewriting on the left, and furthermore,
if $\overline{w_0} \notin \overline{A}$ then it will not be possible to eliminate the $t$'s, so the
algorithm can fail straight away without attempting more rewrites.

\subsubsection{Other Potential Improvements}

There are other potential improvements that could be made but it is not clear
what effect, positive or negative, they would have on performance. These are discussed
in this section.

A potential improvement is changing the definition of the set $S'$ in Section \ref{xandt},
to make the set as small as possible.
First define the set $X$
of letters that meet the criteria for $x$ from Section \ref{xandt}.

\begin{equation}
  X := \{ x \in S \backslash \{t\} |
    x \text{ is contained in }r \text{ and } \ (x \notin T \text{ or } t \notin T) \}
\end{equation}

For any $x \in X$, $a(x)$ and $b(x)$ would be defined in a similar way to $a$ and
$b$ in Section \ref{xandt}, as the maximum and minimum subscripts of $x$ in the word
$\text{AddSubscript}(t)(r)$. Then

\begin{equation}
  S' := \{(x, n) \in S \backslash \{t\} \times \mathbb{Z} \
| \ x \notin X \text{ or } a_x \le n \le b_x \}
\end{equation}

The sets $A$ and $B$ are then defined in an analogous way to in Section \ref{xandt}.

\begin{equation}
  A := S' \backslash \{(x, b (x)) \ |\  x \in X\}
\end{equation}
\begin{equation}
  B := S' \backslash \{(x, a (x)) \ |\  x \in X\}
\end{equation}

The sets $A$ and $B$ are smaller than the alternative definition, where the constraints
on $n$ are only applied to one letter $x$, rather than a set of letters $x$. The fact
that the set is smaller means that the rewrites in the HNN normalization process
are less likely to suceed. This may not seem like a good thing, but it is faster for
some examples.

Consider the following Example, a slight modification of Example \ref{ltrbad2}.

\begin{exmpl}\label{ltrbad3}
    Suppose $r = txt^{-1}(xy)^{-2}$,
    then $r' := \text{AddSubscript}(t)(r) = {x_1}(x_0y_0)^{-2}$ and
    suppose $w = t^n x_1y_1 t (x_0y_0)^{-1}$, where $n > 0$.

    Suppose $S'$ is defined to be the set $\{x_0, x_1, y_0, y_1\}$.
    Then the only rewrite possible in the HNN normalization
    process is to substitute $t(x_0y_0)^{-1}$ with
    $(x_1y_1)^{-1}t$ after which the word will already be in HNN
    normal form.

    If alternatively, $S'$ is defined to be just $\{x_0, x_1\}$,
    then it is also possible to perform a substitution on the left,
    rewriting $t^nx_1y_1$ to $t^{n-1}(y_1x_1)^2y_2$, after which it
    is still possible to perform many more rewrites on the left hand side,
    meaning the word will be normalized in many more steps.
\end{exmpl}

Whilst changing the definition of $S'$ potentially takes away potential
rewrites, in at least one example the option it took away was a bad
rewrite to make anyway.



% There are also examples of where starting on the left is more effective.
% Suppose $r'$ is the same by $w$ is now equal to
% $tx_0t^{-1}(x_0y_0)^2$. Starting on the left by rewriting $t x_0$ to $x_1 t$,
% $w$ will be rewritten to $x_1t t^{-1}(x_0y_0)^2 = x_1(x_0y_0)^2$.
% This was put into normal form without ever having to rewrite a word in $F(S')$.

% Starting on the right is more complicated;
% $t^{-1}(x_0y_0)^2$ must first be rewritten
% to $t^{-1}x_1y_1$ and then $x_0y_0t^{-1}$.
% After this $w$ becomes $tx_0^2 y_0t^{-1}$. This is then
% rewritten to $x_1^2 y_1$. Starting on the right required
% having to rewrite $(x_0y_0)^2$ to $x_1y_1$, invoking
% a recursive call to \textit{Solve}.
\end{exmpl}

\begin{exmpl}
  Consider the word $tw_0t^{-1}w_1tw_2$. Here it is not possible to determine the best order without
  considering what particular words $w_0, w_1, w_2$ are. Starting in the middle gives the word
  $tw_0\phi(b)w_2$, and since $w_2$ might not be in $B$, the first problem may have become easier.

  However starting on the left with the pair $tw_0$
  might also be the best thing to do, since starting on the left would
  make the second problem easier, giving the word $\phi(bw_1 t w_2)$.
\end{exmpl}

\begin{exmpl}\label{ltrbad2}
  Consider the word $tw_0tw_1$. Here it is best to apply the right hand rewrite first.
  Applying the left hand rewrite first will not make the right hand one any easier; the $t$'s will
  not cancel, but applying the right hand one first could make the left hand problem easier.
  After applying the right hand rewrite, the word would become $tw_0\phi(a)t$, where $a \in A$
  and $\overline{a} = \overline{w_1}$. It is possible that it is easier to check $\overline{w_0\phi(a)} \in \overline{A}$
  than to check both $\overline{w_0} \in \overline{A}$ and $\overline{phi}(a) \in \overline{A}$.
\end{exmpl}

Example \ref{rtlbad} and Example \ref{ltrbad2} give an optimal normalization order
for simple examples, with only two occurrences of a power of $t$.
It is not possible to generalize this to more complicated examples,
but some sensible heuristics are possible.

The implemented code normalises the word from left to right, always applying
a substitution at the leftmost position where one is possible. This was found to have better
performance than right to left normalisation, probably because the cancellation
in Example \ref{rtlbad} is more likely than the cancellation in the Example \ref{ltrbad}.

We describe below a potentially improved way of performing the rewrites in the
an order with good performance.

Consider a word $W := w_0t^{n_1}w_1 \dots t^{n_{k-1}}w_{k-1}t^{n_k}w_k$, and the
following two sets

\begin{equation}
    R^{+} := \left\{ i \ | \ \forall j, \sum_0^i n_i \le \sum_0^j n_j\right\}
\end{equation}
\begin{equation}
    R^{-} := \left\{ i \ | \ \forall j, \sum_0^i n_i \ge \sum_0^j n_j\right\}
\end{equation}

Let $I$ be the smaller of the two intervals $[\min R^+, \max R^+]$ and
$[\min R^{-}, \max R^-]$.

Within the interval $I$, let $Q$ be the set of pairs $t^{n_i}w_i$ such that
$\text{sgn}(n_i) \ne \text{sgn}(n_{i+1})$.

Then $Q$ has the property that if there are no applicable rewrites
in the set $Q$, then the word cannot be put into the form $wt^n$ with $w \in F(S')$.
This is because if no rewrite can be performed at the first and last pairs $t^{n_i}w_i$
in the interval, then no substitutions performed outside this interval will
make these rewrites possible.

The rewriting procedure always chooses a rewrite within the set $Q$, prioritising
pairs $w_it^{n_i}$ where $w_i$ has the least occurences of letters not in
the set $T$. These are likely to be the fastest problems to solve.
This heuristic mitigates the exponential behaviour described in Example \ref{ltrbad}.

Care must also be taken to ensure that equivalent problems are not attempted twice.
For example if a rewrite is attempted at a pair $t^{n_i}w_i$, where $0 < n_i$
and fails because $\overline{w_i} \notin A$ is not in the relevant subgroup,
and then later on after substitutions elsewhere, this pair becomes
$t^{n_i}w_i\phi^{-1}(w_k)$, then since $\phi^{-1}(w_k) \in A$, then a rewrite
is still not possible here so none should be attempted.



% \begin{lemma}\label{peakshrink}
%  . Then applying HNN normalization
%   rules to $W$ can never increase the value $\max_a \sum_{i=1}^a n_i$.
% \end{lemma}

% \textit{Proof of Lemma \ref{peakshrink}.} Blank for now. Kind of obvious.



% \textit{Proof of Theorem \ref{rewriteorderthm}.} There are two ways that $W$ may be normalized
% without $\overline{w_a} \in \overline{A}$. Either the problem is simplified from the left, and the $t$
% is cancelled by being rewritten on the left, or after some rewrites to the right of $w_a$, $w_a$
% ends up multiplied by $\phi(a)$, and where $\overline{w_a\phi(a)} \in \overline{A}$.

% In order for the pair $t^{n_k}w_k$ to be simplified from the left,
% $W$ must first be rewritten to
% $W' := w'_0t^{n'_1}w'_1 \dots t^{n'_{a-1}}w_{a-1}t^{n_a}w_a \dots
% t^{n'_{k-1}}w'_{k-1}t^{n'_k}w'_k$ where $n'_{a-1} < -n_a$. But then
% $\sum_{i=1}^{a-2} n'_i \ge \sum_{i=1}^a n'_i$, contradicting the condition
% on $a$ (This condition is preserved by applying rewriting rules to pairs other
% than $t^{n_a}w_a$).

% In order for the pair $t^{n_k}w_k$ to be simplified from the right, $W$ must first
% be rewritten to $W' := w'_0t^{n'_1}w'_1 \dots t^{n_a}w_a t^{n'_{a+1}}w_{a'+1} \dots
% t^{n'_{k-1}}w'_{k-1}t^{n'_k}w'_k$ where $n_{a+1} > 0$. But then
% $\sum_{i=1}^{a+1} n'_i > \sum_{i=1}^a n'_i$ again contradicting the condition on $a$.

% Theorem \ref{rewriteorderthm} gives a precise rewriting order that avoids the undesirable
% inefficiencies in Example \ref{ltrbad} and Example \ref{rtlbad}, the
% pair $t^{n_a}w_a$ that satisfies the condition in this theorem should be rewritten first.
% Rewriting using this order also has the advantage that provided the starting problem
% is true, \textit{Solve} only recursively calls itself on problems that are also true, which
% is an obvious performance benefit, and this means that \textit{Solve} can fail early
% as soon as it recursively calls itself on a problem that is not true.

% It is possible to determine the best order to apply the rewriting procedure.

% Where there is a word of the form $tw_0t^{-1}w_1$, then starting on the left
% might make the right hand problem easier, because the $t$s would cancel. It is never
% better to start on the right

% \begin{exmpl}\label{lastfirst}
% There are occasions when it is impossible that a rewrite might become
% easier or unnecessary if other rewrites are applied first. For example
% consider a word $twtv$, with $w, v \in F(S')$. Then rewriting $tw$ to
% $\phi(a)t$ where $a = w$ in $F(S') / r'$ does not make the second problem
% any easier, I will still have to check if $v$ is in $A$. However,
% starting with rewriting $tv$ to $\phi(a')t$ with $a' = v$ in $F(S') / r'$,
% changes the first problem to $tw\phi(a')$.
% Checking $w\phi(a') \in A$ might be an easier problem than checking $w \in A$
% and $\phi(a') \in A$, and will never be a harder problem. So in this example
% it makes sense to attempt the right hand rewrite first.

% Additionally if the
% right hand rewrite fails, i.e. $v \notin A$, then it will not be possible to
% put the word in the form $gt^n$ with $g \in F(S')$. This means we need not
% attempt to normalize the rest of the word since we are only interested in the result of
% HNN normalization if the word can be put into the form $gt^n$.
% \end{exmpl}

% The case in Example \ref{lastfirst} can be generalized.
% Consider a word \newline $t^{n_0}w_0 \dots t^{n_{k-2}}w_{k-2}t^{n_{k-1}}w_{k-1}t^{n_{k}}w_{k}t^d$.
% If for every $m > 0$, $\text{sign}(n_k)\sum_{i=0}^m n_{k - i} \ge 0$ then
% in order to put the word in the form $gt^n$ with $g \in F(S')$, then it will definitely
% be necessary to check $w_k$ is equal to a word  $a \in A$ in the quotient
% $F(S') / r'$ and rewrite $tw_k$ to $\phi(a)t$ so this
% rewrite should be attempted first.
% There is a similar condition when $n_k < 0$. Call this condition Condition 1.

% Consider a word
% $t^{n_0}w_0 \dots t^{n_{k-2}}w_{k-2}t^{n_{k-1}}w_{k-1}t^{n_{k}}w_{k}t^d$,
% then if for every $j \le i$,and for every $m > 0$, $\text{sign}(n_j)\sum_{l=j}^m n_l \ge 0$ and
% $i < k$, we say that $i$ satisfies Condition 2.
% When $t^{n_i}w_i$ satisfies Condition 2,
% normalizing this pair first will not make any of the rewrites to the right
% of $w_i$ any easier; there will no no cancellation of $t$s,
% so this rewrite should be attempted after anything to the right of it.



% \begin{defn}[thingy]
% For a pair $t^{n_i}w_i$, where $n_i > 0$ we define $\text{thingy}(n_i, w_i)$ to be the number of
% occurrences of letters in $w_i$ but not in $A$, if $n_i < 0$ it is the number of occurrences
% of letters in $w_i$ but not in $B$. The number of occurrences to be the sum of the absolute
% value of the exponents of these letters.
% \end{defn}

% The rewrites are applied with the following priority

% \begin{itemize}
%   \item Any pairs $t_nw_n$ satisfying Condition 1
%   \item Of the pairs that do not satisfy Condition 2, priority is given
%     to the pairs $t^n_iw_i$ with the smallest value of $\text{thingy}(n_i, w_i)$.
%   \item Out of the pairs that have the equal least value of $\text{thingy}$,
%     then attempt to rewrite the leftmost pair first.
% \end{itemize}


% \subsubsection{Proof Lengths}

% It is important to apply the rewriting procedure from left to right. This will usually
% produce shorter proof certificates. Consider normalizing $t'^{-1}wt'vt'^{-1}$ with $w, v \in F(S')$.
% Suppose we normalize right to left.
% $t'v$ is normalized to $pt'$ with $p \in P(F(S \times \mathbb{Z}))$.
% After that substitution the new word $t'^{-1}wp$, with $wp \in P(F(S \times \mathbb{Z}))$.
% Suppose $\text{rhs}(wp)$ is normalized to $q \in P(F(S \times \mathbb{Z}))$.
% Then the final normalized word is $\text{conjP}(1, \text{Trans (wp, q)})t'^{-1}$.

% The proof part of $\text{Trans}(wp, q)$ is $\text{MulFree}(w)(\text{Left}(p))\text{Left}(q)$.

% Normalizing the other way, we first normalize $t'^{-1}w$ to $p_2 t'^{-1}$ with
% $p_2 \in P(F(S \times \mathbb{Z}))$. Then the new word is $p_2vt^{-1}$. Suppose
% $\text{rhs} p_2 v$ is normalized to $q_2 \in P(F(S \times \mathbb{Z}))$.
% Then the final normalized word is $t'^{-1}\text{conjP}(-1, \text{Trans} (p_2v, q_2))$.

% The proof part of $\text{Trans} (p_2v, q)$ is $\text{Left}(p_2)\text{Left}(q)$. There is no
% use of \textit{MulFree} since $\text{Left}(v)=1$. On average using \textit{MulFree}
% will make words longer, and so the left to right normalization produces shorter proofs.
% The left to right normalization was found to produce shorter proofs in practice.

\subsection{Shortening Proofs}

Most of the tactic execution time is spent generating and checking the Lean proof,
and not on generating the proof certificate described in Section \ref{proofcert}.
An algorithm was defined which implemented some heuristics to shorten the certificates
produced. There are two heuristics used to do this.

There are three equalities that the heuristics make use of. Recall that
a certificate is an element of $P(F(S))$, which is a semidirect product of $F(S)$
and $F(F(S))$. Given an element $p \in F(F(S))$, we aim to find an element
$p' \in F(F(S))$ such that $\text{Eval}(p) = \text{Eval}(p')$ (Definition \ref{Eval}).
Given a relator $r$, and words $w, v \in F(S)$, the heuristics make use of the following equalities.

\begin{equation}\label{golf1}
  \text{Eval}([w]) = \text{Eval}([wr^n])
\end{equation}

\begin{equation}\label{golf2}
  \text{Eval}([w][v]) = \text{Eval}([wrw^{-1}v][w])
\end{equation}

\begin{equation}\label{golf3}
  \text{Eval}([w][v]) = \text{Eval}([v][vr^{-1}v^{-1}w])
\end{equation}

A total order $(<)$ is put on the set of words in $F(S)$, this order has
the property the if $\text{Length}(w) < \text{Length}(v)$, then
$r(w,v)$. It is not important what the relation is on words of the same length,
as long as it is a total order, but the Lean implementation uses a lexicographic ordering.

\begin{defn}[Golf\textsubscript{1}]
$\text{Golf}_1$ folds through a word $p \in F(F(S))$ and replaces each
letter $w \in F(S)$ with the least word of the form $wr^n$.
It also performs any cancellations that can be performed after
performing these substitutions, to return a reduced word $p' \in F(F(S))$.
In practice, there are very often cancellations
that can be performed after this normalization of each letter.
\end{defn}

\begin{defn}[Golf\textsubscript{2}]
  $\text{Golf}_2$ performs substitutions of the form in Equations \ref{golf2} and
  \ref{golf3}. If $wrw^{-1}v$ is less than $v$, it will perform the substition
  in Equation \ref{golf2}, and if $vr^{-1}v^{-1}w$ is less than $w$ it performs
  this substitution. It performs these substitiutions until no more can be made.
  Again, it performs any cancellations that can be performed to return
  a reduced word.
\end{defn}

\begin{defn}[Golf]
  For an element $(p, w) \in P(F(S))$, $\text{Golf}(p, w)$ is defined to be
  $((\text{Golf}_2 \circ \text{Golf}_1)(p), w)$.
\end{defn}

These heuristics were surprisingly effective at shortening the proof certificates.
As an extreme example, if $r = aba^{-11}b^4$, and
$w = a^{10}b^{-4}a^{11}b^{-1}ab a^{-11} b^5a^{-11}b a^{11}b^{-1}a^{-1}b^{-1}a^{-10}$,
then the certificate produced by $\text{Solve}$ that $\overline{w} =1$
 has length $72$ before shortening and length $4$ after shortening.
 This shortens the overall tactic execution from 54 seconds to around
 3 seconds, with only 120ms spent executing Golf.

The biggest benefit of the $Golf$ function, is that it reduces the number
of letters in a certificate $p \in F(F(S))$, not just the length of each letter
$w \in F(S)$. It does this because shortening each letter, is also
canonicalising the letters, making cancellation more likely. This is the motivation
for putting a total order on the set of letters $w \in F(S)$.
For example, if $w$ and $wr$ are the same length, but $w < wr$, then
the word $[w][wr]^{-1}$, would not be reduced if Golf only compared lengths of letters,
but would be is reduced to $1$ by using a total order.

% If $w \in F(S)$, then $\text{Eval}([w]) = wrw^{-1}$. Therefore, for any power of $r$, $r^n$,
% $\text{Eval}([wr^n]) = \text{Eval}([w])$. If $wr^n$ is shorter than $w$,
% then $[wr^n]$ is a shorter certificate of an equality than $[w]$. The first
% heuristic goes through a word $p \in F(F(S))$, and replaces any letter $w$ with
% $wr^n$ if it is shorter. It also performs any cancellations that can be performed after
% performing these substitutions. In practice, there are very often cancellations
% that can be performed after this normalization of each letter.

% Given a word of length $2$ in $F(F(S))$, $[w][v]$, then there are
% two ways of generating a certificate of the same equality.


\subsection{Heuristics}

A few heuristics are implemented when there is a simpler method than Magnus' method.
They are implemented in the following order

\begin{itemize}
  \item Check whether the word $w$ is already written using letters in $T$. If $w \in T$,
  then trivially $\overline{w} \in \overline{T}$

  \item If there is a letter $x$ in the relator $r$ such that $x \notin T$, but
  $x$ is in $w$ then by the Freiheitsatz, $\overline{w} \notin \overline{T}$,
  so the algorithm can fail straight away.

  \item If $w$ is not in the subgroup generated by $r$ and $T$ after abelianizing the free
    group, the algorithm can fail straight away.

  \item If the relation $r$ has exactly one occurrence of a letter, say $x$, then
    the problem can be solved by rearranging the equation $r = 1$ to the form
    $x = v$, where $v$ is a word not containing $x$, and making this substitution everywhere
    in $w$.
\end{itemize}

\subsubsection{Injectivity}\label{powproof}

The correctness of the algorithm relies on the fact that the map $\psi_2$ is an injective map.
Since $\psi_2$ is injective, if $p \in P(F(S))$ is a witness of the congruence
$\psi_2(a) \equiv \psi_2(b) \text{ mod } \psi_2(r)$, then there must exist a certificate
$q$ of the congruence $a = b \text{ mod }r$. The question is how to compute this. The proof
of this congruence given in \cite{PutmanOneRelator} relies on the fact that the canonical maps into an amalgamated product
of groups are injective. However the standard proof
seems to rely on the law of the excluded middle, so
it cannot be translated into an algorithm to compute $q$.

Suppose $p \in P(F(S))$ is a witness of the congruence
$\psi_2(a) = \psi_2(b) \text{ mod } \psi_2(r)$. It is not necessarily the case that $k$
is a multiple of $n$ in every occurrence of $t^k$ in $p$. For example
$p := ([t][tr^{-1}t^{-1}][t]^{-1}, 1) \in P(F(S))$ is a witness of the congruence
$r = 1$. Both $\text{lhs}(p)$ and $\text{rhs}(p)$ are in the image of
$\psi_2$ for $n = 2$, when $r$ is in the image of $\psi_2$,
but $p$ is not in the image of $P(\psi_2)$. However where there are
occurrences of $t$, they are all cancelled after $\text{lhs}$ is applied, in fact
one could remove every occurrence of $t$ from $p$ and still have a certificate of the same
congruence.

In practice, it is observed that whenever there is an occurence of $t^k$ it is always
the case that $t$ is a multiple of $n$. If this were true all the time, then a slightly
simpler algorithm could be possible.

\begin{defn}
  Given a word $w \in F(S)$, define the set of partial exponent sums of a letter $t \in S$ to
  be the set of exponent sums of all the initial words of $w$. For example, the partial
  exponent sums of $t$ in $t^n a t$ are the exponent sums of $t$ in
  $t^n$, $t^na$ and $t^nat$.
\end{defn}

\begin{defn}
  $h$ is a map $F(S) \to F(S \cup \{t'\})$, where $t'$ is some letter not in $S$.
  $h$ replaces every occurrence of $t^k$ with $t'^at^b$ in such a way that $a + n b = k$,
  and every partial exponent sum of $t'$ in $h(w)$ is either not a multiple of $n$,
  or it is zero.
\end{defn}

\begin{defn}
  $\theta$ is a group homomorphism $F(S \cup \{t'\})$. Let $s \in S$. Then
  \begin{equation}
    \theta(s) = \begin{cases}
      t & \text{if } s = t' \\
      t^n & \text{if } s = t \\
      s & otherwise
    \end{cases}
  \end{equation}
\end{defn}

$\theta$ and $h$ satisfy $\theta \circ h = \text{id}$. For any $w$ in $F(S)$,
$\theta(w) = \psi_2(w)$.

\begin{defn}(PowProof)\label{PowProof}
  \textit{PowProof} is a map $F(S) \to F(S)$. $\text{PowProof}(w)$ is defined to be
  $h(w)$, but with every occurrence of $t'$ replaced with $1$.
\end{defn}

\begin{theorem}\label{powproof}
  For any $p \in F(F(S))$ if \newline $\text{Eval}(\psi_2(r))(p) = \psi_2(w)$,
  then $\text{Eval}(r)(F(\text{PowProof})(p)) = w$.
\end{theorem}

\begin{sublemma}\label{powproof1}
  Consider $\prod_{i = 1}^a s_i^{k_i}$, as an element of the $F(S \cup \{t'\})$
  with $s_i \in S \cup \{t'\}$ (Note that this is not necessarily a reduced
  word; $k_i$ may be zero and $s_i$ may be equal to $s_{i+1}$).
  Suppose every partial product $\prod_{i=1}^b s_i^{k_i}$,
  with $b \le a$ has the property that if the exponent sum of $t'$ is a multiple
  of $n$, then it is zero. Suppose also that $\prod_{i = 1}^b s_i^{k_i}$ has the
  property that for every occurrence of $t'^k$ in the reduced product, $k$ is a multiple of
  $n$. Then the reduced word $\prod_{i = 1}^a s_i^{k_i}$
  can be written without an occurrence of $t'$.
\end{sublemma}

\textbf{Proof of Lemma \ref{powproof1}.} $\prod_{i=0}^a s_i^{k_i}$ can be written as a reduced
word $\prod_{i = 1}^{c} u_i^{k'_i}$ such that $k'_i$ is never equal to zero and
$u_i \ne u_{i+1}$ for any $i$. The set of partial products of this,
$\prod_{i = 1}^{c} u_i^{k'_i}$, is a subset of the set of partial products of
$\prod_{i=1}^a s_i^{k_i}$,
therefore the exponent sum of $t'$ in every partial product of $\prod_{i=1}^a s_i^{k_i}$, is
either $0$ or not a multiple of $n$. However, by assumption every occurrence $t'^k$ in
$\prod_{i=0}^a s_i^{k_i}$, $k$, is a multiple of $n$, so the exponent sum of $t'$ in every
partial product is $0$. So $\prod_{i=1}^a s_i^{k_i}$ does not contain $t'$.

\textbf{Proof of Theorem \ref{powproof}} \newline
If $\text{Eval}(\psi_2(r))(p)$ is in the image of $\psi_2$, then
$\text{Eval}(r)(F(h)(p))$ has the property
that for every occurrence of $t'^k$, $k$ is a multiple of $n$.
If $p' := F(h)(p)$,
then $\text{Eval}(r)(p')$ can be written as a product of the form in
Lemma \ref{powproof1}.
If $r' = \prod_i u_i^{l_i}$, then to write $\text{Eval}(r)(p')$ in this
form, send $\prod_i \left[\prod_{j = 1}^a s_{ij}^{k_j}\right]
\in P(F(S \cup \{t'\}))$, to
\begin{equation} \label{eq:bigprod}
  \prod_i \left(\left(\prod_{j = 1}^a {s_{ij}}^{k_j}\right) \left(\prod_j u_j^{l_j}\right)
  \left(\prod_{j = 1}^a {s_{i(a - j)}}^{-k_{a - j}}\right)\right)
\end{equation}
If all the nested products in Equation \ref{eq:bigprod} are appended into one long product,
then the product has the form in Lemma \ref{powproof1}. Therefore when the word is reduced
it will not contain $t'$ by Lemma \ref{powproof1}. This means that deleting all occurrences of
$t'$ will in $p'$ will not change $\text{Eval}(r)(p')$, and therefore
$\text{Eval}(r)(F(\text{PowProof})(p)) = \text{Eval}(r)(F(h)(p))$.
Applying $\psi_2$ to both sides gives
$\psi_2(\text{Eval}(r)(F(\text{PowProof})(p)) =
  \psi_2(\text{Eval}(r)(F(h)(p))) =
  \theta(\text{Eval}(r)(F(h)(p))) =
  \text{Eval}(\psi_2(r))(p)$.

\section{Efficiency of the Algorithm}

  The worst case performance of this algorithm is worse than any finite tower of exponents
  \cite{miasnikov2011word}. The more relevant question is what is the typical performance.

  \begin{defn}[Area of a Relation]
  For a finitely presented group $G := \langle S | R\rangle$, if $w \in F(S)$ is equal
  to $1$ in the quotient $G$, then we say it is a \textit{relation}.
  The \textit{area of a relation} is the smallest $N$ such that $w$ can be written
  in the form $\prod_{i=1}^N g_i r_i^{\epsilon_i} g_i^{-1}$, where $\epsilon_i = \pm 1$
  and $r_i \in R$ for all $i$.
  \end{defn}

  \begin{defn}[Dehn Function]
  For a finitely presented group $G := \langle S | R\rangle$, the Dehn function
  of the presentation $\text{Dehn}(n) \in \mathbb{N}$ is defined as the
  largest area of a relation of length at most $n$.
  \end{defn}

  The Dehn function puts a lower bound on the complexity of the one-relator algorithm.
  The area of a relation is by definition the length of the shortest certificate
  that the algorithm might produce, so the complexity of the algorithm is bounded above by
  the Dehn function of a relator.
  The group $\langle a, b | b a b^{-1} a b a^{-1} b^{-1} = a^2\rangle$ is such that
  $\text{Dehn}(n)$ is worse than any finite tower of exponents. This means that the
  complexity of the one relator algorithm is also worse than any finite tower of
  exponents.

  Not all groups have such a fast growing Dehn function. For example, if the relator
  is of the form $r^k$ with $|k| \ne 1$ then $\text{Dehn}(n) \le n$.
  Similarly, even in groups with a rapidly increasing Dehn function,
  there are words that do not have a large area as the worst case.

  So, even though the worst case behaviour is very bad, there are still potentially many
  problems that the algorithm could solve in a practical amount of time. The aim of this
  implementation was to have good performance on relations with a small area.
  A typical Lean tactic state will usually be used on problems where the author knows
  the solution, but simply needs automation to write a formal proof of the solution.
  These relations will usually have a very small area. The aim of this implementation
  was that the algorithm should have good performance on relations with a small area,
  but makes no attempt to solve problems where the area of the relation is very large.

\section{Graph Search Method}

  In this section we present an alternative method to solve word problems in groups.
  This method searches for a sequences of rewrites to prove an equality. This search will
  usually not terminate if there is no such sequence of rewrites. I conjecture that it
  will terminate whenever there is a such sequence of rewrites. It was inspired by an
  online solver written Kyle Miller, with some modifications since that solver
  would sometimes fail to prove true formulas.

\subsection{Outline}

  Given a set of relators $R$, the method first generates a set of rewriting rules
  from the set of relators. Given a relator, the algorithm generates all equalities
  that can be made using the generator such that there is one letter of the starting
  relator on the left hand side of the equality.
  For example, if a relator is $abab^2$, then the equalities generated are
  \begin{equation}\
    \begin{aligned}
      a = b^{-2}a^{-1}b^{-1}, & a^{-1} = bab^2 \\
      b = a^{-1}b^{-2}a^{-1}, & b^{-1} = ab^2a \\
      a=b^{-1}a^{-1}b^{-2}, & a^{-1}=b^2ab \\
      b=a^{-1}b^{-1}a^{-1}b^{-1}, & b^{-1} = baba
      b=b^{-1}a^{-1}b^{-1}a^{-1},& b^{-1} = abab
    \end{aligned}
  \end{equation}

  Given a starting word, the algorithm then generated all words that can be generated
  from this starting word rewrites using the above rules and adds these
  words to a set of leaves. The process is then repeated at the word in the set of
  leaves with the least cost. The cost function assigns a natural number to each word in
  the free group. In general, shorter words have a lower cost, but different cost functions
  are discussed in section TODO. Before being added to the set of leaves, each word is cyclically
  reduced as well, a word is replaced by the shortest of its conjugates.

  As an example, we could apply the first rewriting rule $a = b^{-2}a^{-1}b^{-1}$
  to $aba^{-1}b^{-1}$ and obtain the word
  $\color{red}b^{-2}a^{-1}b^{-1}\color{black}ba^{-1}b^{-1} =
  b^{-2}a^{-2}b^{-1}$ which is then added to the set of leaves. The process
  is then repeated from the word in the set of leaves with the least cost, taking care
  not to repeat any words, until the word is rewritten to $1$.

  In summary, the process is as follows given a word $w$ that we are trying to prove is equal
  to $1$.
  \begin{enumerate}
    \item Generate the set $G$ of all rewriting rules such that there is one letter on the
      left hand side that can be generated from the starting relators.
    \item We store two sets of words, a set of seen words $S$, and a set of leaves $L$.
    \item Add the word $w$ to the set of seen words leaves $S$
    \item Generate all new words that can be made by applying a rewrite rule in $G$,
      and add these words to the set $L$ after cyclically reducing them.
    \item Take the word $w$ in $L$ with the least cost.
      If it is equal to $1$ then stop. Otherwise, check whether this
      word is in $S$. If it is not, then remove
      it from $L$ and go to step 3, otherwise remove it from $L$ and repeat this step.
  \end{enumerate}

A slightly different approach to the above approach would be to check if
a word is in the set $S$ of seen words at step 4, and add it to both sets $S$ and $L$
if it was not in $S$, and neither set otherwise, and then there would be no need to check
if words were in $S$ at step 5, and the set $L$ would be much smaller. However, this was
found to be a lot slower, since so much time was spent checking whether words were in $S$.

\subsection{Cost Function}

The method mentions a cost function which was not defined yet. Two cost functions
were tested, one was simply the length of a word, the other effectively ordered words
by length first and then lexicographically.

The second cost function was defined as follows. Suppose $n$ is the total number of
letters in all words in $R$ and the target word $w$.

\subsection{Generating Proofs}
In order to generate proof terms the algorithm must keep track of what path was taken


\section{To Do}
\begin{itemize}
  \item Comparison with Z3/sledgehammer whatever
  \item Talk about the injectivity proof and how there is never in practice
    a $t$ that isn't $t^n$.
  \item Comparison of search method and one-relator method
  \item Cost functions for tree search
  \item No other implementations for Magnus' method
  \item Mention that source that said Knuth Bendix was no good, maybe compare
    other potential approaches
  \item Mention crappy justification for termination of search
\end{itemize}

\bibliography{references}

\end{document}
