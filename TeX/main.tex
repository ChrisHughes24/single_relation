\def\paperversiondraft{draft}
\def\paperversionblind{blind}

\ifx\paperversion\paperversionblind
\else
  \def\paperversion{blind}
\fi

% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[12pt]{article} % use larger type; default would be 10pt

\usepackage{longtable}
\usepackage{booktabs}
\usepackage{xargs}
\usepackage{xparse}
\usepackage{xifthen, xstring}
\usepackage{ulem}
\usepackage{xspace}
\usepackage{multirow}
\setlength {\marginparwidth }{2cm}
\usepackage{todonotes}
\bibliographystyle{amsalpha}
\makeatletter
\font\uwavefont=lasyb10 scaled 652
\DeclareSymbolFontAlphabet{\mathrm}    {operators}
\DeclareSymbolFontAlphabet{\mathnormal}{letters}
\DeclareSymbolFontAlphabet{\mathcal}   {symbols}
\DeclareMathAlphabet      {\mathbf}{OT1}{cmr}{bx}{n}
\DeclareMathAlphabet      {\mathsf}{OT1}{cmss}{m}{n}
\DeclareMathAlphabet      {\mathit}{OT1}{cmr}{m}{it}
\DeclareMathAlphabet      {\mathtt}{OT1}{cmtt}{m}{n}
%% \newcommand\colorwave[1][blue]{\bgroup\markoverwith{\lower3\p@\hbox{\uwavefont\textcolor{#1}{\char58}}}\ULon}
% \makeatother

% \ifx\paperversion\paperversiondraft
% \newcommand\createtodoauthor[2]{%
% \def\tmpdefault{emptystring}
% \expandafter\newcommand\csname #1\endcsname[2][\tmpdefault]{\def\tmp{##1}\ifthenelse{\equal{\tmp}{\tmpdefault}}
%    {\todo[linecolor=#2!20,backgroundcolor=#2!25,bordercolor=#2,size=\tiny]{\textbf{#1:} ##2}}
%    {\ifthenelse{\equal{##2}{}}{\colorwave[#2]{##1}\xspace}{\todo[linecolor=#2!10,backgroundcolor=#2!25,bordercolor=#2]{\tiny \textbf{#1:} ##2}\colorwave[#2]{##1}}}}}
% \else
% \newcommand\createtodoauthor[2]{%
% \expandafter\newcommand\csname #1\endcsname[2][\@nil]{}}
% \fi


%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1.05in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

\usepackage[utf8x]{inputenc}
\usepackage{amssymb}
\usepackage{listings}
\def\lstlanguagefiles{lstlean.tex}
\lstset{language=lean}

\usepackage{color}
\definecolor{keywordcolor}{rgb}{0.7, 0.1, 0.1}   % red
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4}   % grey
\definecolor{symbolcolor}{rgb}{0.0, 0.1, 0.6}    % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}      % green
\usepackage{listings}


%%% PACKAGES
\usepackage{inputenc}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float

\usepackage{textcomp}


% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{\leftmark}\chead{}\rhead{\rightmark}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{upgreek}
\usepackage{tikz-cd}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{corol}{Corollary}[theorem]
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}[theorem]
\theoremstyle{definition}
\newtheorem{sublemma}{Lemma}[theorem]
\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}
\theoremstyle{definition}
\newtheorem{subdef}{Definition}[theorem]
\newtheorem{exmpl}{Example}[theorem]
\usepackage{lscape}
\usepackage{hyperref}

%%% END Article customizations

%%% The "real" document content comes below...

\title{A Lean tactic to solve the word problem in One Relator Groups}
\author{Christopher Hughes}

\begin{document}

\maketitle
CID 01360659 \newline
Supervised by Kevin Buzzard
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}

This paper describes two methods for automating proofs of equalities in groups, given
a number of equalities as hypotheses. Section \ref{magnusmethod} describes Magnus' method
\cite{mccool_schupp_1973}, which decides the word problem in one-relator groups.
This method was first described by Wilhelm Magnus in 1932 (proper citation). Given a word in
a free group, the ``relator", it will decide whether any other word is in the
normal subgroup containing this word. This is equivalent to deciding first order
formulas in the language of groups of the form $t_1 = t_2 \implies t_3 = t_4$,
where the $t_i$ are all terms in the language of groups.
I am not aware of any other implementation of this method on a computer.

Section \ref{gsmethod} describes an alternative method for solving the word problem,
which we call the ``Graph Search Method".
This method is capable of using more than one equality as a hypothesis,
with the caveat that it may not terminate if the equality it is asked to prove is false.
It is a known result that the word problem for finitely presented groups is undecidable
\cite{collins1986}, however it is semidecidable meaning that  there is an algorithm
that will terminate with a proof whenever an equality is true, but will not necessarily
terminate when an equality does not follow from the hypotheses.

Both methods are implemented as tactics in the Lean proof assistant, meaning that
they will produce a formal proof of the result capable of being checked by the Lean
proof assistant.

\section{Introduction to Lean}

Lean \cite{de_moura_kong_avigad_van_doorn_von_raumer_2018} is a proof assistant;
it is a language capable of expressing mathematical propositions
and definitions, and also a language for writing formal proofs of these propositions.
A formal proof in Lean is checked for correctness by Lean's kernel.
A formal proof checked by Lean's kernel provides
an extremely high level of confidence in a checked proof, at the expense of requiring a lot
of time and experience for a user to be able to write a proof in a completely formal language.

As well as being a proof assistant, Lean is also a programming language. The definitions
that are written in Lean are also executable programs, and Lean can be used to prove correctness
of programs written in Lean as well as verifying mathematical proofs. Lean can
be used as a programming language to write Lean tactics, described in the next section.

There is a large library of formal mathematics in Lean, called mathlib
\cite{ThemathlibCommunity2020}. As of January 2021 this contains 470,000 lines of code.

\subsection{Terms and Types}

Lean is based on type theory; there are terms and types, and every term has a type.
In Lean, the syntax \lstinline{x : ℝ} indicates that \lstinline{x} has type \lstinline{ℝ};
the element \lstinline{x} is a real number.
Lean is based on type theory as opposed to set theory, so whilst on paper it might
be more common to write $x \in \mathbb{R}$, saying $x$ is an element of the set of real numbers, in Lean
you would usually write \lstinline{x : ℝ} in Lean instead.
So for any syntactically correct expression we write down in Lean, that term has a type,
and we use the colon \lstinline{:} to indicate the type of a term.
\lstinline{Type} itself is also a type, so for example the real numbers are a type,
so \lstinline{ℝ : Type},
and if \lstinline{x} is a real number then \lstinline{x : ℝ} or \lstinline{x + 1 : ℝ}.
Another very important type is the type of propositions \lstinline{Prop}.
The type of \lstinline{Prop} is \lstinline{Type}.

In Lean it is possible to make new types out of types. For example if \lstinline{X : Type}
and \lstinline{Y : Type}, then \lstinline{X × Y : Type}; this is the cartesian
product of the types \lstinline{X} and \lstinline{Y}. Importantly you can also make the type
of functions from \lstinline{X} to \lstinline{Y} using the arrow symbol,
\lstinline{X → Y : Type}. If \lstinline{f : X → Y} and \lstinline{x : Y}, then
\lstinline{f x : Y}, similar to how if $f : X \to Y$, then $f(x) \in Y$
in more conventional notation.

To define a binary function from a type \lstinline{X} to itself in Lean,
it is possible to give it the type \lstinline{X × X → X}. However, this is not
the most common way of defining binary functions, it is more common to use the
canonically isomorphic type \lstinline{X → (X → X)}.
This is called the curried form of the function. This type is usually written without
brackets as \lstinline{X → X → X}, and this will be parsed by Lean as the type
\lstinline{X → (X → X)}, brackets are automatically inserted to the right.
Functions of higher arity can be defined similarly.

We can also make the type of dependent functions between sets,
where the type of the output depends on the input. These are known as Pi types, and correspond
to indexed products in more familiar language.
If \lstinline{X : Type} and \lstinline{Y : X → Type}, then we can write
\lstinline{Π i : X, Y i : Type}, and then if \lstinline{i : X},
and \lstinline{f : Π i: X, Y x} then \lstinline{f i : Y i}.
In set theoretic notation, if $X$ is a set,
and $(Y_i)_{i \in X}$ is a family of sets indexed by $X$, then
$\Pi_{i \in X}, Y_i$ is the product of the $Y_i$, and if
$f \in \Pi_{i \in X}, Y_i$, then you might write $f_i \in Y_i$.

Finally we will introduce the lambda notation used to define functions. This
is similar to the $\mapsto$ symbol in conventional maths. You can write
\lstinline{λ x : ℝ, x + 1}, to define a function \lstinline{ℝ → ℝ},
which corresponds to the function $x \mapsto x + 1$. To define a function
in multiple variables, you can put two variables after the lambda, for example
\lstinline{λ x y : ℝ, x + y} defines a binary function with type \lstinline{ℝ → ℝ → ℝ}.

\subsubsection{Propositions as Types}

We now give some examples of propositions and theorems.

\begin{lstlisting}
∀ p q : Prop, p ∧ q → q ∧ p
\end{lstlisting}

The above proposition says that the ``and'' connective, \lstinline{∧}, is symmetric. Note that
we use the same arrow as the function arrow, \lstinline{→}, for implication.

Below is a proof of the above proposition.
\begin{lstlisting}
λ (p q : Prop) (h : p ∧ q), and.intro (and.right h) (and.left h)
\end{lstlisting}

Note that we use the lambda notation that we used to define functions. This is because Lean
makes use of the Curry-Howard correspondence. The proof of this theorem can in fact be
regarded as a function. The function takes two propositions \lstinline{p} and \lstinline{q},
and a proof of \lstinline{p ∧ q}, and returns a proof of \lstinline{q ∧ p}. The proof itself
is a term, which means it has a type, and the type of the proof is the statement of the proposition.

Now consider the following definition of the function
\lstinline{X × Y → Y × X} that swap the two elements. We want to define a function
that works for any two types \lstinline{X} and \lstinline{Y}, not just defining it for
a particular choice of types \lstinline{X} and \lstinline{Y}. Therefore, the
function should take a pair of types as an input as well. The full type of this function
is therefore

\begin{lstlisting}
Π X Y : Type, X × Y → Y × X
\end{lstlisting}

Now we can define a function of this type.

\begin{lstlisting}
λ (X Y : Type) (h : X × Y), prod.mk (prod.snd h) (prod.fst h)
\end{lstlisting}

Here, \lstinline{prod.mk} is the natural function of type \lstinline{Y → X → Y × X}.
\lstinline{prod.snd} is the natural map \lstinline{X × Y → Y}, and \lstinline{prod.fst}
is the natural map \lstinline{X × Y → X}.

This function definition looks very similar to the proof of the proposition
\lstinline{∀ p q : Prop, p ∧ q → q ∧ p}, and the type of the function
looks very similar to the proposition itself. This is an example of the Curry Howard
correspondence.
\lstinline{∀} for propositions corresponds to \lstinline{Π} for types,
logical and, \lstinline{∧}, corresponds to cartesian product, \lstinline{×}, and
\lstinline{prod.mk}, \lstinline{prod.fst} and \lstinline{prod.snd} correspond to
\lstinline{and.intro}, \lstinline{and.left} and \lstinline{and.right} respectively.

This way the same syntax that can be used to define terms of types such as \lstinline{ℝ → ℝ}
can also be used to write proofs of propositions, although proofs of propositions are usually
far more complex than terms of other types.

% \begin{lstlisting}
%   ∀ (a b : ℤ), a * (1 + b) - a = a * b
% \end{lstlisting}

% Informally, in this statement, \lstinline{R} is a commutative ring.
% \lstinline{R : Type}, means that \lstinline{R} is a \lstinline{Type}, or the type of \lstinline{R}
% is \lstinline{Type}. \lstinline{a} and \lstinline{b} are elements of \lstinline{R}, so we write
% \lstinline{a b : R}. We write \lstinline{[comm_ring R]} to state that \lstinline{R} is a commutative
% ring. Strictly speaking \lstinline{comm_ring R} is the type of commutative ring structures on the
% Type \lstinline{R}. Here we are quantifying over all commutative ring structures on \lstinline{R},
% one way of reading this theorem would be: \textit{For all types R, and for any commutative ring
% structure on R and for any a, b in R, $a(1+b) - a = a b$}.


% \subsection{Terms and Types}

% Lean is based on type theory, every expression has a type,
% including types themselves.
% The notation \lstinline{x : y} indicates that
% \lstinline{x} has type \lstinline{y}.

% Lean makes use of the Curry Howard correspondence, every proposition is a type,
% and the inhabitants of the proposition can be thought of as proofs of the proposition.
% The main difference between propositions and other types, is that all elements
% of a proposition are equal.

% If $P$ and $Q$ are propositions and it is known that $P$ implies $Q$, and
% $P$ is true, then $Q$ is also true. T

% By the Curry Howard correspondence, theorems can be regarded as functions, and the
% implication arrow \lstinline{→} is the same as the arrow defining function types.

% For example if \lstinline{X} and \lstinline{Y} are types, then
% \lstinline{f : X → Y} could be read as \lstinline{f} is a function from
% \lstinline{X} to \lstinline{Y}. But if \lstinline{X} and \lstinline{Y} are propositions,
% then \lstinline{f} is a proof that \lstinline{X} implies \lstinline{Y}.

% Suppose \lstinline{X} and \lstinline{Y} are propositions, and we have \lstinline{x : X},
% and \lstinline{f : X → Y}, i.e. \lstinline{X} is true,
% and \lstinline{X → Y} is also true. Then by the modus ponens rule, we know \lstinline{Y}
% is also true. The proof of this, the term of type \lstinline{Y}, is simply \lstinline{f x}
% (Function application can be written without brackets in Lean).
% The same syntax would be used if \lstinline{X} and \lstinline{Y} were Types,
% then if \lstinline{x : X} and \lstinline{f : X → Y} then applying \lstinline{f}
% to \lstinline{x}, gives \lstinline{f x : Y}.

% Similarly the forall quantifier corresponds to Pi-types, or the type of dependent functions,
% where the output type depends on the input to the function.
% For example if \lstinline{s : set ℕ} is a set of natural numbers,
% and \lstinline{h : ∀ x : ℕ, x ∈ s}, is a proof that every natural number is contained
% in this set, then this function applied at \lstinline{2} gives \lstinline{h 2 : 2 ∈ s},
% the proof in the special case when \lstinline{x} is \lstinline{2}. In fact the
% symbols \lstinline{∀} and \lstinline{Π} are interchangeable in Lean, it is only a
% convention that \lstinline{∀} should be used for propositions and \lstinline{Π} for types.

% We have seen that the function arrow corresponds to the implication arrow. There
% are other correspondences between logical connectives, and operators for making types.
% For example logical ``and", \lstinline{∧} corresponds to the product of types,
% logical ``or" \lstinline{∨} correspond to the disjoint union or coproduct of types.

\subsection{Tactics}

Now consider the following theorem called \lstinline{thm} and its proof.

\begin{lstlisting}
theorem thm (a b : ℤ) : a * (1 + b) - a = a * b :=
(mul_add a 1 b).symm ▸ (mul_one a).symm ▸ add_sub_cancel' a (a * b)
\end{lstlisting}

The proof of this theorem required explicitly invoking three elementary lemmas about commutative
rings, the distibutivity law, \lstinline{mul_add}, the right multiplicative identity,
\lstinline{mul_one}, and the lemma \lstinline{add_sub_cancel'},
which says that for any $x$ and $y$, $x + y - x = y$.
Proving more complicated equalities of ring expressions can often require a huge
number of explicit applications of elementary lemmas about rings. This is where Lean's
tactic framework becomes useful. A tactic is a program that helps to generate Lean proofs. There is
a tactic for proving equalities of ring expresions called \lstinline{ring}. Instead
of the above proof, a Lean user could simply write

\begin{lstlisting}
theorem thm (a b : ℤ) : a * (1 + b) - a = a * b :=
begin
  ring
end
\end{lstlisting}

The \lstinline{begin} and \lstinline{end} indicates that the user wants to use the tactic
framework to generate a proof. The \lstinline{ring} tactic has generated a proof term
which can be checked by Lean's kernel, saving the user the effort of writing a full proof
term themselves.
 Users can use more complicated combinations of tactics
to prove a theorem. When using tactics to write a Lean proof, a tactic state is displayed to a user.
This tactic state can be manipulated using tactics. An example of a tactic state is displayed
below.

\begin{exmpl}

The code below is a statement of the theorem that if $G$ is a group, and
$a, b ∈ G$ are such that $ab = b^2a$, then for any natural number $n$,
$a^nb = b^{2^n}a^n$. In between \lstinline{begin} and \lstinline{end} the user
can write some tactics.

\begin{lstlisting}
example {G : Type} [group G] (a b : G) (h : a * b = b ^ 2 * a) (n : ℕ) :
  a ^ n * b = b ^ 2 ^ n * a ^ n :=
begin

end
\end{lstlisting}

If the user types the above code then the following tactic state is displayed to the user.

\begin{lstlisting}
1 goal
G : Type,
_inst_1 : group G,
a b : G,
h : a * b = b ^ 2 * a,
n : ℕ
⊢ a ^ n * b = b ^ 2 ^ n * a ^ n
\end{lstlisting}

The turnstile \lstinline{⊢} indicates the current goal, the proposition the user
is currently trying to prove. The preceding lines indicate the hypotheses and variables.
\lstinline{G : Type} indicates that \lstinline{G} is a type.
\lstinline{_inst_1} is a group structure on the Type \lstinline{G}.
A term of type \lstinline{group G}, is a tuple containing a binary operation on
\lstinline{G}, an identity, and an inverse function, as well as proofs that these
satisfy the group axioms. \lstinline{a} and \lstinline{b} are elements of
\lstinline{G}, \lstinline{h} is the hypothesis that \lstinline{a * b = b ^ 2 * a},
and \lstinline{n} is a natural number.

If the user writes a tactic, then the tactic state will change. If the user
wanted to prove this by induction on \lstinline{n} they could write

\begin{lstlisting}
induction n with k ih,
\end{lstlisting}

The tactic state after this line will now be

\begin{lstlisting}
2 goals
case nat.zero
G : Type,
_inst_1 : group G,
a b : G,
h : a * b = b ^ 2 * a
⊢ a ^ 0 * b = b ^ 2 ^ 0 * a ^ 0

case nat.succ
G : Type,
_inst_1 : group G,
a b : G,
h : a * b = b ^ 2 * a,
k : ℕ,
ih : a ^ k * b = b ^ 2 ^ k * a ^ k
⊢ a ^ k.succ * b = b ^ 2 ^ k.succ * a ^ k.succ
\end{lstlisting}

The tactic split the goal into two cases, in the first case, labelled \lstinline{case nat.zero},
\lstinline{n} is zero, and in the second case, labelled \lstinline{case nat.succ}
\lstinline{n} is the successor of \lstinline{k}. The term \lstinline{k.succ} means \lstinline{k + 1},
the successor of \lstinline{k}. In the second case, an induction
hypothesis \lstinline{ih} has been added.

The user could then use the \lstinline{rw} tactic to solve the first goal,
for example typing \lstinline{rw pow_zero} changes \lstinline{a ^ 0}
to \lstinline{1} and the new goal after this line will be
\begin{lstlisting}
⊢ 1 * b = b ^ 2 ^ 0 * 1
\end{lstlisting}

\lstinline{pow_zero} is the theorem that says if $a$ is an
element of any monoid, then $a^0= 1$.

To solve the first goal the \lstinline{simp} tactic can be used.
The \lstinline{simp} tactic applies as many simplification rules as possible.
These rules are lemmas in the library marked by the library authors as simplification
lemmas that are proofs of an equality where the right hand side is judged to be simpler
than the left hand side. For example there is a lemma simplifying \lstinline{1 * a} to
\lstinline{a}, or \lstinline{b ^ 1} to \lstinline{b}.

To solve the second goal, the \lstinline{group_rel} tactic described
later in this document can be used, and it will finish off the proof.

\end{exmpl}

\section{Magnus' Method}\label{magnusmethod}

This section starts by defining and stating a few important definitions and results about both
HNN extensions and free groups.

The tactic for solving this method is split into three stages. Firstly, the Lean tactic state
is converted into a word problem on an efficient representation of the free group. The tactic must
compute the relator and the target word based on the tactic state; it must convert a Lean
expression representing an element of a efficient representation of the free group. This process is very similar
for both tactics described in this paper and is described in Section \ref{tacdescript}. Then
the problem is solved using an efficient representation of the free group, and a proof certificate
is computed. We call the function that performs this part $\textit{Solve}$.
 The proof certificate is not a Lean proof, but does contain enough information
about the solution to be able to be used to compute a formal Lean proof. This certificate is
described in Section \ref{proofcert}.

Throughout this section if $w \in F(S)$ is an element of the free group and $r \in F(S)$
is a relator we use the notation $\overline{w} \in F(S) / r$ for the element of the quotient
group $F(S) / r$ corresponding to $w$. Sometimes if $w, v \in F(S)$,
we write $w \equiv v \text{ mod } r$, to mean $\overline{w} = \overline{v}$.

\subsection{Free Group}

The free group is implemented in Lean as the set of reduced words. An element of
the free group over a type $S$ of letters is a list of pairs $S \times \mathbb{Z}$,
the letter and the exponent. We say that a list is reduced whenever
the exponent part of every element of the list is non zero, and
no two adjacent elements of the list have the same letter. The free group is the set of
reduced lists.

Multiplication of elements of the free group is implemented by appending the lists
whilst replacing any adjacent occurrences of $(s, m)$ and $(s, n)$ with $(s, m + n)$, and removing
any occurrence of $(s, 0)$. Inversion is given by reversing the list and negating
the exponent part of every pair. The identity is given by the empty list.

\begin{defn}[Length]\label{length}
  The length of a word $w$ in the free group is the sum of the absolute values of the exponent
  parts of each element of the corresponding reduced list.
\end{defn}

\begin{defn}[Cyclically Reduced]\label{cycred}
  A word $w$ in the free group is cyclically reduced if it cannot be made shorter
  by conjugating. For example for any $a, b \in S$,
  the element $aba^{-1}$ is not cyclically reduced
  because it is conjugate to $b$ which is a shorter word.
\end{defn}

We state the following theorem \textit{Freiheitsatz}. This theorem is an important
part of the correctness of Magnus' method. The proof is omitted.

\begin{theorem}[Freiheitsatz]\label{freiheitsatz}
  Suppose $F(S)$ is the free group over a set $S$ and $r$ is a a cyclically reduced
  word, and $T \subset S$ is a set of letters such that $r$ cannot be written
  using only letters in $T$. Then $T$ is a basis for a free subgroup of $F(S)$.
  \cite{mccool_schupp_1973}
\end{theorem}

\subsection{HNN Extensions}\label{HNN}

Magnus' method makes use of isomorphisms between one-relator groups and HNN extensions.
In this section we define an HNN extension of a group relative to an
an isomorphism between two subgroups.

\begin{defn}[HNN Extension]
  Given a group $G$, subgroups $A$ and $B$ of $G$, and an isomorphism $\phi: A \to B$, we can define
  the \textit{HNN extension} relative to $\phi$ of $G$. Let $\langle t \rangle$ be an infinite cyclic
  group generated by $t$. The HNN extension is the coproduct
  of $G$ and $\langle t \rangle$ quotiented
  by the normal closure of the set $\{ta t^{-1} \phi(a)^{-1} \ \mid \ a \in A\}$
\end{defn}

The map from $G$ into the HNN extension is injective. TO DO citation.
If $w \in G \ast \langle t \rangle$, we use the notation $\overline{w}$ to
for the element of the HNN extension that $w$ maps to.

\begin{defn}[HNN normal form]\label{HNNnormalform}
  Let $w = g_0t^{k_1}g_1t^{k_2}g_2 \cdots t^{k_n}g_n \in G \ast \langle t \rangle$.
  Then $w$ is in \textit{HNN normal form} if for every $i$, $k_i \ne 0$, $k_i > 0$
  implies $g_i \notin A$ and $k_i < 0$ implies $g_i \notin B$.
\end{defn}

Note that the HNN normal form is not unique; two words $w, v \in G \ast \langle t \rangle$
that are equal after mapping into the HNN extension and both in normal form might not be
equal as elements of $G \ast \langle t \rangle$. However, if $w \in G \ast \langle t \rangle $
maps to $1$ in the HNN extension
then the following lemma tells us that the unique HNN normal form for $\overline{w}$ is $1$.

\begin{theorem}[Britton's Lemma]\label{britton}
  Let $w \in G \ast \langle t \rangle$.
  If $w$ is in HNN normal form and $w$ contains a $t$, then
  $\overline{w} \ne 1$.
  \cite{CharlesF.Miller1968OBTA}
\end{theorem}

\begin{corol}\label{genbritton}
  If a word $w \in G \ast \langle t \rangle$ meets the conditions of Britton's Lemma,
  then $\overline{w}$ cannot be written as a $t$ free word.
  In other words if $v \in G \ast \langle t \rangle$ is such that
  $\overline{v} = \overline{w}$ then $v \notin G$.
\end{corol}
\begin{proof}
Suppose $\overline{w} = \overline{g}$ with $g \in G$;
then $g^{-1}w$ also meets the conditions
in Theorem \ref{britton}, and therefore
$\overline{gw^{-1}} \ne 1$, contradicting $\overline{w} = \overline{g}$.
\end{proof}
Given a word $w \in G \ast \langle t \rangle$, the HNN normalization process
replaces any occurrences of $ta$ with $\phi(a)t$ when $a \in A$, and
any occurrence of $t^{-1}b$ with $\phi^{-1}(b)t^{-1}$ when $b \in B$.
Applying this rewriting procedure will always produce a word $w'$ in HNN normal form and
such that $w$ and $w'$ are equal after quotienting by the defining relations
of the HNN extension, $\{ta t^{-1} \phi(a^{-1}) \ \mid \  a \in A\}$.

The HNN normalization process describes an algorithm for deciding
whether two words $w, v \in G \ast \langle t \rangle$ are equal
after mapping into the HNN extension, by applying the normalization procedure to
 $wv^{-1}$.
In order to compute this algorithm, it is also necessary to have an algorithm for checking equality
of elements in $G$,
for checking whether an element of $G$ is in either of the subgroups $A$ or $B$, and for computing $\phi$.

\subsection{The Proof Certificate}\label{proofcert}

If $w, v, r \in F(S)$, then the tactic writes a Lean proof of the equality
$\overline{w} = \overline{v}$ of elements in $F(S) / r$. The Lean proof is typically
a very complicated object, so the tactic first produces a simpler proof certificate which
contains enough information about why the elements $\overline{w}$ and $\overline{v}$ are equal
from which a formal Lean proof can be computed. This section describes that proof certificate,
and some operations on the type of proof certificates.

An element of a group $G$ is equal to $1$ in the quotient by the normal closure
of a relator $r \in G$ if and only if it can be written as a product of conjugates of
$r$ and $r^{-1}$.
More precisely, there is a group homomorphism $\textit{Eval}_r : F(G) \to G$ from the free group
over $G$ into $G$ that sends a basis element of $F(G)$,
$g \in G$ to $grg^{-1} \in G$. The image of this map is exactly the kernel of the quotient map.
Therefore an element $p$ of $F(G)$ such that $\text{Eval}_r(p) = w$
can be seen as a witness that $w$ is in the kernel of the quotient map.

The Lean tactic
for checking whether an element of $w \in F(S)$ are equal to $1$ in the quotient by $r \in F(S)$,
generates an element $p$ of $F(F(S))$ such that $\text{Eval}_r(p) = w$, which will be used to write
a formal Lean proof that $\overline{w} = 1$.

\begin{defn}(Eval)\label{Eval}
  $\text{Eval}_r$ is a map $F(G) \to G$, sending a basis element $g \in G$ to $grg^{-1}$.
\end{defn}

If $w, v, r \in G$, then the certificate that $\overline{w} = \overline{v}$ in $G / r$,
is the pair of an element
$p \in F(G)$ such that $w = \text{Eval}_r(p) w$, and the element $v \in F(S)$.
We call this set of pairs $P(G)$, and we now define
a group structure on this set of pairs.

\begin{defn}[The group P(G)]
  For any $g \in G$ define an automorphism $\rho(g)$ of $F(G)$ by sending a basis
  element $h \in G$ to $gh$. This defines a left action of $G$ on $F(G)$.
  Define the group $P(G)$ to be
  \begin{equation}
  P(G) := F(G) \rtimes_{\rho} G
  \end{equation}
  This group has multiplication given by $(a, b) (a', b') = (a \rho(b)(a'), bb')$
\end{defn}

We define the group $P(G)$ for an arbitrary group $G$, however in practice we only
ever use $P(G)$ when $G$ is a free group.

\begin{subdef}[lhs and rhs]
Define two group homomorphisms from $P$ into $G$: let
$\text{rhs}$ be the obvious map sending $(a, b)$ to $b$.
For an element $r \in G$, let $\text{lhs}_r$ be the map sending
$(a,b)$ to $\text{Eval}_r(a)b$.
Since $\text{Eval}_r(a)$ is in the kernel of the quotient map,
for any $p\in P(G)$, $\text{lhs}_r(p)$ and $\text{rhs}(p)$ are equal in the quotient by $r$.
Therefore an element $p$ of $P(G)$ can be regarded as a certificate of the congruence
$\text{lhs}_r(p) \equiv \text{rhs}(p) \text{ mod } r$.
\end{subdef}

One way of expressing this is that the group $G / r$ is the coequalizer of
the two surjective maps lhs and rhs.

% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAAUAKAcQEoQAvqXSZc+QigCM5KrUYs23QcJAZseAkQBMM6vWatEIbgHoAToNkwoAc3hFQAMzMQAtkjIgcEJNJBwACyxHHCQAWj8AmDooNhwAdwgomIRqBjoAIxgGdlENCRAGGBCQPXlDEAAdSpwYAA8cYAYAuAEAfQshJxd3RE9vX2pA4NDESOjYowSkidTCzOzc9XE2MywbANCygzZq2obgMxaBZW63Qa8fRC0BCgEgA
\begin{tikzcd}
  P(G) \arrow[r, "\text{lhs}_r", two heads, shift left] \arrow[r, "\text{rhs}"', two heads, shift right] & G \arrow[r] & G/r
\end{tikzcd}

Because both $\text{lhs}_r$ and $\text{rhs}$ are group homomorphisms, if $p \in P(G)$ is a certificate
of the congruence $a \equiv b \text{ mod } r$ and $q$ is a certificate of the congruence
$c \equiv d \text{ mod } r$, then $pq$ is a certificate of the congruence $ac \equiv bd \text{ mod } r$.
Similarly, $p^{-1}$ is a certificate of the congruence $a^{-1} \equiv b^{-1} \text{ mod } r$.

\begin{subdef}($P$ is functorial).
  Given a homomorphism $f: G \to H$,
  functoriality of the free group gives a natural map $F(f): F(G) \to F(H)$.
  Define the map $P(f): P(G) \to P(H)$ to send $(p, b) \in P(G)$ to $(F(f)(p), f(b)) \in P(H)$.
  Given a certificate of the congruence $a \equiv b \text{ mod } r$, this map returns
  a certificate of the congruence $f(a) \equiv f(b) \text{ mod } f(r)$.
\end{subdef}

\begin{subdef}(Trans)
  Given $p,q \in P(G)$ such that $p$ is a certificate of the congruence $a \equiv b \text{ mod } r$
  and $q$ is a certificate of the congruence $b \equiv c \text{ mod } r$. We can define
  $\text{Trans}(p,q)$ such that $\text{Trans}(p,q)$ is a certificate of the congruence
  $a = c \text{ mod } r$.
  If $p = (p_1, p_2)$ and $q = (q_1, q_2)$, then we define $\text{Trans}(p,q) := (p_1q_1, q_2)$.
\end{subdef}

\begin{subdef}(Refl)
  Given $a \in G$, $(1, a)$ is a certificate of the congruence $a = a \text{ mod } r$. Call
  this $\text{Refl}(a)$.
\end{subdef}

It is also possible to define $\textit{Symm}$ such that $\text{lhs}_r(\text{Symm}(p)) = \text{rhs}(p)$
and vice versa, but this is not used in the algorithm.

\begin{subdef}(ChangeRel)
  Given a certificate $p \in P(G)$ of the congruence $a \equiv b \text{ mod } r$, it is possible
  to make a certificate of the congruence $a \equiv b \text{ mod } g r g^{-1}$ for any $g \in G$.
  For any $g \in G,$ recall that $\rho(g): F(G) \to F(G)$ is the map sending $h \in G$ to
  $hg$ extended to a map on $F(G)$.
  Then $\text{ChangeRel}(g,(p_1,p_2))$ is defined to be $(\rho(g)(p_1), p_2)$ for
  $g \in G$ and $(p_1, p_2) \in P(G)$.

  In other words, $\text{ChangeRel}$ satisfies the following equations
  \begin{equation}
    \text{lhs}_{grg^{-1}}\left(\text{ChangeRel}(g, p)\right) = \text{lhs}_r(p)
  \end{equation}
  \begin{equation}
    \text{rhs}(\text{ChangeRel}(p)) = \text{rhs}(p)
  \end{equation}
\end{subdef}

\subsubsection{Performance}

The representation of $F(F(S))$ can be improved. In this section
we refer to elements of $F(F(S))$ as sentences, elements of $F(S)$ as words,
and elements of $S$ as letters.
 The automorphism $\rho$ multiplies
every word in a sentence by another word. The consequence is that many of the words in
$F(S)$ making up an element of $F(F(S))$ are very similar. Take the sentence $[w][v] \in F(F(S))$,
where $w, v, u \in F(S)$, and consider the element $\rho(u)[w][v] = [uw][uv]$. Both words
in this sentence begin with the word $u$, and it is often the case that in the sentences in $F(F(S))$
that come up in practice, many of their words start with the same letters.
It is more efficient to only store the difference between adjacent letters, so the
element $[w][v]$ would be represented as the sequence $w, w^{-1}v$, and
the element $[uw][uv]$ would be represented as the sequence $uw, w^{-1}v^{-1}$.
If $u$ is a long word, which it often is, then this representation will usually be shorter.
The longer the word $w$ such that the algorithm is attempting
to prove $\overline{w} = 1$, the longer a typical length of $u$ is, so in fact
the standard representation tends to give certificates more of length or less quadratic in
the length of $w$.
This also has the advantage that to compute $\rho$ only the first term in the sequence needs
to be changed.

We now describe in detail this representation. We again
represent an element of $F(F(S))$ as a finite sequence of elements of $F(S) \times \mathbb{N}_{\ge 1}
\times\{-1,1\}$.

Define the set $M$ to be the set of finite sequences of elements of
$F(S) \times \mathbb{N}_{\ge 1}
\times\{-1,1\}$, with the property that there are no adjacent pairs of the form
$(g_i, n_i, s_i)(1, n_{i+1}, -s_i)$. We will see later that a pair of this form will evaluate
to $(g_i r^{s_i})^{n_i}r^{-s_in_{i+1}} \dots g_i^{-n_i}$,
so there is a natural cancellation that can be made, whilst preserving the same
evaluation. This reduction corresponds to cancellation of inverses in
the usual representation of the free group. Later,
we will define a Reduce function on sequences.

Given a tuple $(g, n, b) \in F(S) \times \mathbb{N}_{\ge 1} \times\{-1,1\}$,
define an element $f(g, n, b) \in P(F(S))$.

\begin{equation}
  \begin{cases}
    f(g, n, 1) := ([g], g) ^ n \\
    f(g, n, -1) := ([g]^{-1}, g)^n
  \end{cases}
\end{equation}

To understand the map $f$ it is helpful to see what $\text{lhs}_r \circ f$ is for a given
$r \in F(S)$. It is straightforward to verify that.

\begin{equation}
  \begin{cases}
    (\text{lhs}_r \circ f)(g, n, 1) = (gr)^n \\
    (\text{lhs}_r \circ f)(g, n, -1) = (gr^{-1})^n
  \end{cases}
\end{equation}

Therefore given a sequence $(g_1, n_1, s_1), (g_2, n_2,s_2) \dots (g_k, n_k,s_k) \in L$,
we can define the corresponding element of $F(F(S))$ that is represented by this
sequence to be

\begin{equation}
    \left(\prod_{i=1}^k f(g_i, n_i, s_i)\right)\left(\prod_{i=1}^k (1, g_i)^{-n_i}\right)
\end{equation}

This is simply the left projection of the element $\left(\prod_{i=1}^k f(g_i, n_i, s_i)\right)$.

The evaluation map of a sequence, the map $M \to F(S)$, given an element $r \in F(S)$
can now be defined as

\begin{equation}
  \text{Eval}_r((g_1, n_1, s_1), (g_2, n_2,s_2) \dots (g_k, n_k,s_k)) :=
  \left(\prod_{i=1}^k (g_ir^s_i)^{n_i}\right)\left(\prod_{i=1}^k g_i^{-n_i}\right)
\end{equation}

One way of viewing this representation is that it stores a way of representing
as a sequence of applications of $h(g_i,s_i)$, to a word $w$, where each $n_i$
represents how many times the map $h(g_i, s_i)$ should be applied.

\begin{equation}
  h(g_i, s_i)(w) = g_ir^{s_i}wg_i^{-1}
\end{equation}

The map $\rho$ can be defined on this representation much more efficiently,
since only the first
element in the sequence needs to be changed. Recall that the map
$\rho$ defines an action of $F(S)$ on $F(F(S))$ by extending the left regular
action of $F(S)$ on itself to an action on $F(F(S))$.

\begin{equation}
  \begin{cases}
    \rho(w)(1) := 1 \\
    \rho(w)((g_1, 1, s_1), \dots ,(g_k, n_k, s_k)) := (wg_1, 1, s_1), \dots, (g_k, n_k, s_k)\\
    \rho(w)((g_1, n_1, s_1),\dots ,(g_k, n_k, s_k)) := \\
     \ \ \ \ \ \ (wg_1, 1, s_1),(g_1, n_1 - 1, s_1),\dots,(g_k, n_k, s_k) & \text{if }n_1 > 1
  \end{cases}
\end{equation}

Multiplication can also now be defined on this representation.

First we can define a reduction map to eliminate pairs of the form
 $(g_i, n_i, s_i)(1, n_{i+1}, -s_i)$.
\begin{defn}[Reduce]
  Reduce eliminates pairs of this form in a sequence with some rewriting rules.
  In this definition $m$ is an arbitrary finite sequence of triples. Each arrow $(\to)$ indicates
  that the if the sequence on the left appears as a subsequence, then
  that subsequence should be replaced by the sequence on the right.
  \begin{equation}
    \begin{cases}
    (g_i, 1, s_i), (1, 1, -s_i), m \to \rho(g_i)(m) \\
    (g_i, 1, s_i), (1, 2, -s_i) \to
      (g_i, 1, -s_i) &  \\
    (g_i, n_i, s_i), (1, 1, -s_i), m \to (g_i, n_i - 1, s_i),
      \rho(g_i)(m) & \text{if } n_i > 1\\
    (g_i, n_i, s_i), (1, 2, -s_i) \to (g_i, n_i - 1, s_i), (g_i, 1, -s_i) & \text{if } n_i > 1\\
    (g_i, 1, s_i), (1, n_{i+1}, -s_i) \to
      (g_i, 1, -s_i), (1, n_{i+1} - 2, -s_i) &\text{if } n_{i+1} > 2 \\
    (g_i, n_i, s_i), (1, n_{i+1}, -s_i) \to
    (g_i, n_i - 1, s_i), (g_i, 1, -s_i), (1, n_{i + 1} - 2, -s_i) &
      \text{if } n_i > 1, n_{i+1} > 2
    \end{cases}
  \end{equation}
  % \begin{cases}
  %   %     (g_ig_{i+2}, 1, s_{i+2}), (g_{i+2}, n_{i+2} - 1, s_{i+2}) & n_i = n_{i+1} = 1 \\
  %   %     (g_i, n_i - 1, s_i), (g_i g_{i+2}, 1, s_{i+2}), (g_{i+2}, n_{i+2}, s_{i+2}) &
  %   %       n_i > 1 \text{ and } n_{i+1} = 1 \\
  %   %     (g_i, 1, -s_i), (1, n_{i+1}-1, -s_i), (g_i g_{i+2}, n_{i+2}, s_{i+2}) &
  %   %       n_i = 1 \text{ and } n_{i+1} > 1

  %   %   \end{cases}
  %   % \end{equation}
\end{defn}
% \begin{defn}[Reduce]
%   Reduce elimates pairs of this form

%   \begin{equation}
%     \begin{cases}
%       \text{Reduce}(S) = s & \text{The length of }s\text{ is less than or equal to }1 \\
%       \text{Reduce}((g_i, 1, s_i)(1, 1, -s_i), S) = \text{Reduce}{\rho(g_i)(S) \\
%       \text{Reduce}((g_i, n_i, s_i)(1,1,-s_i), S) = \rho
%     \end{cases}
%   \end{equation}
  % \begin{equation}
  %    (g_i, n_i, s_i), (1, n_{i+1}, -s_i), s
  % \end{equation}
  % and replaces it with the following
    % \begin{equation}
    %   \begin{cases}
    %     (g_ig_{i+2}, 1, s_{i+2}), (g_{i+2}, n_{i+2} - 1, s_{i+2}) & n_i = n_{i+1} = 1 \\
    %     (g_i, n_i - 1, s_i), (g_i g_{i+2}, 1, s_{i+2}), (g_{i+2}, n_{i+2}, s_{i+2}) &
    %       n_i > 1 \text{ and } n_{i+1} = 1 \\
    %     (g_i, 1, -s_i), (1, n_{i+1}-1, -s_i), (g_i g_{i+2}, n_{i+2}, s_{i+2}) &
    %       n_i = 1 \text{ and } n_{i+1} > 1

    %   \end{cases}
    % \end{equation}

Multiplication can be defined on this representation.
For a finite sequence
\begin{equation}
  m := (g_1, n_1, s_1), \dots, (g_k, n_k, s_k)
\end{equation}
define
\begin{equation}
  v(m) := \prod_{i=1}^k g_i \in F(S)
\end{equation}

For a pair of sequence $m$ and $l$ use the notation $m ++ l$ to append the sequences.
Then the product of two sequences $m$ and $l$, $m \cdot l$ is defined to be
\begin{equation}
  m \cdot l = \text{Reduce}(m ++ \rho(v(m))(l))
\end{equation}

It may be sensible to store $v(m)$ as part of the data of a sequence $m$ so it does not
need to be recomputed every time sequences are multiplied.

As an illustration of the efficiency of this
representation of the free group, consider certificates of equalities
of the form $\overline{(wr)^n w^{-n}} = 1$. For
large positive values of $n$, they
have a much shorter representation using the method described in this section.
In the efficient representation this certificate will be
\begin{equation}
  (w, |n|, \text{sgn}(n))
\end{equation}
so the sequence
will be of length either $2$ or $1$. In the less efficient representation,
this certificate will be
\begin{equation}
  \prod_{i=1}^n [w^i]
\end{equation}

This certificate will be a sequence of length $n$ even after the word is reduced,
the overall data used will be quadratic in $n$, since the length of the letters
increases with the size of $n$ as well.

\subsection{Adding and Removing Subscripts}

We now define some maps on the free group that will be used as part of the Magnus'
method algorithm.

Given a letter $t$ in the free group over a set $S$, we can define a map into a
semidirect product. We use subscript notation for elements of $S \times \mathbb{Z}$,
the element $(s, n)$ will be written as $s_n$.

\begin{defn}[ChangeSubscript]\label{csub}
  Define a homomorphism ChangeSubscript from $\mathbb{Z}$ to the automorphism
  group of $F(S \times \mathbb{Z})$. If $s_n \in S \times \mathbb{Z}$ is a basis
  element of the free group, then $\text{ChangeSubscript}(m)(s_n) = (s_{m+n})$.
\end{defn}

\begin{defn}[AddSubscripts]\label{AddSubscripts}
  There is a homomorphism $\text{AddSubscripts}(t)$ from $F(S)$ into $F(S \times \mathbb{Z})
  \rtimes_{\text{ChangeSubscript}} \mathbb{Z}$ sending a basis element $s \in S$ to
  $s_0 \in F(S \times \mathbb{Z})$ when $s \ne t$ and sending $t$ to
  $(1, 1_\mathbb{Z}) \in F(S \times \mathbb{Z}) \rtimes \mathbb{Z}$. Loosely, this map
  replaces occurrences of $t^n s t^{-n}$ with $s_n$.
\end{defn}

The map AddSubscripts is only used during the algorithm on words $w$ when the sum of the
exponents of $t$ in $w$ is zero, meaning the result will always be of the form
$(w', 0_{\mathbb{Z}})$.

\begin{defn}[RemoveSubscripts]
  RemoveSubscripts sends a basis element of $F(S\times \mathbb{Z})$,  $s_n \in S\times \mathbb{Z}$,
  to $t^n s t^{-n}$.
\end{defn}
RemoveSubscripts is a group homomorphism and if $r$ is a word such that
$\text{AddSubscripts}(r)$ is of the form $(r', 0_\mathbb{Z})$,
then $\text{RemoveSubscripts}(r') = r$.

\subsection{Overview of The Method}

Given an element $w \in F(S)$ of a free group,
a relator $r$ in the free group, and a set of letter $T \subseteq S$,
we write $\overline {w}$ for the corresponding element in $F(S) / r$
and $F(T)$ for the subgroup of $F(S)$ generated by $T$,
and $\overline{F(T)}$ for the image of $F(T)$ in $F(S)/r$. IF $T = \emptyset$,
then this is equivalent to checking whether a word is equal to $1$ in the quotient
$F(S) / r$.

The algorithm decides whether an element $\overline{w} \in F(S) / r$ is in $\overline{F(T)}$,
and if it is, returns an element $w' \in F(T)$ such that $\overline{w'} = \overline{w}$.

We describe the implementation of a function Solve whose arguments are a word $w$
in the free group $F(S)$, a relator $r \in F(S)$,
and a subset $T$ of $S$. If there is a word $w' \in F(S)$
such that $w' \in F(T)$ and $\overline{w} \in F(S) / r$ is
equal to $\overline{w'}$, then it returns an element $p \in P(F(S))$
such that $\text{lhs}_r(p) = w$ and $\text{rhs}(p) = w'$. It terminates
without returning anything if there is no such word.

Without loss of generality we can assume $r$ is cyclically reduced and
conjugate $r$ if this is not the case. We can use $\text{ChangeRel}$,
to make the correct proof certificate after conjugating $r$.

There are several cases to consider. The first is the case when all letters in $r$
are also in $T$, or $r \in F(T)$. Whenever $r \notin F(T)$, then there are two
cases to consider, the first is when there is a letter $t$ in $r$, such that
the sum of the exponents of $t$ in $r$ is zero.
The second case is when there is not such a letter $t$.

\subsubsection{Case 1: All letters in r are in T}\label{allinT}
For this case it is helpful to consider the group $F(S)$ as the
coproduct of the subgroup generated by the letters in $T$ and
the subgroup generated by the rest of the letters:
$F(S) \cong F(T) \ast F(S \backslash T)$.

Since every letter in $r$ is also in $T$ then $F(S) / r \cong F(T) / r \ast F(S \backslash T)$.
An element of $F(S)$ can therefore be written in the form
$w_0v_0w_1v_1 \dots w_nv_n$, where $w_i \in F(T)$ and $v_i \in F(S \backslash T)$.
To check if this word is in $\overline{F(T)}$, first check if the $w_i$ are equal
to $1$ in the quotient $F(T) / r$, and perform this substitution whenever possible,
and then check if after performing these substitutions the $v_i$ cancel each other.

The problem can therefore be reduced to deciding whether an element of
$w_i \in F(T)$ is equal to one in the quotient $F(S) /r$
To decide the word problem whenever $\overline{w_i} = 1$, perform this substitution and
then check whether the resulting word in $F(T) \ast F(S \backslash T)$ is in $T$.

\subsubsection{Case 2: There is a letter in r that is not in T}\label{xandt}

We proceed by induction on the length of the relation $r$.FC

\textbf{Base Case}
The base case is the case where the relation $r$ is of the form $a^n$ with
$n \in \mathbb{Z}$, and $a$ a letter in $S$. It is straightforward to decide
the word problem in this group, since $F(S) / a^n$ is isomorphic
to the binary coproduct of $F(S \backslash \{a\})$ and $\mathbb{Z}/n\mathbb{Z}$.

\textbf{Case 2a: Letter with exponent sum zero}\label{expsumzero}

Let $t$ be a letter in $r$ such that the sum of the exponents of $t$
in $r$ is zero.
Apply the map $\text{AddSubscripts}(t)$ (Definition \ref{AddSubscripts}) to $r$.
Since the exponent sum of $t$ is equal to zero, $\text{AddSubscripts}(t)(r)$ is
of the form $(r', 0_\mathbb{Z})$.
The length (Definition \ref{length}) of the relation $r' \in F(S \times \mathbb{Z})$
is less than the length of $r$. This is important since we will reduce the word problem
for $r$ to solving a word problem with respect to the relator $r'$,
and to be sure that this process terminates,
the length of $r'$ must be less than the length of $r$.
If $t \notin T$ and the exponent sum of $t$ in $w$ is not zero,
then $\overline{w} \notin \overline{F(T)}$.
If $t \in T$, then $w$ can be written in the
form $w' t^n$ where $t$ has exponent sum zero in $w'$,
and then $\overline{w'}$ is a word in $\overline{F(T)}$ if
and only if $\overline{w} \in \overline{F(T)}$.

A naive approach would be to apply $\text{AddSubscripts}(t)$ to $w$, and solve the word
problem in $F(S \times \mathbb{Z})$ with respect to $r'$.
However, the image of the normal closure of $r'$ under $\text{AddSubscripts}(t)$ restricted
to $F(S \times \mathbb{Z})$ is not the normal closure of $r'$; it is the normal closure of
the set of all relations of the form $\text{ChangeSubscript}(n)(r')$ for every $n$.

Pick $x \in S$ such that $x \ne t$,
  $x$ is a letter in $r$ and such that if $t \in T$ then $x \notin T$.
  If this is not possible, then apply the procedure in Section \ref{allinT}.
We can assume that the first letter of $r$ is
$x$, since otherwise $r$ can be conjugated until the first letter is $x$.
Let $a$ and $b$ be respectively the
smallest and greatest subscript of $x$ in $r'$. Because $x$ is the first
letter of $r$, we know $a \le 0$ and $b \ge 0$. Let $S'$ be the set
\begin{equation}
S' := \{(i_1, i_2) \in S \backslash \{t\} \times \mathbb{Z} \
| \ i_1 \ne x \vee a \le i_2 \le b \}
\end{equation}

Define two subsets of $S'$ by
\begin{equation}
  A := S' \backslash \{x_b\}
\end{equation}
\begin{equation}
  B := S' \backslash \{x_a\}
\end{equation}
Then there is an isomorphism $\phi$ between $A$ and $B$ given by
$\phi := \text{ChangeSubscript}(1)$. This isomorphism simply
increases the value of the subscript of each letter by $1$.
We claim the group $F(S) / r$ is isomorphic to the HNN extension of
$F(S') / r'$ relative to $\phi$.

To demonstrate this isomorphism we now define two maps $h$ and $h'$,
between $F(S)$ and the HNN extension of $F(S') \ast \langle t \rangle$ such that
the maps descend to well defined maps $\overline{h}$ and $\overline{h'}$, between the quotients
$F(S) / r$, and the HNN extension relative to $\phi$ which is the
quotient $F(S') \ast \mathbb{Z}$ by the normal closure of the set
$\{r'\} \cup \{\phi(s_i)ts_{i+1}^{-1}t^{-1} \ \mid \ s_i \in A\}$.
We will check that $\overline{h} \circ \overline{h'} = \text{id}$ and
$\overline{h'} \circ \overline{h} = \text{id}$

The homomorphism $h$ from $F(S)$ to $F(S') \ast \langle t \rangle$ sends a letter
$s \in S \backslash \{t\}$ to $s_0$ and
the letter $t$ to the stable letter $t$.
Since $t s_i t^{-1} = s_{i+1}$ in
the HNN extension for $s_i \in S'$,
$r$ is sent to $r'$ by this map so that $\overline{h}$ is well defined
on the quotient.

Now let $h'$ send $s_i \in S'$ to $t^{i} s t^{-i}$ and the stable letter $t$ to $t$.
Again, $r'$ is sent to $r$ by $h'$, and $\overline{h'} (t s_i t^{-1}) = t^{i+1} s t^{-(i+1)} =
g (\phi (s_i))$ so $\overline{h'}$ preserves the defining relations of the HNN extension and it
is well defined.

Now check the compositions are indeed the identity.

If $s \in S$ and $s \ne t$
\begin{equation}
  h'(h(s)) = h'(s_0) = s
\end{equation}

and

\begin{equation}
  h'(h(t)) = h'(t) = t
\end{equation}

So it is true that $h' \circ h = \text{id}$.

Also for $s_i \in S'$

\begin{equation}
  \overline{h}(\overline{h'}(s_i)) = \overline{h}(\overline{t^i s t^{-i}}) =
  \overline{t^i s_0 t^{-i}} = \overline{\phi^i(s_0)} = s_i
\end{equation}

Where the equality $t^i s_0 t^{-i} = \phi^i(s_0)$ is given by the defining relations
of the HNN extension.

and
\begin{equation}
  h(h'(t)) = h(t) = t
\end{equation}

So $\overline{h} \circ \overline{h'} = \text{id}$ and $\overline{h}$ and $\overline{h'}$ are isomorphisms.

We then apply the HNN normalization procedure, which will be described in detail in Section
\ref{HNNnorm}. This
will put a word into the HNN normal form described in Lemma \ref{britton}.

We chose $x$ and $t$ such that either $x \notin T$ or $t \notin T$.
In either case, if $\overline{w}$ is in $\overline{F(T)}$,
then an HNN normal form of $\overline{h}(\overline{w})$ will be of the form $g t^n$
with $g \in F(S') / r$. In the case $x \notin T$, then this is because any word in
$F(S')$ not containing $x_i$ for any $i$ must be in $A \cap B$, there can be no occurrence of
$tg$ with $g \notin A$ or $t^{-1}g.$
If $t \notin T$, then it must be possible to write $\overline{w}$ without $t$,
so in fact $\overline{h}(\overline{w})$ can be normalized
to $g \in F(S') / r'$. We can check whether any words in $F(S') / r'$ are in the subgroups
generated by $A$ or $B$ using
Magnus' method again for the shorter relation $r'$, and rewrite these words using the letters
in $A$ or $B$ when possible.

Once we have rewritten $h(w)$ to be in the form $g t^n$ with $g \in F(S')$, i.e.
found an element $gt^n \in F(S') \ast \langle t \rangle$ that is equal to $h(w)$ after
mapping into the HNN extension.
it is enough to check that
$\overline{\text{RemoveSubscripts}(g)}$ is in $\overline{F(T)}$.
If $t \in T$ then this amounts to solving the word problem for $r'$
and the set $T' := \{ s_i \in S' | s \in T, i \in \mathbb{Z} \}$.
If $t \notin T$,
this amounts to checking that $n = 0$ and solving the word problem for
$r'$ and the set $T' := \{s_0 \in S' | s \in T\}$.

\textbf{Case 2.b: No letter in  with exponent sum zero}\label{noexpsumzero}

If there is no letter $t$ in $r$ with exponent sum zero, then we reduce to the case
that there is such a letter by appropriately choosing a change of variables. Choose $x$
and $t$ to be letters in $r$ such that $x \ne t$ and such that if $t \notin T$ then $x \notin T$.
Let $\alpha$ be the exponent sum
of $t$ in $r$ and let $\beta$ be the exponent sum of $x$.
Note that $\alpha$ and $\beta$ are non zero.

Then define the map $\psi$ on $F(S)$ by defining it on a basis element $s \in S$.

\begin{equation}
  \psi(s) =
  \begin{cases}
     t^\beta & \text{if }s = t \\
     xt^{-\alpha} &\text{if } s = x \\
     s & \text{otherwise}
  \end{cases}
\end{equation}

The map $\psi$ descends to a map $\overline{\psi}: F(S) / r \to F(s) / \psi(r)$. The map
$\psi$ is equal to $\psi_1 \circ \psi_2$,
where $\psi_2$ and $\psi_1$ are defined as follows:

\begin{equation}
  \psi_1(s) =
  \begin{cases}
     xt^{-\alpha} &\text{if } s = x \\
     s & \text{otherwise.}
  \end{cases}
\end{equation}

\begin{equation}
  \psi_2(s) =
  \begin{cases}
     t^\beta & \text{if } s = t \\
     s & \text{otherwise}
  \end{cases}
\end{equation}

We have that $\overline{\psi_1}: F(S) / r \to F(S) / \psi_1(r)$
is an isomorphism, with inverse given by sending $x$ to $xt^\alpha$. Meanwhile,
$\overline{\psi_2}: F(S) / r$ to $F(S) / \psi_2(r)$
is also injective.
This is proven constructively in Theorem \ref{powproof}.
Hence $\overline{\psi}: F(S) / r$ to $F(S) / \psi(r)$ is injective.

The image of the $F(T)$ under $\psi$ might not be the
subgroup generated by a set of letters, but it is always contained in $F(T)$.
By the Freiheitsatz, if $\overline{\psi(w)}$ can be written
as a word $w'$ using letters in $T$ then this solution is unique.
Therefore, to check if $\overline{\psi(w)}$ is in the subgroup generated by
$\overline{\psi(T)}$, one can first write it as a word in $w' \in F(T)$ if possible,
and then check if $w'$ is in $\psi(T)$.
The exponent sum of $t$ in $\psi(r)$ is $0$, so the problem of checking
if $\psi(w)$ can be written as a word in $F(T)$ can be solved using
the method described in Section \ref{expsumzero} (Sort out this label).

If $t \in T$ then $\psi(T)$ is generated by
$T' := T \backslash \{t\} \cup t^\beta$. By the Freiheitsatz, if $\psi(w)$ can be written
as a word $w'$ using letters in $T$ then this solution is unique. Therefore,
to check if $\psi(w)$ is in the subgroup generated by $T'$, one can first
write it as a word in $w'$ in $T$ if possible, and then check that for every
occurrence of $t^k$ in $w'$, $k$ is a multiple of $\alpha$.

If $t \notin T$, then  $\psi(T) = T$.


% \subsection{Base Case}\label{BaseCase}

% The base case is the case where the relation $r$ is of the form $a^n$ with
% $n \in \mathbb{Z}$, and $a$ a letter in $S$. It is straightforward to decide
% the word problem in this group, since $F(S) / a^n$ is isomorphic
% to the binary coproduct of $F(S \backslash \{a\})$ and $\mathbb{Z}/n\mathbb{Z}$.
% However computing the appropriate proof term requires some explanation.

% To compute the proof term, we write a function that takes an unnormalized word
% $w \in F(S)$, and a normalized proof word with proof $p \in P(F(S))$, and returns
% a word $q \in P(F(S))$, such that $\text{lhs}(q) = w\text{lhs}(p)$ and
% $\text{rhs}(q)$ is a normalization of $w \text{rhs}(p)$. By normalized,
% we mean that there is no occurrence of $a^k$ where $k \ne 0$ is a multiple of $n$.

% We can normalize the word $a^k$ where $k$ is a multiple of $n$ to
% $([1]^(n / k), 1) \in P(F(S))$, where $[1] \in F(F(S))$, is the basis element
% corresponding to $1 \in F(S)$.

% \begin{defn}(BaseCaseCore)
%   $\text{BaseCaseCore}(w, p)$ is defined by recursion on the list representation of $w$.
%   If $w = 1$, then return $p$. If $\text{rhs}(p) = 1$ then,
% \end{defn}

\subsection{HNN normalization}\label{HNNnorm}

We first present a simplified version of the HNN normalization that does not compute
the proof certificates, and then explain how to compute the certficates at the
same time as normalization.

To compute the HNN normalized term,
first compute the following map from $F(S)$ into the binary
coproduct $F(S') \ast \langle t \rangle$, where $\langle t \rangle$ is an infinite
cyclic group generated by $t$.

\begin{defn}\label{tocoprod}
  Define a map on a basis element $s$ as follows
  \begin{equation}
    \begin{cases}
      s_0 & s \ne t \\
      t & i = t
    \end{cases}
  \end{equation}
\end{defn}

It is important that $a \le 0 \le b$, to ensure that the image
of this map is contained in $F(S' \times \langle t \rangle)$.

Then apply the HNN normalization procedure. For this particular HNN extension
$\phi$ is \textit{ChangeSubscript} (Definition \ref{csub}).
We work in the $F(S') \ast \langle t \rangle$, and apply the following rewriting rules.

For each occurrence of $tw$ where there is an $a \in A$ such that
$\overline{a} = \overline{w}$ replace $tw$ with $\phi(a)t$.

For each occurrence of $t^{-1}w$ where there is an $b \in B$ such that
$\overline{b} = \overline{w}$ replace $tw$ with $\phi^{-1}(b)t^{-1}$.

We can use $\textit{Solve}$ to check whether there is such $a$ and $b$ with
these properties.



% \begin{defn}[HNNNormalizeCore]
% The function \textit{HNNNormalizeCore} takes an unnormalized word $w$ and a normalized word $v$
% both in the binary coproduct. It will return an HNN normalized word $vw$. It is defined by cases.
% When $w = 1$, it returns $v$. When $w = gw'$ for some $g \in F(S')$ and $v = 1$, then it returns

%   \begin{equation}
%       \text{HNNNormalizeCore}(v, w) = \\
%       \begin{cases}
%         v & \text{if } w=1 \\
%         \text{HNNNormalizeCore}(n, w') & \text{if } w = gw' \text{ for some } g \text{ and } v = 1 \\
%         \text{HNNNormalizeCore}(vg, w') & \text{if } w = gw' \text{ for some } g \in F(S') \\
%         \text{HNNNormalizeCore}(v' \text{ChangeSubscript(k, {Solve}(a, A), w')) \text{if } w = aw'
%           \text{ for some } a \in A
%       \end{cases}
%   \end{equation}
% \end{defn}

\subsubsection{Computing Proof Certificates}\label{HNNPC}

To compute proof certificates a slight modification of the procedure described in Section
\ref{HNNnorm} is used.

First define a modification of Definition \ref{tocoprod}, from $F(S)$ into the binary
coproduct $P(F(S \times \mathbb{Z})) \ast \langle t \rangle$.

\begin{defn}\label{tocoprodP}
  Define a map on the basis as follows
  \begin{equation}
    \begin{cases}
      \text{Refl}(i, t^0) \in F(S' \times \langle t \rangle) & i \in S \text{ and } i \ne t \\
      t & i = t
    \end{cases}
  \end{equation}
\end{defn}

There is also a map $Z$ from $P(F(S \times \mathbb{Z})) \ast \langle t \rangle$ into
$P(F(S))$. This map is not computed as part of the algorithm, but is useful to define anyway.

\begin{defn}
  The map $Z$ sends $t' \in \langle t \rangle$ to $\text{Refl}(t) \in P(F(S))$.
  It sends $p \in P(F(S \times \mathbb{Z}))$ to $P(\text{RemoveSubscripts})(p) \in P(F(S))$
\end{defn}

The aim is to define a normalization process into that turns a word $w \in F(S)$ into
word $n \in P(F(S \times \mathbb{Z})) \ast \langle t \rangle$
such that after applying \textit{rhs}, the same word is returned as in the
normalization process described in Section \ref{HNNnorm}. We also want
$\text{lhs}_r(Z(n))$ to be equal to $w$, so we end up with a certificate that $w$
is equal to some normalized word.

\begin{defn}(conjP)\label{conjP}
  Let $(p, a) \in P(F(S \times \mathbb{Z}))$ and $k \in \mathbb{Z}$.
  Define \textit{ConjP} to map into $P(F(S \times \mathbb{Z}))$
  \begin{equation}
    \text{ConjP}(k, (p, a)) = (\rho((t,0)^k, p), \text{ChangeSubscript}(k, a))
  \end{equation}
\end{defn}

\textit{conjP} has the property that (TO DO : which relation am I taking $\text{lhs}$ with)
$\text{lhs}(Z(\text{conjP}(k, p))) = t^k \text{lhs}(Z(p))t^{-k}$,
and similarly for \textit{rhs}. Note that $\textit{conjP}$ maps into $P(F(S \times \mathbb{Z}))$
and not $P(F(S'))$, although $\textit{rhs}$ of every word computed
will be in $F(S')$.

The procedure described in Section \ref{HNNnorm}
replaced each occurrence of $wt^{-1}$ with \newline $t^{-1}\text{ChangeSubscript}(-1)(a)$,
where $a \in A$ was a word equal to $w \in F(S')$ in the quotient $F(S') / r'$.

To compute the certificates apply the following rewriting procedure:
for each occurrence of $tp$ where $\overline{\text{rhs}(p)} = \overline{a}$ for
some $a \in A$, and $q$ is a certificate of this equality,
replace $tp$ with $\text{ConjP}(1, \text{Trans}(p, q))t$

Similarly, for each occurrence of $t^{-1}p$ where $\overline{\text{rhs}(p)} = \overline{b}$ for
some $b \in B$ and $q$ is a certificate of this equality,
replace $t^{-1}p$ with $\text{ConjP}(-1, \text{Trans}(p, q))t^{-1}$

\subsubsection{Performance}\label{HNNperf}

The order in which the rewriting rules are applied can have a big effect on the performance
of the algorithm.

\begin{exmpl}\label{ltrbad}
Suppose $r' = {x_1}{x_0}^{-2}$ and
$w = t^n x_1 t x_0^{-1}$, where $n > 0$.
Then $S'$ is the set $\{x_0, x_1\}$,
$A$ is the subgroup generated by $S' \backslash x_1$ and
$B$ the subgroup generated by $S' \backslash x_0$.
Suppose we first make the substitution $tx_0^{-1}$ to $x_1^{-1} t$;
then $w$ becomes $t^{n}x_1x_1^{-1}t = t^{n+1}$.
This is in HNN normal form.

Now consider trying the HNN normalization process from the left.
For any $m \in \mathbb{Z}$, $x_1^m = x_0^{2m}$,
so the HNN normalization proces will rewrite $tx_1^m to x_1^{2m}t$.
Therefore $t^nx_1$ will be rewritten to $x_1^{2^n}t^n$.
Hence $w$ gets rewritten to $x_1^{2^n} t^{n+1} x_0^{-1}$,
which then will eventually be rewritten to $t^{n+1}$. The maximum length
of $w$ during the normalization process was greater than $2^n$.
\end{exmpl}

Applying one rewrite rule first
might mean that another rewrite is unnecessary, or a call to \textit{Solve} is
given an easier problem.

\begin{exmpl}\label{rtlbad}
Consider the word $tw_0t^{-1}w_1$ with $w_0, w_1 \in F(S')$. In this situation
it is best to start by attempting to prove $\overline{w_0} \in \overline{A}$.
Applying the left hand rewrite first will put the word into HNN normal form
straight away; it will not be necessary to check $\overline{w_1} \in \overline{B}$.

Rewriting starting on the right first might give a word such as
$tw_0 \phi^{-1}(b)t^{-1}$, where $\overline{b} = \overline{w_1}$. But since
$\phi^{-1}(b) \in A$, checking whether $\overline{w_0\phi^{-1}(b)} \in \overline{A}$ is
no easier than checking $\overline{w_0} \in \overline{A}$ has not become
any easier. So in this example it is better to start rewriting on the left, and furthermore,
if $\overline{w_0} \notin \overline{A}$ then it will not be possible to eliminate the $t$'s, so the
algorithm can fail straight away without attempting more rewrites.

\subsubsection{Other Potential Improvements}

There are other potential improvements that could be made but it is not clear
what effect, positive or negative, they would have on performance. These are discussed
in this section.

A potential improvement is changing the definition of the set $S'$ in Section \ref{xandt},
to make the set as small as possible.
First define the set $X$
of letters that meet the criteria for $x$ from Section \ref{xandt}.

\begin{equation}
  X := \{ x \in S \backslash \{t\} |
    x \text{ is contained in }r \text{ and } \ (x \notin T \text{ or } t \notin T) \}
\end{equation}

For any $x \in X$, $a(x)$ and $b(x)$ would be defined in a similar way to $a$ and
$b$ in Section \ref{xandt}, as the maximum and minimum subscripts of $x$ in the word
$\text{AddSubscript}(t)(r)$. Then

\begin{equation}
  S' := \{(x, n) \in S \backslash \{t\} \times \mathbb{Z} \
| \ x \notin X \text{ or } a_x \le n \le b_x \}
\end{equation}

The sets $A$ and $B$ are then defined in an analogous way to in Section \ref{xandt}.

\begin{equation}
  A := S' \backslash \{(x, b (x)) \ |\  x \in X\}
\end{equation}
\begin{equation}
  B := S' \backslash \{(x, a (x)) \ |\  x \in X\}
\end{equation}

The sets $A$ and $B$ are smaller than the alternative definition, where the constraints
on $n$ are only applied to one letter $x$, rather than a set of letters $x$. The fact
that the set is smaller means that the rewrites in the HNN normalization process
are less likely to suceed. This may not seem like a good thing, but it is faster for
some examples.

Consider the following Example, a slight modification of Example \ref{ltrbad2}.

\begin{exmpl}\label{ltrbad3}
    Suppose $r = txt^{-1}(xy)^{-2}$,
    then \newline $r' := \text{AddSubscript}(t)(r) = {x_1}(x_0y_0)^{-2}$ and
    suppose $w = t^n x_1y_1 t (x_0y_0)^{-1}$, where $n > 0$.

    Suppose $S'$ is defined to be the set $\{x_0, x_1, y_0, y_1\}$.
    Then the only rewrite possible in the HNN normalization
    process is to substitute $t(x_0y_0)^{-1}$ with
    $(x_1y_1)^{-1}t$ after which the word will already be in HNN
    normal form.

    If alternatively, $S'$ is defined to be just $\{x_0, x_1\}$,
    then it is also possible to perform a substitution on the left,
    rewriting $t^nx_1y_1$ to $t^{n-1}(y_1x_1)^2y_2$, after which it
    is still possible to perform many more rewrites on the left hand side,
    meaning the word will be normalized in many more steps.
\end{exmpl}

Whilst changing the definition of $S'$ potentially takes away potential
rewrites, in at least one example the option it took away was a bad
rewrite to make anyway.



% There are also examples of where starting on the left is more effective.
% Suppose $r'$ is the same by $w$ is now equal to
% $tx_0t^{-1}(x_0y_0)^2$. Starting on the left by rewriting $t x_0$ to $x_1 t$,
% $w$ will be rewritten to $x_1t t^{-1}(x_0y_0)^2 = x_1(x_0y_0)^2$.
% This was put into normal form without ever having to rewrite a word in $F(S')$.

% Starting on the right is more complicated;
% $t^{-1}(x_0y_0)^2$ must first be rewritten
% to $t^{-1}x_1y_1$ and then $x_0y_0t^{-1}$.
% After this $w$ becomes $tx_0^2 y_0t^{-1}$. This is then
% rewritten to $x_1^2 y_1$. Starting on the right required
% having to rewrite $(x_0y_0)^2$ to $x_1y_1$, invoking
% a recursive call to \textit{Solve}.
\end{exmpl}

\begin{exmpl}
  Consider the word $tw_0t^{-1}w_1tw_2$. Here it is not possible to determine the best order without
  considering what particular words $w_0, w_1, w_2$ are. Starting in the middle gives the word
  $tw_0\phi(b)w_2$, and since $w_2$ might not be in $B$, the first problem may have become easier.

  However starting on the left with the pair $tw_0$
  might also be the best thing to do, since starting on the left would
  make the second problem easier, giving the word $\phi(bw_1 t w_2)$.
\end{exmpl}

\begin{exmpl}\label{ltrbad2}
  Consider the word $tw_0tw_1$. Here it is best to apply the right hand rewrite first.
  Applying the left hand rewrite first will not make the right hand one any easier; the $t$'s will
  not cancel, but applying the right hand one first could make the left hand problem easier.
  After applying the right hand rewrite, the word would become $tw_0\phi(a)t$, where $a \in A$
  and $\overline{a} = \overline{w_1}$. It is possible that it is easier to check $\overline{w_0\phi(a)} \in \overline{A}$
  than to check both $\overline{w_0} \in \overline{A}$ and $\overline{phi}(a) \in \overline{A}$.
\end{exmpl}

Example \ref{rtlbad} and Example \ref{ltrbad2} give an optimal normalization order
for simple examples, with only two occurrences of a power of $t$.
It is not possible to generalize this to more complicated examples,
but some sensible heuristics are possible.

The implemented code normalises the word from left to right, always applying
a substitution at the leftmost position where one is possible. This was found to have better
performance than right to left normalisation, probably because the cancellation
in Example \ref{rtlbad} is more likely than the cancellation in the Example \ref{ltrbad}.

We describe below a potentially improved way of performing the rewrites in the
an order with good performance.

Consider a word $W := w_0t^{n_1}w_1 \dots t^{n_{k-1}}w_{k-1}t^{n_k}w_k$, and the
following two sets

\begin{equation}
    R^{+} := \left\{ i \ | \ \forall j, \sum_0^i n_i \le \sum_0^j n_j\right\}
\end{equation}
\begin{equation}
    R^{-} := \left\{ i \ | \ \forall j, \sum_0^i n_i \ge \sum_0^j n_j\right\}
\end{equation}

Let $I$ be the smaller of the two intervals $[\min R^+, \max R^+]$ and
$[\min R^{-}, \max R^-]$.

Within the interval $I$, let $Q$ be the set of pairs $t^{n_i}w_i$ such that
$\text{sgn}(n_i) \ne \text{sgn}(n_{i+1})$.

Then $Q$ has the property that if there are no applicable rewrites
in the set $Q$, then the word cannot be put into the form $wt^n$ with $w \in F(S')$.
This is because if no rewrite can be performed at the first and last pairs $t^{n_i}w_i$
in the interval, then no substitutions performed outside this interval will
make these rewrites possible.

The rewriting procedure always chooses a rewrite within the set $Q$, prioritising
pairs $w_it^{n_i}$ where $w_i$ has the least occurences of letters not in
the set $T$. These are likely to be the fastest problems to solve.
This heuristic mitigates the exponential behaviour described in Example \ref{ltrbad}.

Care must also be taken to ensure that equivalent problems are not attempted twice.
For example if a rewrite is attempted at a pair $t^{n_i}w_i$, where $0 < n_i$
and fails because $\overline{w_i} \notin A$ is not in the relevant subgroup,
and then later on after substitutions elsewhere, this pair becomes
$t^{n_i}w_i\phi^{-1}(w_k)$, then since $\phi^{-1}(w_k) \in A$, then a rewrite
is still not possible here so none should be attempted.



% \begin{lemma}\label{peakshrink}
%  . Then applying HNN normalization
%   rules to $W$ can never increase the value $\max_a \sum_{i=1}^a n_i$.
% \end{lemma}

% \textit{Proof of Lemma \ref{peakshrink}.} Blank for now. Kind of obvious.



% \textit{Proof of Theorem \ref{rewriteorderthm}.} There are two ways that $W$ may be normalized
% without $\overline{w_a} \in \overline{A}$. Either the problem is simplified from the left, and the $t$
% is cancelled by being rewritten on the left, or after some rewrites to the right of $w_a$, $w_a$
% ends up multiplied by $\phi(a)$, and where $\overline{w_a\phi(a)} \in \overline{A}$.

% In order for the pair $t^{n_k}w_k$ to be simplified from the left,
% $W$ must first be rewritten to
% $W' := w'_0t^{n'_1}w'_1 \dots t^{n'_{a-1}}w_{a-1}t^{n_a}w_a \dots
% t^{n'_{k-1}}w'_{k-1}t^{n'_k}w'_k$ where $n'_{a-1} < -n_a$. But then
% $\sum_{i=1}^{a-2} n'_i \ge \sum_{i=1}^a n'_i$, contradicting the condition
% on $a$ (This condition is preserved by applying rewriting rules to pairs other
% than $t^{n_a}w_a$).

% In order for the pair $t^{n_k}w_k$ to be simplified from the right, $W$ must first
% be rewritten to $W' := w'_0t^{n'_1}w'_1 \dots t^{n_a}w_a t^{n'_{a+1}}w_{a'+1} \dots
% t^{n'_{k-1}}w'_{k-1}t^{n'_k}w'_k$ where $n_{a+1} > 0$. But then
% $\sum_{i=1}^{a+1} n'_i > \sum_{i=1}^a n'_i$ again contradicting the condition on $a$.

% Theorem \ref{rewriteorderthm} gives a precise rewriting order that avoids the undesirable
% inefficiencies in Example \ref{ltrbad} and Example \ref{rtlbad}, the
% pair $t^{n_a}w_a$ that satisfies the condition in this theorem should be rewritten first.
% Rewriting using this order also has the advantage that provided the starting problem
% is true, \textit{Solve} only recursively calls itself on problems that are also true, which
% is an obvious performance benefit, and this means that \textit{Solve} can fail early
% as soon as it recursively calls itself on a problem that is not true.

% It is possible to determine the best order to apply the rewriting procedure.

% Where there is a word of the form $tw_0t^{-1}w_1$, then starting on the left
% might make the right hand problem easier, because the $t$s would cancel. It is never
% better to start on the right

% \begin{exmpl}\label{lastfirst}
% There are occasions when it is impossible that a rewrite might become
% easier or unnecessary if other rewrites are applied first. For example
% consider a word $twtv$, with $w, v \in F(S')$. Then rewriting $tw$ to
% $\phi(a)t$ where $a = w$ in $F(S') / r'$ does not make the second problem
% any easier, I will still have to check if $v$ is in $A$. However,
% starting with rewriting $tv$ to $\phi(a')t$ with $a' = v$ in $F(S') / r'$,
% changes the first problem to $tw\phi(a')$.
% Checking $w\phi(a') \in A$ might be an easier problem than checking $w \in A$
% and $\phi(a') \in A$, and will never be a harder problem. So in this example
% it makes sense to attempt the right hand rewrite first.

% Additionally if the
% right hand rewrite fails, i.e. $v \notin A$, then it will not be possible to
% put the word in the form $gt^n$ with $g \in F(S')$. This means we need not
% attempt to normalize the rest of the word since we are only interested in the result of
% HNN normalization if the word can be put into the form $gt^n$.
% \end{exmpl}

% The case in Example \ref{lastfirst} can be generalized.
% Consider a word \newline $t^{n_0}w_0 \dots t^{n_{k-2}}w_{k-2}t^{n_{k-1}}w_{k-1}t^{n_{k}}w_{k}t^d$.
% If for every $m > 0$, $\text{sign}(n_k)\sum_{i=0}^m n_{k - i} \ge 0$ then
% in order to put the word in the form $gt^n$ with $g \in F(S')$, then it will definitely
% be necessary to check $w_k$ is equal to a word  $a \in A$ in the quotient
% $F(S') / r'$ and rewrite $tw_k$ to $\phi(a)t$ so this
% rewrite should be attempted first.
% There is a similar condition when $n_k < 0$. Call this condition Condition 1.

% Consider a word
% $t^{n_0}w_0 \dots t^{n_{k-2}}w_{k-2}t^{n_{k-1}}w_{k-1}t^{n_{k}}w_{k}t^d$,
% then if for every $j \le i$,and for every $m > 0$, $\text{sign}(n_j)\sum_{l=j}^m n_l \ge 0$ and
% $i < k$, we say that $i$ satisfies Condition 2.
% When $t^{n_i}w_i$ satisfies Condition 2,
% normalizing this pair first will not make any of the rewrites to the right
% of $w_i$ any easier; there will no no cancellation of $t$s,
% so this rewrite should be attempted after anything to the right of it.



% \begin{defn}[thingy]
% For a pair $t^{n_i}w_i$, where $n_i > 0$ we define $\text{thingy}(n_i, w_i)$ to be the number of
% occurrences of letters in $w_i$ but not in $A$, if $n_i < 0$ it is the number of occurrences
% of letters in $w_i$ but not in $B$. The number of occurrences to be the sum of the absolute
% value of the exponents of these letters.
% \end{defn}

% The rewrites are applied with the following priority

% \begin{itemize}
%   \item Any pairs $t_nw_n$ satisfying Condition 1
%   \item Of the pairs that do not satisfy Condition 2, priority is given
%     to the pairs $t^n_iw_i$ with the smallest value of $\text{thingy}(n_i, w_i)$.
%   \item Out of the pairs that have the equal least value of $\text{thingy}$,
%     then attempt to rewrite the leftmost pair first.
% \end{itemize}


% \subsubsection{Proof Lengths}

% It is important to apply the rewriting procedure from left to right. This will usually
% produce shorter proof certificates. Consider normalizing $t'^{-1}wt'vt'^{-1}$ with $w, v \in F(S')$.
% Suppose we normalize right to left.
% $t'v$ is normalized to $pt'$ with $p \in P(F(S \times \mathbb{Z}))$.
% After that substitution the new word $t'^{-1}wp$, with $wp \in P(F(S \times \mathbb{Z}))$.
% Suppose $\text{rhs}(wp)$ is normalized to $q \in P(F(S \times \mathbb{Z}))$.
% Then the final normalized word is $\text{conjP}(1, \text{Trans (wp, q)})t'^{-1}$.

% The proof part of $\text{Trans}(wp, q)$ is $\rho(w)(\text{Left}(p))\text{Left}(q)$.

% Normalizing the other way, we first normalize $t'^{-1}w$ to $p_2 t'^{-1}$ with
% $p_2 \in P(F(S \times \mathbb{Z}))$. Then the new word is $p_2vt^{-1}$. Suppose
% $\text{rhs} p_2 v$ is normalized to $q_2 \in P(F(S \times \mathbb{Z}))$.
% Then the final normalized word is $t'^{-1}\text{conjP}(-1, \text{Trans} (p_2v, q_2))$.

% The proof part of $\text{Trans} (p_2v, q)$ is $\text{Left}(p_2)\text{Left}(q)$. There is no
% use of \textit{\rho} since $\text{Left}(v)=1$. On average using \textit{\rho}
% will make words longer, and so the left to right normalization produces shorter proofs.
% The left to right normalization was found to produce shorter proofs in practice.

\subsection{Shortening Proofs}\label{golfsec}

Most of the tactic execution time is spent generating and checking the Lean proof,
and not on generating the proof certificate described in Section \ref{proofcert}.
An algorithm was defined which implemented some heuristics to shorten the certificates
produced. There are two heuristics used to do this.

There are three equalities that the heuristics make use of. Recall that
a certificate is an element of $P(F(S))$, which is a semidirect product of $F(S)$
and $F(F(S))$. Given an element $p \in F(F(S))$, we aim to find an element
$p' \in F(F(S))$ such that $\text{Eval}(p) = \text{Eval}(p')$ (Definition \ref{Eval}).
Given a relator $r$, and words $w, v \in F(S)$, the heuristics make use of the following equalities.

\begin{equation}\label{golf1}
  \text{Eval}([w]) = \text{Eval}([wr^n])
\end{equation}

\begin{equation}\label{golf2}
  \text{Eval}([w][v]) = \text{Eval}([wrw^{-1}v][w])
\end{equation}

\begin{equation}\label{golf3}
  \text{Eval}([w][v]) = \text{Eval}([v][vr^{-1}v^{-1}w])
\end{equation}

A total order $(<)$ is put on the set of words in $F(S)$, this order has
the property the if $\text{Length}(w) < \text{Length}(v)$, then
$r(w,v)$. It is not important what the relation is on words of the same length,
as long as it is a total order, but the Lean implementation uses a lexicographic ordering.

\begin{defn}[Golf\textsubscript{1}]
$\text{Golf}_1$ folds through a word $p \in F(F(S))$ and replaces each
letter $w \in F(S)$ with the least word of the form $wr^n$.
It also performs any cancellations that can be performed after
performing these substitutions, to return a reduced word $p' \in F(F(S))$.
In practice, there are very often cancellations
that can be performed after this normalization of each letter.
\end{defn}

\begin{defn}[Golf\textsubscript{2}]
  $\text{Golf}_2$ performs substitutions of the form in Equations \ref{golf2} and
  \ref{golf3}. If $wrw^{-1}v$ is less than $v$, it will perform the substition
  in Equation \ref{golf2}, and if $vr^{-1}v^{-1}w$ is less than $w$ it performs
  this substitution. It performs these substitiutions until no more can be made.
  Again, it performs any cancellations that can be performed to return
  a reduced word.
\end{defn}

\begin{defn}[Golf]
  For an element $(p, w) \in P(F(S))$, $\text{Golf}(p, w)$ is defined to be
  $((\text{Golf}_2 \circ \text{Golf}_1)(p), w)$.
\end{defn}

These heuristics were surprisingly effective at shortening the proof certificates.
As an extreme example, if $r = aba^{-11}b^4$, and
$w = a^{10}b^{-4}a^{11}b^{-1}ab a^{-11} b^5a^{-11}b a^{11}b^{-1}a^{-1}b^{-1}a^{-10}$,
then the certificate produced by $\text{Solve}$ that $\overline{w} =1$
 has length $72$ before shortening and length $4$ after shortening.
 This shortens the overall tactic execution from 54 seconds to around
 3 seconds, with only 120ms spent executing Golf.

The biggest benefit of the $Golf$ function, is that it reduces the number
of letters in a certificate $p \in F(F(S))$, not just the length of each letter
$w \in F(S)$. It does this because shortening each letter, is also
canonicalising the letters, making cancellation more likely. This is the motivation
for putting a total order on the set of letters $w \in F(S)$.
For example, if $w$ and $wr$ are the same length, but $w < wr$, then
the word $[w][wr]^{-1}$, would not be reduced if Golf only compared lengths of letters,
but would be is reduced to $1$ by using a total order.

% If $w \in F(S)$, then $\text{Eval}([w]) = wrw^{-1}$. Therefore, for any power of $r$, $r^n$,
% $\text{Eval}([wr^n]) = \text{Eval}([w])$. If $wr^n$ is shorter than $w$,
% then $[wr^n]$ is a shorter certificate of an equality than $[w]$. The first
% heuristic goes through a word $p \in F(F(S))$, and replaces any letter $w$ with
% $wr^n$ if it is shorter. It also performs any cancellations that can be performed after
% performing these substitutions. In practice, there are very often cancellations
% that can be performed after this normalization of each letter.

% Given a word of length $2$ in $F(F(S))$, $[w][v]$, then there are
% two ways of generating a certificate of the same equality.


\subsection{Heuristics}

A few heuristics are implemented when there is a simpler method than Magnus' method.
They are implemented in the following order

\begin{itemize}
  \item Check whether the word $w$ is already written using letters in $T$. If $w \in T$,
  then trivially $\overline{w} \in \overline{T}$

  \item If there is a letter $x$ in the relator $r$ such that $x \notin T$, but
  $x$ is in $w$ then by the Freiheitsatz, $\overline{w} \notin \overline{T}$,
  so the algorithm can fail straight away.

  \item If $w$ is not in the subgroup generated by $r$ and $T$ after abelianizing the free
    group, the algorithm can fail straight away.

  \item If the relation $r$ has exactly one occurrence of a letter, say $x$, then
    the problem can be solved by rearranging the equation $r = 1$ to the form
    $x = v$, where $v$ is a word not containing $x$, and making this substitution everywhere
    in $w$.
\end{itemize}

\subsubsection{Injectivity}\label{powproofs}

The correctness of the algorithm relies on the fact that the map $\overline{\psi_2}$ is an injective map.
%% define \psi_2
Since $\overline{\psi_2}$ is injective, if $p \in P(F(S))$ is a witness of the congruence
$\psi_2(a) \equiv \psi_2(b) \text{ mod } \psi_2(r)$, then there must exist a certificate
$q$ of the congruence $a = b \text{ mod }r$. The question is how to compute this. The proof
of this congruence given in \cite{PutmanOneRelator} relies on the fact that the canonical maps into an amalgamated product
of groups are injective. However the standard proof
seems to rely on the law of the excluded middle, so
it cannot be translated into an algorithm to compute $q$.

%% n is the thing that we're

Suppose $p \in P(F(S))$ is a witness of the congruence
$\psi_2(a) = \psi_2(b) \text{ mod } \psi_2(r)$. It is not necessarily the case that $k$
is a multiple of $\beta$ in every occurrence of $t^k$ in $p$. For example
$p := ([t][t\psi_2(r)^{-1}t^{-1}][t]^{-1}, 1) \in P(F(S))$ is a witness of the congruence
$\psi_2(r) \equiv 1\text{ mod } \psi_2(r)$. Both $\text{lhs}_{\psi_2(r)}(p)$
and $\text{rhs}(p)$ are in the image of
$\psi_2$ for $\beta \ge 2$,
but $p$ is not in the image of $P(\psi_2)$. In fact
one could remove every occurrence of $t$ from $p$ and still have a certificate of the same
congruence.

In practice, it is observed that whenever there is an occurence of $t^k$ it is always
the case that $t$ is a multiple of $n$. If this were true all the time, then a slightly
simpler algorithm could be possible.

\begin{defn}
  Given a word $w \in F(S)$, define the set of partial exponent sums of a letter $t \in S$ to
  be the set of exponent sums of all the initial words of $w$. For example, if $a \in S$ and
  $a \ne t$, the partial
  exponent sums of $t$ in $t^n a t$ are the exponent sums of $t$ in
  $t^n$, $t^na$ and $t^nat$.
\end{defn}

\begin{defn}
  $h$ is a map $F(S) \to F(S \cup \{t'\})$, where $t'$ is some letter not in $S$.
  $h$ replaces every occurrence of $t^k$ with $t'^at^b$ in such a way that $a + n b = k$,
  and every partial exponent sum of $t'$ in $h(w)$ is either not a multiple of $n$,
  or it is zero.
\end{defn}

\begin{defn}
  $\theta$ is a group homomorphism $F(S \cup \{t'\})$. Let $s \in S$. Then
  \begin{equation}
    \theta(s) = \begin{cases}
      t & \text{if } s = t' \\
      t^n & \text{if } s = t \\
      s & otherwise
    \end{cases}
  \end{equation}
\end{defn}

$\theta$ and $h$ satisfy $\theta \circ h = \text{id}$. For any $w$ in $F(S)$,
$\theta(w) = \psi_2(w)$.

\begin{defn}(PowProof)\label{PowProof}
  \textit{PowProof} is a map $F(S) \to F(S)$. $\text{PowProof}(w)$ is defined to be
  $h(w)$, but with every occurrence of $t'$ replaced with $1$.
\end{defn}

\begin{theorem}\label{powproof}
  For any $p \in F(F(S))$ if \newline $\text{Eval}(\psi_2(r))(p) = \psi_2(w)$,
  then $\text{Eval}(r)(F(\text{PowProof})(p)) = w$.
\end{theorem}

\begin{sublemma}\label{powproof1}
  Consider $\prod_{i = 1}^a s_i^{k_i}$, as an element of the $F(S \cup \{t'\})$
  with $s_i \in S \cup \{t'\}$ (Note that this is not necessarily a reduced
  word; $k_i$ may be zero and $s_i$ may be equal to $s_{i+1}$).
  Suppose every partial product $\prod_{i=1}^b s_i^{k_i}$,
  with $b \le a$ has the property that if the exponent sum of $t'$ is a multiple
  of $n$, then it is zero. Suppose also that $\prod_{i = 1}^b s_i^{k_i}$ has the
  property that for every occurrence of $t'^k$ in the reduced product, $k$ is a multiple of
  $n$. Then the reduced word $\prod_{i = 1}^a s_i^{k_i}$
  can be written without an occurrence of $t'$.
\end{sublemma}

\textbf{Proof of Lemma \ref{powproof1}.} $\prod_{i=0}^a s_i^{k_i}$ can be written as a reduced
word $\prod_{i = 1}^{c} u_i^{k'_i}$ such that $k'_i$ is never equal to zero and
$u_i \ne u_{i+1}$ for any $i$. The set of partial products of this,
$\prod_{i = 1}^{c} u_i^{k'_i}$, is a subset of the set of partial products of
$\prod_{i=1}^a s_i^{k_i}$,
therefore the exponent sum of $t'$ in every partial product of $\prod_{i=1}^a s_i^{k_i}$, is
either $0$ or not a multiple of $n$. However, by assumption every occurrence $t'^k$ in
$\prod_{i=0}^a s_i^{k_i}$, $k$, is a multiple of $n$, so the exponent sum of $t'$ in every
partial product is $0$. So $\prod_{i=1}^a s_i^{k_i}$ does not contain $t'$.

\textbf{Proof of Theorem \ref{powproof}} \newline
If $\text{Eval}(\psi_2(r))(p)$ is in the image of $\psi_2$,
then
$\text{Eval}(\psi_2(r))(F(h)(p))$ has the property
that for every occurrence of $t'^k$, $k$ is a multiple of $n$.

If $p' := F(h)(p)$,
then $\text{Eval}(r)(p')$ can be written as a product of the form in
Lemma \ref{powproof1}.
If $r' = \prod_i u_i^{l_i}$, then to write $\text{Eval}(\psi_2(r))(p')$ in this
form, send $\prod_i \left[\prod_{j = 1}^a s_{ij}^{k_j}\right]
\in P(F(S \cup \{t'\}))$, to
\begin{equation} \label{eq:bigprod}
  \prod_i \left(\left(\prod_{j = 1}^a {s_{ij}}^{k_j}\right) \left(\prod_j u_j^{l_j}\right)
  \left(\prod_{j = 1}^a {s_{i(a - j)}}^{-k_{a - j}}\right)\right)
\end{equation}
If all the nested products in Equation \ref{eq:bigprod} are appended into one long product,
then the product has the form in Lemma \ref{powproof1}. Therefore when the word is reduced
it will not contain $t'$ by Lemma \ref{powproof1}. This means that deleting all occurrences of
$t'$ will in $p'$ will not change $\text{Eval}(r)(p')$, and therefore
$\text{Eval}(r)(F(\text{PowProof})(p)) = \text{Eval}(r)(F(h)(p))$.
Applying $\psi_2$ to both sides gives
$\psi_2(\text{Eval}(r)(F(\text{PowProof})(p)) =
  \psi_2(\text{Eval}(r)(F(h)(p))) =
  \theta(\text{Eval}(r)(F(h)(p))) =
  \text{Eval}(\psi_2(r))(p)$.

\section{Efficiency of the Algorithm}

  The worst case performance of this algorithm is worse than any finite tower of exponents
  \cite{miasnikov2011word}. The more relevant question is what is the typical performance.

  \begin{defn}[Area of a Relation]
  For a finitely presented group $G := \langle S | R\rangle$, if $w \in F(S)$ is equal
  to $1$ in the quotient $G$, then we say it is a \textit{relation}.
  The \textit{area of a relation} is the smallest $N$ such that $w$ can be written
  in the form $\prod_{i=1}^N g_i r_i^{\epsilon_i} g_i^{-1}$, where $\epsilon_i = \pm 1$
  and $r_i \in R$ for all $i$.
  \end{defn}

  \begin{defn}[Dehn Function]
  For a finitely presented group $G := \langle S | R\rangle$, the Dehn function
  of the presentation $\text{Dehn}(n) \in \mathbb{N}$ is defined as the
  largest area of a relation of length at most $n$.
  \end{defn}

  The Dehn function puts a lower bound on the complexity of the one-relator algorithm.
  The area of a relation is by definition the length of the shortest certificate
  that the algorithm might produce, so the complexity of the algorithm is bounded above by
  the Dehn function of a relator.
  The group $\langle a, b | b a b^{-1} a b a^{-1} b^{-1} = a^2\rangle$ is such that
  $\text{Dehn}(n)$ is worse than any finite tower of exponents. This means that the
  complexity of the one relator algorithm is also worse than any finite tower of
  exponents.

  Not all groups have such a fast growing Dehn function. For example, if the relator
  is of the form $r^k$ with $|k| \ne 1$ then $\text{Dehn}(n) \le n$.
  Similarly, even in groups with a rapidly increasing Dehn function,
  there are words that do not have a large area as the worst case.

  So, even though the worst case behaviour is very bad, there are still potentially many
  problems that the algorithm could solve in a practical amount of time. The aim of this
  implementation was to have good performance on relations with a small area.
  A typical Lean tactic state will usually be used on problems where the author knows
  the solution, but simply needs automation to write a formal proof of the solution.
  These relations will usually have a very small area. The aim of this implementation
  was that the algorithm should have good performance on relations with a small area,
  but makes no attempt to solve problems where the area of the relation is very large.

\section{Graph Search Method}\label{gsmethod}

  In this section we present an alternative method to solve word problems in groups.
  This method searches for a sequences of rewrites to prove an equality. This search will
  usually not terminate if there is no such sequence of rewrites. I conjecture that it
  will terminate whenever there is a such sequence of rewrites. It was inspired by an
  online solver written Kyle Miller, with some modifications since that solver
  would sometimes fail to prove true formulas.

  This section describes a solver that attemps to prove a word in the free group
  is in the normal closure of a finite set of relators. It then describes
  a Lean tactic that makes use of this solver, but employs some methods
  to convert various Lean tactic states into a form that can be tackled by
  the solver.

  Just as was the case with the tactic for Magnus' Method, the tactic has two parts,
  a solver that determines whether a word is in the normal closure of a set of relators,
  and uses a more efficient representation of elements of the free group, and keeps track
  of some sort of proof certificate, and a Lean tactic that translates a Lean tactic state
  into a problem about elements in the efficient representation of the free group, and
  writes a formal Lean proof of the result using the certificate.

\subsection{Substitution heuristic}\label{subst}
  Sometimes it happens that it is very straightforward to eliminate a variable.
  For example, if one of the relators has exactly one occurence of one of the
  letters, then this letter and the relator can easily be eliminated.

  For example, consider the following problem.

  \begin{equation}
    \begin{aligned}
      aba =b^{-1}c \\
      bac^2ba^2 = 1 \\
      \vdash ab=ba
    \end{aligned}
  \end{equation}

The first equation can be rewritten as $c = baba$, which means that $c$ can be eliminated
from the problem, and the new problem is
\begin{equation}
  \begin{aligned}
    babababa^2 = 1 \\
    \vdash ab=ba
  \end{aligned}
\end{equation}

\subsection{Outline}

  Given a set of relators $R$, the method first generates a set of rewriting rules
  from the set of relators. Given a relator, the algorithm generates all equalities
  that can be made using the generator such that there is one letter of the starting
  relator on the left hand side of the equality.
  For example, if a relator is $abab^2$, then the equalities generated are
  \begin{equation}\
    \begin{aligned}
      a = b^{-2}a^{-1}b^{-1}, & a^{-1} = bab^2 \\
      b = a^{-1}b^{-2}a^{-1}, & b^{-1} = ab^2a \\
      a=b^{-1}a^{-1}b^{-2}, & a^{-1}=b^2ab \\
      b=a^{-1}b^{-1}a^{-1}b^{-1}, & b^{-1} = baba
      b=b^{-1}a^{-1}b^{-1}a^{-1},& b^{-1} = abab
    \end{aligned}
  \end{equation}

  Given a starting word, the algorithm then generated all words that can be generated
  from this starting word rewrites using the above rules and adds these
  words to a set of leaves. The process is then repeated at the word in the set of
  leaves with the least cost. The cost function assigns a natural number to each word in
  the free group. In general, shorter words have a lower cost, but different cost functions
  are discussed in section TODO. Before being added to the set of leaves, each word is cyclically
  reduced as well, a word is replaced by the shortest of its conjugates.

  As an example, we could apply the first rewriting rule $a = b^{-2}a^{-1}b^{-1}$
  to $aba^{-1}b^{-1}$ and obtain the word
  $\color{red}b^{-2}a^{-1}b^{-1}\color{black}ba^{-1}b^{-1} =
  b^{-2}a^{-2}b^{-1}$ which is then added to the set of leaves. The process
  is then repeated from the word in the set of leaves with the least cost, taking care
  not to repeat any words, until the word is rewritten to $1$.

  In summary, the process is as follows given a word $w$ that we are trying to prove is equal
  to $1$.
  \begin{enumerate}
    \item Eliminate as many variables as possible using the method outlined in Section \ref{subst}
    \item Generate the set $G$ of all rewriting rules such that there is one letter on the
      left hand side that can be generated from the starting relators.
    \item We store two sets of words, a set of seen words $S$, and a set of leaves $L$.
    \item Add the word $w$ to the set of seen words leaves $S$
    \item Generate all new words that can be made by applying a rewrite rule in $G$,
      and add these words to the set $L$ after cyclically reducing them.
    \item Take the word $w$ in $L$ with the least cost.
      If it is equal to $1$ then stop. Otherwise, check whether this
      word is in $S$. If it is not, then remove
      it from $L$ and go to step 3, otherwise remove it from $L$ and repeat this step.
  \end{enumerate}

A slightly different approach to the above approach would be to check if
a word is in the set $S$ of seen words at step 5, and add it to both sets $S$ and $L$
if it was not in $S$, and neither set otherwise, and then there would be no need to check
if words were in $S$ at step 6, and the set $L$ would be much smaller. However, this was
found to be a lot slower, since so much time was spent checking whether words were in $S$.


\subsection{Cost Function}

The method mentions a cost function which was not defined yet. Two cost functions
were tested, one was simply the length of a word, the other effectively ordered words
by length first and then lexicographically, after the letters are put in some arbitrary order.

The second cost function was defined as follows. Suppose $n$ is the total number of
letters in all words in $R$ and the target word $w$.
Label the letters $a_1, \dots a_n$. Then the cost of a word is defined inductively on the
length of the word as follows

\begin{equation}
  \text{Cost}(1) = 0
\end{equation}

\begin{equation}
  \text{Cost}(a_i^{\pm1} w) = i + n \text{Cost}(w)
\end{equation}


This cost function was found to have better performance than simply using the length of a word
as the cost function. As an example consider the problem of proving that
if $ab = ba$, then $a^5b^5=b^5a^5$. Using length as the cost function, the size
of the search graph for this problem is 5906, but using the improved cost function,
the search graph size contains 604 words. This method is obviously not the
best method for problems in abelian groups like this, but the improved cost
function should also speed up the search in problems where some, but not all
of the variables commute.

\subsection{Generating Proofs}
In order to generate proof terms the algorithm must keep track of what path was taken
to rewrite a word to the identity. The following information is stored at each node
in the search graph, in the data type
\begin{lstinline}
  !path_step!
\end{lstinline}

\begin{lstlisting}
@[derive inhabited] structure path_step : Type :=
(rel_index : ℕ) -- Index of the relator used to make the rewrite
(rel_is_inv : bool) -- Boolean representing whether the relator or
  --its inverse was used to make the substitution
(old_word : free_group) -- word before the substitution was made
(new_word : free_group) -- word after the substitution was made
(new_word_cost : ℕ) -- cost of the new word
(word_letter_index : ℕ) -- Letter index in the old word,
  -- indicating where the substitution was made
(rel_letter_index : ℕ) -- Letter index in the relator indicating
  -- which letter in the relator was substituted.
\end{lstlisting}

This information is enough to retrack the path that was taken and write a
proof that will always take the same standard form.

Given words \begin{lstinline} !word₁ rel word₂ conj old_word new_word rel_conj! \end{lstinline}
in the free group, it is true that if
\begin{lstinline} !conj⁻¹ * word₁ * rel_conj⁻¹ * rel * rel_conj * word₂ * conj! \end{lstinline}
is in the normal closure of a set of relators, \begin{lstinline}!word₁ * word₂ = old_word!\end{lstinline},
and \begin{lstinline}!rel!\end{lstinline} is
one of the relators, then \begin{lstinline}!old_word!\end{lstinline} is also in the
normal closure of the set of relators.

Intuitively, we are splitting \begin{lstinline}!old_word!\end{lstinline} into two parts,
\begin{lstinline}!word₁!\end{lstinline} and \begin{lstinline}!word₂!\end{lstinline},
inserting some conjugate of \begin{lstinline}!rel!\end{lstinline} in between
\begin{lstinline}!word₁!\end{lstinline} and \begin{lstinline}!word₂!\end{lstinline},
and then conjugating the result as well.

\section{The Tactic}\label{tacdescript}

The practical problems that might come up in Lean proofs sometimes are not already in
exactly the form that can be solved by the algorithm described, and some work
is needed to convert them to a form that can be solved.

As an example, consider the following proposition. If $a$ and $b$ are elements
of a group, and $ab = b^2a$, then for any natural number $n$, $a^nba^{-n} = b^{2^n}$.
The proof is by induction, and we consider the inductive step. The information
visible to the user at the inductive step is as follows

For the inductive step of this proof, we have the hypothesis that $ab = b^2a$,
the induction hypothesis says that $a^nba^{-n} = b^{2^n}$,
and we need to prove $a^{n+1}ba^{-(n+1)} = b^{2^{n+1}}$.

This can be converted into a word problem. Our atoms will be
$a$, $a^n$, $b$ and $b^{2^n}$ and we add assumptions for commuting powers,
because $a$ commutes with $a^n$, and $b$ commutes with $b^{2^n}$.

We will introduce new variables $c := a^n$ and $d := b^{2^n}$. The problem
then becomes the following

\begin{equation}\label{problem}
  \begin{aligned}
    ac=ca \\
    bd=db \\
    ab=b^2a \\
    cbc^{-1} = d \\
    \vdash acbc^{-1}a^{-1} = d^2
  \end{aligned}
\end{equation}

We now descibe some of the process of converting the Lean tactic state into the
problem above. The tactic has to effectively identify the atoms,
\begin{lstinline} !a! \end{lstinline}, \begin{lstinline} !a ^ n! \end{lstinline},
\begin{lstinline} !b! \end{lstinline}, and \begin{lstinline} !b ^ 2 ^ n! \end{lstinline}.

\begin{lstlisting}
  G : Type,
  _inst_1 : group G,
  a b : G,
  h : a * b = b ^ 2 * a,
  n : ℕ,
  ih : a ^ n * b * a ^ -↑n = b ^ 2 ^ n
  ⊢ a ^ n.succ * b * a ^ -↑(n.succ) = b ^ 2 ^ n.succ
\end{lstlisting}

Note that \begin{lstinline} !n.succ! \end{lstinline} means $n + 1$, or the
successor of $n$. Another thing to note is the coercion arrow
\begin{lstinline} !↑! \end{lstinline}. This arrow represents the canonical map
from the natural numbers to the integers - usually in maths, this is not explicitly
written, but in Lean it must be used, and this provides a small challenge in
identifying the atoms, care has to be taken to ensure that the tactic identifies
that
\begin{lstinline} !a ^ -↑n! \end{lstinline} is indeed the inverse of
\begin{lstinline} !a ^ n! \end{lstinline}.

Formally speaking the \begin{lstinline} !^! \end{lstinline} notation
is being used for two distinct functions in this tactic state. One
is a binary function whose domain is $G \times \mathbb{Z}$,
the other has domain $G \times \mathbb{N}$. There is a lemma in Lean
identifying that these two functions are compatible, in other words,
that applying the natural number power function, is the same as
applying the canonical map from $\mathbb{N}$ to $\mathbb{Z}$, and then
applying the integer power. The tactic must invoke this lemma in order to prove
that \begin{lstinline} !a ^ -↑n! \end{lstinline} is the inverse of
\begin{lstinline} !a ^ n! \end{lstinline}.

The tactic must also identify that \begin{lstinline} !b ^ 2 ^ n.succ! \end{lstinline}
is equal to \begin{lstinline} !(b ^ 2 ^ n) ^ 2! \end{lstinline}.

In order to transform this tactic state into the problem in Equation \ref{problem},
the hypotheses and the goal must first be rewritten. Below is an incomplete list of some
of the rules used to normalize the goal.

\begin{itemize}
  \item Normalize all numerals to the form
    $1 + 1 + 1 + \dots$
  \item Normalize all powers to use the integer power, not the natural power
  \item Rewrite expression of the form $a ^ (m + n)$ to $a^m a^n$
  \item Apply the distributivity law within exponents, expand
    $a(b+c)$ to $ab + ac$.
\end{itemize}

Lean has an inbuilt tactic called \begin{lstinline} !simp! \end{lstinline} which is used
to perform the above substitutions.

Normalizing all numerals to the form $1 + 1 + 1 + \dots$, may seem very inefficient,
but whilst it is certainly not the fastest approach, the
solving algorithm has such poor performance on large exponents, that it is not made
that much worse by this approach, and the tactic should probably not be used with
large exponents anyway.

After this normalization procedure is applied, the new tactic state is as follows.
Note that this stage will be invisible to the user of the tactic.

\begin{lstlisting}
  G : Type,
  _inst_1 : group G,
  a b : G,
  h : a * b = b * b * a,
  n : ℕ,
  ih : a ^ ↑n * b * (a ^ ↑n)⁻¹ = b ^ ↑((1 + 1) ^ n)
  ⊢ a * a ^ ↑n * b * (a ^ ↑n * a)⁻¹ = b ^ ↑((1 + 1) ^ n) * b ^ ↑((1 + 1) ^ n)
\end{lstlisting}

Now it is easier to identify the atoms, since \begin{lstinline} !a ^ ↑n! \end{lstinline}
appears in exactly the same form wherever it appears, and similarly, \begin{lstinline} !b ^ ↑((1 + 1) ^ n)!
\end{lstinline} appears as the exact same expression each time it appears. It is
now easy to automatically convert it to the problem in Equation \ref{problem}.
\section{Comparison of Methods}

In this section we compare the two methods,
Magnus' method and the graph search
method. We also compare two different superposition provers,
one implemented in Lean called \lstinline{super}, and another much more optimised
one called SPASS, which does not produce a proof verifiable by Lean.

A superposition is a more general automated theorem prover than either of the two methods
described above; it is a general purpose first order logic prover. In order to use these provers
the problems must be stated in a slighty different form to have reasonable performance.
As an example, the problem of checking if $a$ is in the normal closure of the
relators $aba^{-1}b^{-2}$ and $bab^{-1}a^{-2}$, would be written in the following form.
We assume we have four unary function symbols in our first order language,
$a$, $b$, $a'$ and $b'$, and we give the prover the following problem to prove. In
this format, there is no need to explicitly apply the associativity law, which
leads to a large performance improvement. The \lstinline{super} tactic cannot solve
even quite straightforward problems without stating them in the form below.

\begin{equation}
  \begin{aligned}
  \forall x, a (a' (x)) = x \\
  \forall x, a' (a (x)) = x \\
  \forall x, b (b' (x)) = x \\
  \forall x, b' (b (x)) = x \\
  \forall x, a(b(a'(b'(b'(x))))) = x \\
  \forall x, b(a(b'(a'(a'(x))))) = x \\
  \vdash \forall x, a(x) = x
  \end{aligned}
\end{equation}

It is important to note that this is a comparison of one implementation
of Magnus' method, and that other implementations that use the optimizations for the HNN
normalization procedure described in Section \ref{HNNperf} may perform much better on
particular examples. The implementation of Magnus' method that
we are comparing here uses a simple left to right HNN normalization
procedure, where the leftmost possible rewrite is always attempted first.

Both tactics work in two stages, one stage solves the word problem, and a second stage
to produce and check the formal Lean proof. For the best comparison of the two
methods, it is best not to include the time taken to write and check a Lean proof,
since this takes a lot of the time, and the differences in performance are not a reflection
of the merits of one method over another, but instead reflect two different methods
to produce a Lean proof. Both times are included in the table below however, the ``solver
time" is the time taken excluding the proof writing and checking time, and
the ``tactic time" is the total time taken to execute the tactic. For the superposition
provers, the time taken for the \lstinline{super} tactic to prove a problem is best compared
with the tactic execution time of the two methods described in this paper, whilst
the time taken by SPASS is best compared with the ``solver" time for the two methods described in
this paper.

For both methods we include the length of the solution found. For Magnus'
method this means the length of the word in $F(F(S))$ which is computed as the certificate.
For the graph search method, this means the number of substitutions required
in the proof.
We also include the tree size for the graph search method, which is the
size of the set $L$ at the end of the computation.
\pagebreak
\begin{landscape}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{longtable}
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
\begin{longtable}[!h]{l|l|l|lll|llll|l|l}
  \hline
  \multirow{2}{*}{Label} & \multirow{2}{*}{Relators} & \multirow{2}{*}{Word} & \multicolumn{3}{l|}{Magnus' method} & \multicolumn{4}{l|}{Graph search method} & super & SPASS \\ \cline{4-12}
   &  &  & Tactic & Solver & Length & Tactic & Solver & Length & Tree & Tactic & Solver \\ \hline
  \endhead
  %
  \multirow{6}{*}{1} & \multirow{6}{*}{$abab$} & $babab^{-1}a^{-1}b^{-1}a^{-1}$ & 326ms & 1ms & 2 & 673ms & 1ms & 2 & 4 & 369ms & 20ms \\
   &  & \$(baba)\textasciicircum{}4 & 337ms & 6ms & 4 & 782ms & 6ms & 4 & 4 & 683ms & 20ms \\
   &  & $(b^{-1}a^{-1}b^{-1}a^{-1})^4$ & 514ms & 1ms & 4 & 594ms & 5ms & 4 & 4 & 1.29s & 20ms \\
   &  & $(baba)^{10}$ & 669ms & 2ms & 10 & 1.73s & 31ms & 10 & 10 & \textgreater{}600s & 20ms \\
   &  & $(baba)^{100}$ & 4.26s & 10ms & 100 & 66.2s & 1.54s & 100 & 100 &  & 30ms \\
   &  & $(baba)^{200}$ & 5.23s & 19ms & 200 & 436s & 6.61s & 200 & 200 &  & 60ms \\ \hline
  \multirow{4}{*}{2} & \multirow{4}{*}{$aba^{-1}b^{-2}$} & $aba^{-1}bab^{-1}a^{-1}b^{-1}$ & 390ms & 2ms & 2 & 577ms & 64ms & 2 & 20 & 1.32s & 20ms \\
   &  & $a^2ba^{-2}ba^2b^{-1}a^{-2}b^{-1}$ & 324ms & 6ms & 6 & 529ms & 20ms & 6 & 91 & 5.17s & 20ms \\
   &  & $a^5ba^{-5}ba^5b^{-1}a^{-5}b^{-1}$ & 5.14s & 141ms & 62 & 5.60s & 3.89s & 70 & 20193 & \textgreater{}600s & 90ms \\
   &  & $a^9ba^{-9}ba^9b^{-1}a^{-9}b^{-1}$ &  & 920ms & 1022 &  &  &  &  &  & 160s \\ \hline
  \multirow{3}{*}{3} & \multirow{3}{*}{$(ac)b(ac)^{-1}b^{-2}$} & $acb(ac)^{-1}bacb^{-1}(ac)^{-1}b^{-1}$ & 180ms & 4ms & 2 & 277ms & 2ms & 2 & 20 & 3.3s & 20ms \\
   &  & $(ac)^2b(ac)^{-2}b(ac)^2b^{-1}(ac)^{-2}b^{-1}$ & 420ms & 7ms & 6 & 775ms & 13ms & 6 & 90 & 7.1s & 20ms \\
   &  & $(ac)^5b(ac)^{-5}b(ac)^5b^{-1}(ac)^{-5}b^{-1}$ & 4.97s & 57ms & 62 & 4.06s & 1.28s & 62 & 6338 & \textgreater{}600s & 40ms \\ \hline
  \multirow{2}{*}{4} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}$aba^{-1}b^{-2}$,\\ $bca^{-1}$\end{tabular}} & $aba^{-1}bab^{-1}a^{-1}b^{-1}$ & \multicolumn{3}{c|}{\multirow{2}{*}{NA}} & 297ms & 2ms & 2 & 24 & 13.8s & 20ms \\
   &  & $a^2ba^{-2}ba^2b^{-1}a^{-2}b^{-1}$ & \multicolumn{3}{c|}{} & 490ms & 19ms & 6 & 100 & 45.5s & 20ms \\ \hline
  \multirow{3}{*}{5} & \multirow{3}{*}{$aba^{-1}b^{-1}$} & $a^2b^2a^{-2}b^{-2}$ & 413ms & 13ms & 4 & 366ms & 6ms & 4 & 34 & 446ms & 20ms \\
   &  & $a^5b^5a^{-5}b^{-5}$ & 1.45s & 69ms & 25 & 1.14s & 100ms & 25 & 604 & 1.14s & 20ms \\
   &  & $a^{10}b^{10}a^{-10}b^{-10}$ & 4.21s & 110ms & 100 & 5.83s & 1.35s & 100 & 5314 & 106s & 30ms \\ \hline
  \multirow{3}{*}{6} & \multirow{3}{*}{$acb(ac)^{-1}b^{-1}$} & $(ac)^2b^2(ac)^{-2}b^{-2}$ & 215ms & 5ms & 4 & 339ms & 4ms & 4 & 34 & 3.34s & 20ms \\
   &  & $(ac)^5b^5(ac)^{-5}b^{-5}$ & 1.16s & 33ms & 25 & 1.17s & 96ms & 25 & 604 & 8.91s & 20ms \\
   &  & $(ac)^{10}b^{10}(ac)^{-10}b^{-10}$ & 4.32s & 92ms & 100 & 7.95s & 1.16s & 100 & 5314 & MEM & 20ms \\ \hline
  7 & $aba^{-3}b^4$ & $a^2baba^{-3}b^3a^{-2}b^{-4}a^{3}b^{-1}a^{-1}$ & 689ms & 359ms & 2 & 329ms & 12ms & 2 & 86 & 12.8s & 30ms \\ \hline
  8 & $aba^{-11}b^4$ & $a^{10}baba^{-11}b^3a^{-10}b^{-4}a^{11}b^{-1}a^{-1}$ & 1.42s & 768ms & 2 & 782ms & 56ms & 2 & 230 & \textgreater{}600s & 20ms \\ \hline
  9 & $aba^{-1}b^{-3}$ & $a^4baba^{-1}b^{-4}a^{-3}ba^{-1}b^{-3}$ & 11.1s & 67ms & 80 & 439ms & 6ms & 2 & 52 & 327ms & 20ms \\ \hline
  10 & \begin{tabular}[c]{@{}l@{}}$aba^{-1}b^{-2}$,\\ $bab^{-1}a^{-2}$\end{tabular} & $a$ & \multicolumn{3}{c|}{NA} & 337ms & 215ms & 5 & 1368 & 4.67s & 30ms \\ \hline
  11 & \begin{tabular}[c]{@{}l@{}}$aca^{-1}c^{-1}$,\\ $bdb^{-1}d^{-1}$,\\ $aba^{-1}b^{-2}$,\\ $cbc^{-1}d^{-1}$\end{tabular} & $acbc^{-1}a^{-1}d^{-2}$ & \multicolumn{3}{c|}{NA} & 492ms & 13ms & 3 & 120 & MEM & 30ms \\ \hline
  12 & \begin{tabular}[c]{@{}l@{}}$aca^{-1}c^{-1}$,\\ $b^2d^2b^{-2}d^{-2}$,\\ $ab^2a^{-1}b^{-4}$,\\ $cb^2c^{-1}d^{-2}$\end{tabular} & $acb^2c^{-1}a^{-1}d^{-4}$ & \multicolumn{3}{c|}{NA} & 1.50s & 61ms & 5 & 216 & 25.9s & 20ms \\ \hline
  13 & \begin{tabular}[c]{@{}l@{}}$aca^{-1}c^{-1}$,\\ $c^{-1}dcdc^{-1}d^{-1}cd^{-1}$,\\ $ac^{-1}dca^{-1}c^{-1}d^{-2}c$\end{tabular} & $ada^{-1}d^{-2}$ & \multicolumn{3}{c|}{NA} & 570ms & 15ms & 3 & 120 & 18.7s & 20ms \\ \hline
  14 & \begin{tabular}[c]{@{}l@{}}$aca^{-1}c^{-1}$,\\ $aba^{-1}b^{-2}$,\\ $cbc^{-1}bcb^{-1}c^{-1}b^{-1}$\end{tabular} & $acbc^{-1}a^{-1}bacb^{-1}c^{-1}a^{-1}b^{-1}$ & \multicolumn{3}{c|}{NA} & 502ms & 34ms & 8 & 354 & 35.9s & 30ms
  \label{tab:my-table}\\
  \end{longtable}
\end{landscape}
\pagebreak

For problems where the single relator is of the form $r^n$, where
$r$ is a word in the free group, and $n > 1$, then both methods perform
very well. There is a theorem (proper citation) that says that
these problems can be solved whilst only performing rewrites that
make the word shorter. Neither method will perform a rewrite that
makes the word longer on these problems, so both perform well. The table
above shows the performance of each algorithm for the relator $abab$,
labelled 1 in the table.
Magnus' method appears to be linear time on these problems, whereas
the graph search method is not. To prove $(baba)^n = 1$, will require
$n$ steps and at each step the number of rewrites the algorithm
will add to the graph will be proportional to the length of the graph,
so one would expect at least quadratic time. The overall tree size
is small because most of the words added to the graph will
be duplicates of words already in the graph. The \lstinline{super} tactic performed particularly
badly here, which is surprising given how easy it is to simplify some of the words.

The next problem we compare, is given the relator $ab = b^2a$,
prove that $a^nba^{-n}b = ba^nba^{-n}$ for various different values of $n$.
The reason this is true is
because in this particular group, $a^n b a^{-n} = b^{2^n}$, and therefore
$a^n b a^{-n}$ must commute with $b$. Therefore, in order to prove the result,
the graph search method must perform rewrites that make the word longer,
whereas it aims to make words shorter, and will only perform rewrites that
make a word longer after having exhausted all rewrite sequences that
do not make the word longer. So one would expect the graph search method to perform
poorly on these problems, and this is corroborated by the data, particularly the solve time,
although the overall tactic execution time is similar. The superposition provers both performed
badly on this problem as well, this was the only problem where either of my algorithms outperformed
SPASS, since Magnus' method was much faster for large values of $n$.

I also experimented with the same relation but changing each occurence of $a$ with $ac$.
This worsened the performance of \lstinline{super}, perhaps because the extra letter $c$ necessitated the
inclusion of two new function symbols, $c$ and $c'$, and two new equalities,
$\forall x, c (c' (x)) = x$ and $\forall x, c' (c (x)) = x$ in the format that the problem
is inputted to \lstinline{super}.

I experimented here with using the same problem as above, with relator  $ab = b^2a$, and
attempting to prove $a^nba^{-n}b = ba^nba^{-n}$, but adding an entirely superfluous
relator $bca^{-1}$. This significantly damaged the performance of \lstinline{super}, but the performance
of the graph search method was only slightly worse.

We also compare the relator $aba^{-1}b^{-1}$ and words of the form $a^nb^na^{-n}b^{-n}$ for
different values of $n$. For this problem Magnus' method has the edge although of course
neither method is a sensible choice for this problem. Again, Magnus' method particularly
outperforms the search method on the solve time, although overall tactic execution times are
similar.

The next three problems, labelled 7, 8 and 9, all have a very easy solution with only two
subsitutions required. The problems labelled 8 and 9 are both problems where Magnus' method performs
very badly.
They both exhibit behaviour similar to the behaviour in Example \ref{ltrbad}. The graph search
method is much faster on both of these examples, since they can both be done in just two
substitutions, both of which make the word shorter. Note the difference in length between the
solution found by Magnus' method in each of the final two problems. The first has length $80$,
which is much longer than the minimum required to solve the problem, which is $2$.
For the final problem it did find a solution of length $2$, but this was only after the golfing
heuristics described in Section \ref{golfsec}. For this problem it initially found a certificate
of length $36$, before the golfing code shortened it. Surprisingly, \lstinline{super}
performed extremely badly on problems 7 and 8, despite them only requiring two substitutions,
both of which made the word much shorter.

The final four problems are all related to the group with presentation
$\langle a, b \ : ab = b^2a\rangle$. Problem 11 is the inductive step
of the proof that $a^nb = b^{2^n}a$ in this group. \lstinline{super} failed to prove this
within ten minutes. The graph search method was able to eliminate the variable
$b$, and this may have given it an advantage over \lstinline{super}. Therefore problem 12
is the same problem, but with every occurence of $b$ replaced with $b^2$ and
with every occurence of $d$ replace with $d^2$. It is of course still possible
to eliminate $b$ or $d$, but the algorithm does not attempt this when every occurence
is a square. The graph search method still performed well on this problem, and
surprisingly this change meant that \lstinline{super} was naw able to solve the problem, albeit
very slowly. I also tried making it easier for \lstinline{super}, by eliminating $b$ before giving the problem
to \lstinline{super} in the problem labelled 13. \lstinline{super} was then able to solve it.

The final problem in the table is the inductive step of the proof that
$a^nba^{-n}$ and $b$ commute in the group $\langle a, b \ : ab = b^2a\rangle$.
The graph search method performs very well on this problem, but \lstinline{super} is slow.

In conclusion, Magnus' method appears to be better than either the graph search method
or \lstinline{super} on most one-relator problems. The problems where it performs badly are
carefully constructed to exploit the weaknesses, and these weaknesses could be fixed
by implementing the optimizations in Section \ref{HNNperf}. Magnus' method even
outperforms SPASS on one problem, so there is weak evidence to suggest this method
is overall faster than a superposition prover, especially considering that SPASS
has had far more time invested in optimizing it than my implementation of Magnus'
Method. However it is difficult to draw any firm conclusions because of the drastic variation
in performance depending on the problem given. Both the graph
search method and Magnus' method outperform \lstinline{super}.

\section{To Do}
\begin{itemize}
  \item Mention crappy justification for termination of search.
\end{itemize}

\bibliography{references}

\end{document}
